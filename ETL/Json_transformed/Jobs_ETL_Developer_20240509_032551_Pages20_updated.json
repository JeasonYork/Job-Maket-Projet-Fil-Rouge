[
    {
        "source": "LinkedIn",
        "company": "MCI",
        "location": "New Caledonia",
        "link": "https://nc.linkedin.com/jobs/view/sql-developer-at-mci-3912049201?position=2&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=acYZPwwJr44VgyKwDppebA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Full-Time\nJob Title\nGlobal Sales Director\nJob Type\nFull - Time\nLocation\nAngeles City, Remote US\nMCI is a leading Business Process Outsourcing (BPO) company that specializes in delivering tailored solutions to meet the diverse needs of our clients. With a commitment to excellence and a focus on innovation, we have established ourselves as a trusted partner in the industry.\nWe are seeking an SQL Developer, who will be responsible for designing, developing, and maintaining databases and database applications. The role will involve working closely with software developers, database administrators (DBAs), and other stakeholders to design database solutions, write SQL queries, optimize database performance, and ensure data integrity and security.\nTo be considered for this role, you must complete a full application on our company careers page, including all screening questions and a brief pre-employment test.\nKey Responsibilities\nDesign and develop database schemas, tables, views, stored procedures, and functions to support application requirements and business needs.\nCollaborate with software developers and architects to integrate database functionality into application architecture and design.\nWrite and optimize complex SQL queries for data retrieval, manipulation, and analysis, ensuring efficient performance and minimal resource consumption.\nIdentify and resolve performance bottlenecks, query inefficiencies, and indexing issues to enhance database performance and scalability.\nPerform routine database maintenance tasks, such as backups, restores, and data migrations, to ensure data availability, integrity, and recoverability.\nMonitor database performance, storage utilization, and resource utilization, and implement proactive measures to optimize performance and prevent downtime.\nDesign and implement data integration processes, ETL (Extract, Transform, Load) workflows, and data migration scripts to facilitate seamless data flow between different systems and platforms.\nValidate and cleanse data during the ETL process to maintain data quality and consistency across the organization.\nImplement database security policies, access controls, and encryption mechanisms to protect sensitive data and comply with regulatory requirements (e.g., GDPR, HIPAA).\nConduct regular security audits and vulnerability assessments to identify and mitigate security risks and ensure compliance with data protection standards.\nDocument database designs, data models, and technical specifications to facilitate system maintenance, troubleshooting, and knowledge sharing.\nProvide technical support and guidance to development teams, DBAs, and other stakeholders on database-related issues, best practices, and performance optimization techniques.\nWONDER IF YOU ARE A GOOD FIT FOR THIS POSITION?\nAll positive, and driven applicants are encouraged to apply. The Ideal candidates for this position are highly motivated and dedicated and should possess the below qualities\nBachelor's degree in Computer Science, Information Technology, or related field.\nProven experience as an SQL Developer or Database Developer, with expertise in SQL query writing, database design, and performance tuning.\nProficiency in SQL programming languages (e.g., T-SQL, PL/SQL) and database management systems (e.g., Microsoft SQL Server, Oracle, MySQL).\nStrong understanding of database architecture, relational database principles, and data modeling concepts.\nMust be authorized to work in the country where the job is based.\nMust be willing to submit up to a LEVEL II background and/or security investigation with a fingerprint. Job offers are contingent on background/security investigation results.\nMust be willing to submit to drug screening. Job offers are contingent on drug screening results.\nWANT AN EMPLOYER THAT VALUES YOUR CONTRIBUTION?\nWe offer competitive compensation packages, professional development opportunities, and a collaborative work environment that values diversity and inclusion.\nThis job operates in a professional office environment. While performing the duties of this job, the employee will be largely sedentary and will be required to sit/stand for long periods while using a computer and telephone headset. The employee will be regularly required to operate a computer and other office equipment, including a phone, copier, and printer. The employee may occasionally be required to move about the office to accomplish tasks; reach in any direction; raise or lower objects, move objects from place to place, hold onto objects, and move or exert force up to forty (40) pounds.\nIt is the policy of MCI and affiliates to provide reasonable accommodation when requested by a qualified applicant or employee with a disability unless such accommodation would cause undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment. If reasonable accommodation is needed, of Human Resources.\nAt MCI and its subsidiaries, we embrace differences and believe diversity is a benefit to our employees, our company, our customers, and our community. All aspects of employment at MCI are based solely on a person's merit and qualifications. MCI maintains a work environment free from discrimination, one where employees are treated with dignity and respect. All employees share in the responsibility for fulfilling MCI's commitment to a diverse and equal opportunity work environment.\nMCI does not discriminate against any employee or applicant on the basis of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations, and ordinances. MCI will consider for employment qualified applicants with criminal histories in a manner consistent with local and federal requirements.\nMCI will not tolerate discrimination or harassment based on any of these characteristics. We adhere to these principles in all aspects of employment, including recruitment, hiring, training, compensation, promotion, benefits, social and recreational programs, and discipline. In addition, it is the policy of MCI to provide reasonable accommodation to qualified employees who have protected disabilities to the extent required by applicable laws, regulations, and ordinances where an employee works.\nMCI (www.mci.world) helps customers take on their CX and DX challenges differently, creating industry-leading solutions that deliver exceptional experiences and drive optimal performance. MCI assists companies with business process outsourcing, staff augmentation, call center services, customer services, and IT Services needs by providing general and specialized hosting, software, staff, and services.\nIn 2019 Marlowe Companies Inc. (MCI) was named by Inc. Magazine as Iowa‚Äôs Fastest Growing Company in the State of Iowa and was named the 452nd Fastest Growing Privately Company in the USA, making the coveted top 500 for the first time. MCI‚Äôs subsidiaries had previously made Inc. Magazine's List of Fastest-Growing Companies 15 times respectively. MCI has fifteen business process outsourcing service delivery facilities in Iowa, Georgia, Florida, Texas, Massachusetts, New Hampshire, South Dakota, New Mexico, California, Kansas, and Nova Scotia.\nDriving modernization through digitalization, MCI ensures clients do more for less. MCI is the holding company for a diverse lineup of tech-enabled business services operating companies. MCI organically grows, acquires, and operates companies that have a synergistic products and services portfolios, including but not limited to Automated Contact Center Solutions (ACCS), customer contact management, IT Services (IT Schedule 70), and Temporary and Administrative Professional Staffing (TAPS Schedule 736), Business Process Management (BPM), Business Process Outsourcing (BPO), Claims Processing, Collections, Customer Experience Provider (CXP), Customer Service, Digital Experience Provider (DXP), Account Receivables Management (ARM), Application Software Development, Managed Services, and Technology Services, to mid-market, Federal & enterprise partners. MCI now employs 10,000+ talented individuals with 150+ diverse North American client partners across the following MCI brands GravisApps, Mass Markets, MCI Federal Services (MFS), The Sydney Call Center, OnBrand24, and Valor Intelligent Processing (VIP).\nThe purpose of the above job description is to provide potential candidates with a general overview of the role. It's not an all-inclusive list of the duties, responsibilities, skills, and qualifications required for the job. You may be asked by your supervisors or managers to perform other duties. You will be evaluated in part based upon your performance of the tasks listed in this job description.\nThe employer has the right to revise this job description at any time. This job description is not a contract for employment, and either you or the employer may terminate employment at any time, for any reason.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "40",
            "Level": "",
            "Experience": null
        },
        "title": "Other",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "SQL Server",
                "MySQL"
            ],
            "Collaboration": [
                "Teams"
            ],
            "EnSoftSkils": [
                "Organization"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "digiRocks recrute ‚úÖ",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digirocks-recrute-%E2%9C%85-3903481080?position=3&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=u1YGRyud%2BVE0ms1zi0jDYQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üòé Envie d'accompagner des organisations dans leurs strat√©gies, Fan de data?\nRejoins un jeune cabinet de conseil en strat√©gie sp√©cialis√© en data. Le cabinet a √©t√© cr√©√© il y a 4 ans pas des anciens de grands cabinets de conseil en strat√©gie qui ne se reconnaissaient plus dans ce qu'est devenu le \"consulting\". Cependant ils n'ont pas perdu espoir de pouvoir apporter du conseil √† haute valeur ajout√©e dans une ambiance friendly, fa√ßon start-up, sans sacrifier l'excellence.\nJean-Patrick recrute un(e) Consultant Data Engineer √† Paris en CDI\n‚úÖ MISSION :\nVous serez responsable de la mise en ≈ìuvre de bout en bout de la pile de donn√©es, de la collecte au reporting, avec un accent sur l'infrastructure et les processus techniques. Vous travaillerez avec des Consultants en Strat√©gie & Data et les soutiendrez dans la r√©solution des d√©fis li√©s aux donn√©es de leurs clients. Vous contribuerez √† la d√©finition des strat√©gies de donn√©es, √† la mise en ≈ìuvre des syst√®mes de donn√©es et vous soutiendrez l'exploitation des donn√©es dans des projets transformationnels. En g√©n√©ral, vous serez responsable de comprendre intimement les probl√®mes, de concevoir une strat√©gie technique pour les adresser et de faciliter une ex√©cution technique de haute qualit√©.\n‚úÖ R√âSULTATS ATTENDUS :\nüöÄ R√©sultat 1: Unificateur de Donn√©es : Architecturer, assembler, assimiler, nettoyer et conformer de grands ensembles de donn√©es complexes pour livrer des insights commerciaux et alimenter les exp√©riences de produits de donn√©es.\nüöÄ R√©sultat 2: Agent de S√©curit√© des Donn√©es : Concevoir et construire une infrastructure de donn√©es fiable et √©volutive avec les techniques de confidentialit√© et de s√©curit√© de pointe pour prot√©ger les donn√©es.\nüöÄ R√©sultat 3: DataOps : Poss√©der la pile de donn√©es de bout en bout, y compris la collecte d'√©v√©nements, la gouvernance des donn√©es, les int√©grations de donn√©es et la mod√©lisation.\nüöÄ R√©sultat 4: Gardien des Donn√©es : Assurer la coh√©rence et la qualit√© de l'environnement technique et de la structure des donn√©es √† travers des m√©triques, de la documentation, des processus, des tests de donn√©es et de la formation.\nRequirements\n‚úÖ PROFIL RECHERCH√â :\nDipl√¥m√© d'une Grande Ecole de Commerce ou d'ing√©nieur, avec une premi√®re exp√©rience r√©ussie comme Data Engineer, id√©alement dans un contexte similaire au Conseil,\nConnaissance des services de Data Warehouses Cloud. Exp√©rience avec Google BigQuery, Snowflake, AWS Redshift/Athena, Looker, Azure SQL DWH, ou Azure Databricks est tr√®s souhaitable.\nConnaissance des architectures de donn√©es relationnelles et de grandes donn√©es, de l'entreposage de donn√©es, de l'int√©gration de donn√©es, de la mod√©lisation de donn√©es, de l'optimisation de donn√©es et des techniques d'analyse de donn√©es.\nExp√©rience dans la construction de pipelines de donn√©es de bout en bout en utilisant des plateformes de donn√©es sur site ou bas√©es sur le cloud.\nExp√©rience pratique dans la livraison de solutions comprenant des bases de donn√©es, SQL avanc√© et d√©veloppement logiciel dans des langues telles que Python.\nInt√©ress√© et connaissant les technologies Big Data et les technologies de l'√©cosyst√®me Apache telles que Beam, Spark, Kafka, Airflow, bases de donn√©es, int√©gration, gestion des donn√©es de r√©f√©rence, assurance qualit√©, manipulation de donn√©es et technologies de gouvernance des donn√©es.\nExp√©rience avec les plateformes cloud publiques et l'infrastructure cloud qui est essentielle.\nExpos√© aux outils ETL/ELT et de gouvernance.\nInt√©ress√© par les technologies et principes IA et ML.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "ML",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Lyon, Auvergne-Rh√¥ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-thales-3903089036?position=4&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=5BTSlqnHIPJ1%2Bp4q6XNZcA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üì¢ Nous recherchons un(e) Data Engineer, bas√©(e) √† Lyon\nüëâQuelques mots sur les activit√©s num√©riques de Thales Lyon :\nLes activit√©s num√©riques repr√©sentent une entit√© rattach√©e au groupe Thales, sp√©cialis√©e dans l‚ÄôIT et pr√©sente au national.\nL‚Äôagence de Lyon adresse divers sujets d‚Äôexpertise : ing√©nierie logiciels, cybers√©curit√©, infog√©rance des infrastructures et transformation digitale.\nüéØ\nVotre r√¥le et missions\nEn nous rejoignant, vous int√©grerez le centre de comp√©tences\nAugmented data\n,\nsp√©cialis√© dans la conception, le d√©veloppement et l‚Äô√©volution d‚Äôapplications data centr√©es. Vous y boosterez votre carri√®re en travaillant sur des technologies telles que\nSpark, Elasticsearch, Kube ...\nle plus souvent dans un environnement\nAgile\n.\nDans le cadre des projets que nous op√©rons aujourd‚Äôhui :\n- Vous contribuerez √† la conception, au maintien, √† la scalabilit√© des plateformes d‚Äôanalyse de donn√©es au travers de votre expertise sur les sujets data (base de donn√©es, gestion de flux, ETL ‚Ä¶)\n- Vous contribuerez √† la conception et √† la mise en production des pipelines d‚Äôanalyses et de transformations de donn√©es en veillant √† leur bonne adaptation aux besoins m√©tiers et aux contraintes techniques du client\n- Vous pourrez intervenir sur des sujets de visualisations, dans le but de notamment accompagn√©es nos clients sur la conception de Dashboard m√©tier intelligent ‚Ä¶\n- Vous serez √©galement amen√©es √† √©changer directement avec des DevOps/Datascientist pour la mise en place, l‚Äôint√©gration des pipelines et l‚Äô√©laboration des algorithmes de traitements de donn√©es.\n- A l‚Äô√©chelle du d√©partement, Vous serez un acteur majeur du d√©veloppement de notre activit√© et du lancement de nouveaux projets de valorisation de donn√©es.\nüôã‚Äç‚ôÄÔ∏è üôã‚Äç‚ôÇÔ∏è\nVotre profil\nDe formation Bac +5 en informatique (√©cole d‚Äôing√©nieur, Master ou √©quivalent), vous justifiez d‚Äôune premi√®re exp√©rience r√©ussie sur un projet data ? Vous souhaitez participer √† la conception et intervenir sur des solutions de r√©cup√©ration et d‚Äôexploitation de donn√©es m√©tiers dans des contextes critiques et hautement s√©curis√©s ?\nAutonome, dynamique, organis√©(e) et proactif(ve), vous souhaitez √©voluer au sein d‚Äô√©quipes passionn√©es par l‚Äôexploration et l‚Äôint√©gration des technologies nouvelles au service des m√©tiers de nos clients ?\nVous avez des comp√©tences qui couvrent les domaines suivants :\nMise en place et gestion de base de donn√©es (SQL, Elasticsearch, Clickhouse ...)\nLangages de programmations (Java, Python)\nGestion de flux (Kafka, flink, logstash ‚Ä¶)\nEnvironnements big Data (Spark/hadoop )\nPrincipes et outils de type ETL\nVous √™tes de plus int√©ress√©(e):\nPar les environnements containeris√©s (docker, kubernetes, helm ...)\nLes concepts DevOps (Ansible, CI/CD...)\nLes sujets de Datavisualisation (Vega, Kibana, python librairies...)\nVous aimez travailler en √©quipe ? Vous √™tes reconnu(e) pour vos qualit√©s relationnelles et vos capacit√©s de vulgarisation ?\nAlors notre poste d‚ÄôIng√©nieur(e) Data(H/F) est fait pour vous !\nüôå\nVotre carri√®re chez Thales\nDiff√©rentes opportunit√©s vous permettront de d√©couvrir d'autres domaines ou sites. Vous pourrez √©voluer et d√©velopper vos comp√©tences dans diff√©rents domaines.\nExplorez un espace attentif au d√©veloppement personnel.\nD√©veloppez vos talents dans un autre domaine du groupe Thales, en d√©couvrant de nouveaux produits, de nouveaux clients, un nouveau pays ou en vous orientant vers une solution plus complexe.\nChoisissez entre une expertise technique ou un parcours de leadership.\nVous travaillerez dans une entreprise r√©solument humaine avec des valeurs fortes comme la s√©curit√© au travail, l‚Äô√©galit√© Homme/Femme et l‚Äô√©quilibre vie personnelle/professionnelle (Accord T√©l√©travail).\nRattach√©(e) √† la Convention m√©tallurgie, vous b√©n√©ficierez aussi de ses multiples avantages (‚Ä¶)\nVous souhaitez en savoir plus ?\nN‚Äôh√©sitez pas √† contacter notre √©quipe de recrutement ou nos √©quipes directement.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "DevTools": [
                "Docker"
            ],
            "Automation": [
                "Kubernetes",
                "Ansible"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Leadership"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Air France",
        "location": "Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-d%C3%A9veloppeur-big-data-%23-h-f-at-air-france-3900080172?position=5&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=NRt5Zl9%2FQscrskQoCBvcMA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description du poste\nIntitul√© du poste\nData Engineer / D√©veloppeur Big Data # H/F\nM√©tier\nSyst√®mes d'informations - D√©veloppement\nCat√©gorie socio-professionnelle\nCadre\nPr√©sentation du contexte\nVous avez peut-√™tre d√©j√† voyag√© avec nous, mais que connaissez-vous de nos m√©tiers et de la richesse des donn√©es qu‚Äôils g√©n√®rent au quotidien ? Comment le traitement et l‚Äôexploitation de ces donn√©es peut contribuer √† notre strat√©gie de Revenue Management, ou encore aux multiples op√©rations √† r√©aliser pour permettre √† un vol de partir √† l‚Äôheure ?\nAir France-KLM fait r√™ver 104 millions de passagers par an, en les emmenant vers plus de 250 destinations, gr√¢ce √† une flotte de plus de 500 appareils. Le Groupe emploie 80 000 collaborateurs partout dans le monde :les opportunit√©s sont vastes pour mettre √† profit ses comp√©tences, apprendre et se d√©velopper !\nLe d√©partement de d√©veloppement DATA, OR & AI d‚ÄôAir France, au sein de la direction des Syst√®mes d‚ÄôInformation, intervient dans toute la cha√Æne de captation et de traitement des donn√©es du groupe pour d√©livrer √† nos m√©tiers des solutions applicatives cl√©s en main.\nLe d√©partement est √©galement en charge de l‚Äôensemble des outils techniques (ETL, DataLakes, DataWarehouses, Data visualisation) et du d√©veloppement des talents et comp√©tences de Data Engineering.\nNotre mission ? Transformer la donn√©e brute en d√©cision intelligente, pour mieux optimiser les m√©tiers d‚ÄôAir France ‚Äì KLM !\nPour cela, nous avons chacun un r√¥le essentiel √† jouer, pourquoi le v√¥tre ne serait pas celui de Data Engineer et de d√©veloppeur Big Data ?\nDescription de la mission\nAu sein de notre d√©partement, vous travaillerez main dans la main avec d‚Äôautres Data Engineers et d√©veloppeurs Big Data ainsi qu‚Äôavec des sp√©cialistes des m√©tiers.\nInt√©gr√© au sein d‚Äôune product team agile passionn√©e et dynamique :\nVous participez √† l‚Äôanalyse des besoins m√©tiers du commercial, des op√©rations a√©riennes, de l‚Äôexploitation sol en a√©roport, de la maintenance a√©ronautique ou encore du Cargo.\nVous contribuez √† la d√©finition, au d√©veloppement, √† l‚Äôindustrialisation et √† la maintenance d‚Äôapplications Big Data ou en Business Intelligence\nVous pr√©sentez la restitution de vos travaux et accompagnez les utilisateurs d‚Äôun point de vue fonctionnel ou m√©thodologique\nVous serez en contact avec les directions m√©tier du groupe Air France KLM.\nNous attachons beaucoup d'importance au d√©veloppement des comp√©tences de nos collaborateurs ainsi qu‚Äô√† leur offrir des conditions de travail favorables √† l‚Äôautonomie et aux missions √† forte valeur ajout√©e. L'ouverture, le respect, la bienveillance et le partage sont des valeurs humaines port√©es par l'entreprise.\nProfil recherch√©\nVous √™tes dipl√¥m√© de niveau Master ou Ing√©nieur dans les domaines informatiques, vous avez acquis une exp√©rience professionnelle dans le d√©veloppement d‚Äôapplications.\nVous disposez d‚Äôune exp√©rience du d√©veloppement indispensable en Backend / Java\nVous ma√Ætrisez les bases de donn√©es relationnelles et le langage SQL\nEn Compl√©ment, Vous Avez Une Connaissance Ou Une Exp√©rience Dans Tout Ou Partie Des Concepts Ou Outils Suivants\nEnvironnement Big Data (Spark, Hadoop, Elasticsearch, Kafka, ...)\nBase de donn√©es noSQL (MongoDB, HBase, REDIS) ou Data Warehouse Teradata\nOutil de Datavisualisation (Spotfire, PowerBI, Qlik ou Kibana)\nSolutions de Cloud (GCP) et hybride (GCP / AZURE)\n(Ces comp√©tences compl√©mentaires ou manquantes pouvant aussi s'acqu√©rir √† travers un parcours de reskilling et de formations aux outils du data engineering dispens√© en interne).\nVous avez particip√© √† des projets organis√©s en Scrum ou Kanban, et avez peut-√™tre m√™me ≈ìuvr√© comme Scrum-Master, ce qui vous permettra de vous int√©grer ais√©ment au sein d‚Äôune Product Team. Votre esprit de synth√®se, votre force de conviction et votre ma√Ætrise de la communication facilitent les d√©cisions avec l‚Äôensemble des collaborateurs de l‚Äô√©quipe, √©ventuellement en langue anglaise, √† l‚Äô√©crit comme √† l‚Äôoral.\nVous √™tes autonome, rigoureux(se), responsable et curieux(se), vous aimez travailler en √©quipe. Vous poss√©dez de bonnes capacit√©s d'√©coute, d'analyse, de synth√®se et de communication.\nEt bien s√ªr, vous √™tes passionn√©(e), enthousiaste et ing√©nieux(se)\nCe que nous vous offrons\nDe la cr√©ation de valeur pour l‚Äôensemble des m√©tiers d‚ÄôAir France KLM\nDes challenges et probl√©matiques complexes √† r√©soudre\nL‚Äôopportunit√© de d√©ployer des solutions Data industrielles √† l‚Äô√©chelle !\nUne grande part de responsabilit√© dans une structure hi√©rarchique horizontale\nUn important degr√© de libert√© pour apprendre et d√©velopper son expertise au sein de l‚Äô√©quipe\nOn vous attend le plus rapidement possible ! Et pour une dur√©e ind√©termin√©e ;)\nType de contrat\nCDI\nTemps partiel possible\nNon\nType d'horaires\nAdministratif\nProfil candidat\nNiveau d'√©tudes min. requis\nBac + 5 et plus\nLangue\nAnglais (4 - Confirm√© / C1)\nLocalisation du poste\nLocalisation du poste\nFrance, Provence-Alpes-C√¥te d'Azur, Alpes Maritimes (06)\nSite\nValbonne\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Confirm√©"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java"
            ],
            "DataBase": [
                "MongoDB",
                "HBase",
                "Elasticsearch",
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "Azure"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-sql-gcp-f-h-at-orange-business-3916557264?position=6&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=4BBfpaE2gtnp4tmuT%2BVqng%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l‚Äôun des leaders europ√©ens de la Data transformation ?\nNous l‚Äôavons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les √©tapes du voyage de la donn√©e.\nDepuis 30 ans, Business & Decision, entit√© de Digital Services s'est impos√©e comme un partenaire strat√©gique pour la transformation Data de nombreux clients, dans des secteurs tr√®s vari√©s. Digital Services est aujourd‚Äôhui l‚ÄôESN d‚ÄôOrange Business alliant les expertises historiques Cloud et Digital d‚ÄôOrange ainsi que le c≈ìur de m√©tier Data/IA de Business & Decision. Son but est d‚Äôaccompagner les entreprises et les acteurs publics dans leur transformation gr√¢ce aux 4000 experts pr√©sents dans plusieurs grandes villes fran√ßaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse ‚Ä¶\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.\nVotre quotidien ?\nEn int√©grant Orange Business, vous pouvez participer √† une grande diversit√© d‚Äôactivit√©s dans la Data. En voici un aper√ßu :\nAu d√©marrage du projet :\nRecueillir et analyser les besoins du client\nR√©diger les sp√©cifications fonctionnelles et techniques\nEstimre les charges\nPendant la phase de r√©alisation :\nMod√©liser des datawarehouses et datamart (int√©gration de flux et consolidation des donn√©es)\nD√©velopper les proc√©dures d‚Äôalimentation (ETL)\nD√©velopper en SQL\n/ PLSQL / Shell\nGarantir la qualit√© des donn√©es et leur disponibilit√©\nConcevoir et d√©velopper des solutions frontend BI √† des fins analytics & dashboarding\nR√©aliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre √©volution et de nos enjeux, vous pouvez aussi √©voluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d‚Äôinitiative est toujours la bienvenue !\nQualifications\nVous poss√©dez 3 ans d'exp√©rience ou plus dans la mise en ≈ìuvre de projets d√©cisionnels et ing√©nierie ou analyse data.\nVous avez de\nsolides comp√©tences en d√©veloppement SQL\n(job, scripting, d√©ploiement), vous avez l‚Äôhabitude de travailler dans un\nenvironnement Google Cloud Plateform\nainsi qu‚Äôavec\nPower BI\n.\nEnvie d‚Äôapprendre de nouvelles technos ? Vous souhaitez partager vos comp√©tences et b√©n√©ficier des expertises de la Team Orange Business ?\nOutre l‚Äôaspect technique, c‚Äôest une personnalit√© qui est recherch√©e !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-mod%C3%A9lisation-sql-f-h-at-orange-business-3916551577?position=7&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vVVscyPLDbH99Q%2BXCr5Ruw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Et si Business & Decision et Orange Business conjuguaient leurs forces pour devenir l‚Äôun des leaders europ√©ens de la Data transformation ?\nNous l‚Äôavons fait ! Notre alchimie nous positionne comme un acteur unique intervenant sur toutes les √©tapes du voyage de la donn√©e.\nDepuis 30 ans, Business & Decision, entit√© de Digital Services s'est impos√©e comme un partenaire strat√©gique pour la transformation Data de nombreux clients, dans des secteurs tr√®s vari√©s. Digital Services est aujourd‚Äôhui l‚ÄôESN d‚ÄôOrange Business alliant les expertises historiques Cloud et Digital d‚ÄôOrange ainsi que le c≈ìur de m√©tier Data/IA de Business & Decision. Son but est d‚Äôaccompagner les entreprises et les acteurs publics dans leur transformation gr√¢ce aux 4000 experts pr√©sents dans plusieurs grandes villes fran√ßaises comme Montpellier, Niort, Lyon, Bordeaux, Lille et Toulouse ‚Ä¶\nDescription du poste\nOrange Business, recherche pour son site de Lille, son futur Data Engineer pour rejoindre sa team Data.\nVotre quotidien ?\nEn int√©grant Orange Business, vous pouvez participer √† une grande diversit√© d‚Äôactivit√©s dans la Data. En voici un aper√ßu :\nAu d√©marrage du projet :\nRecueillir et analyser les besoins du client\nR√©diger les sp√©cifications fonctionnelles et techniques\nEstimer les charges\nPendant la phase de r√©alisation :\nMod√©liser des datawarehouses et datamart (int√©gration de flux et consolidation des donn√©es)\nD√©velopper les proc√©dures d‚Äôalimentation (ETL)\nD√©velopper en SQL / PLSQL / Shell\nGarantir la qualit√© des donn√©es et leur disponibilit√©\nR√©aliser la recette et les tests\nSuivre et mettre en production\nEn fonction de votre √©volution et de nos enjeux, vous pouvez aussi √©voluer sur des missions transverses (conseil, coaching, avant-vente, formation, audit, etc.). La prise d‚Äôinitiative est toujours la bienvenue !\nQualifications\nVous poss√©dez 5 ans d'exp√©rience ou plus dans la mise en ≈ìuvre de projets d√©cisionnels et en mod√©lisation.\nVous avez de s\nolides comp√©tences en d√©veloppement SQL\n(job, scripting, d√©ploiement) ainsi que sur Python.\nEnvie d‚Äôapprendre de nouvelles technos ? Vous souhaitez partager vos comp√©tences et b√©n√©ficier des expertises de la Team Orange Business ?\nOutre l‚Äôaspect technique, c‚Äôest une personnalit√© qui est recherch√©e !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Inetum",
        "location": "St.-Ouen, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-inetum-3843966639?position=8&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=BlCfRho2hVXsr1gigd79gQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "D√©tail de l'offre\nInformations g√©n√©rales\nEntit√© de rattachement\nInetum est un leader europ√©en des services num√©riques. Pour les entreprises, les acteurs publics et la soci√©t√© dans son ensemble, les 28 000 consultants et sp√©cialistes du groupe visent chaque jour l'impact digital des solutions qui contribuent √† la performance, √† l'innovation et au bien commun.\nPr√©sent dans 19 pays au plus pr√®s des territoires, et avec ses grands partenaires √©diteurs de logiciels, Inetum r√©pond aux enjeux de la transformation digitale avec proximit√© et flexibilit√©.\nPort√© par son ambition de croissance et d'industrialisation, Inetum a g√©n√©r√© en 2023 un chiffre d'affaires de 2,5 milliards d'‚Ç¨.\nPour r√©pondre √† un march√© en croissance continue depuis plus de 30ans, Inetum a fait le choix d√©lib√©r√© de se recentrer sur 4 m√©tiers afin de gagner en puissance et proposer des solutions sur mesure, adapt√©es aux besoins sp√©cifiques de ses clients le conseil (Inetum Consulting), la gestion des infrastructures et applications √† fa√ßon (Inetum Technologies), l'impl√©mentation de progiciels (Inetum Solutions) et sa propre activit√© d'√©diteur de logiciels (Inetum Software). Inetum a conclu des partenariats strat√©giques avec 4 grands √©diteurs mondiaux - Salesforce, ServiceNow, Microsoft et SAP et poursuit une strat√©gie d'acquisitions d√©di√©e afin d'entrer dans le top 5 europ√©en sur ces technologies et proposer la meilleure expertise √† ses clients.\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDescription du poste\nM√©tier\nApplications Delivery - Software Development\nIntitul√© du poste\nData Engineer H/F\nContrat\nCDI\nDescription De La Mission\nLe p√¥le BFA de la branche Application Services du groupe INETUM, recherche plusieurs Data Engineers afin d'intervenir aupr√®s de clients grands comptes au sein des march√©s bancaires et de l'assurance.\nAu sein de l'√©quipe Data, en tant que Data Engineer, vous participez √† la r√©alisation de divers projets et vos missions sont\nApporter votre connaissance en Big Data permettant la manipulation des donn√©es\nConcevoir les plateformes permettant de traiter des volumes de donn√©es importants\nMettre en place des bases de donn√©es\nPr√©parer le pipeline de donn√©es pour que les donn√©es d√©ploy√©es soient s√©curis√©es et claires afin d'√™tre analys√©es et transform√©es.\nProfil\nDe formation ing√©nieure en informatique Bac + 5 informatique ou scientifique\nBonne communication orale et √©crite en fran√ßais et niveau d‚Äôanglais professionnel\nSavoir- √™tre Bon esprit d'analyse et de synth√®se, sens de l'organisation et de la qualit√©, force de proposition, rigueur, travail en √©quipe, adaptabilit√©.\nSi vous vous reconnaissez, n'h√©sitez pas √† postuler !\nLocalisation du poste\nLocalisation du poste\nFrance\nVille\nSaint-Ouen\nCrit√®res candidat\nNiveau d'√©tudes min. requis\nBac+5\nNiveau d'exp√©rience min. requis\nPlus de 2 ans\nComp√©tences\nSQL\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "30 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Scalian",
        "location": "Valbonne, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-scalian-3819563847?position=9&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=ls0HHJwQgfpq8HfBIbSMQA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Ing√©nieur DATA / Data engineer (H/F)\nValbonne/Sophia-Antipolis\nType : CDI\nLieu : Locaux Scalian Sophia-Antipolis\nT√©l√©travail : En fonction des possibilit√©s\nDate de prise de poste : imm√©diatement ou en fonction de votre pr√©avis\nSalaire : en fonction du profil - entre 40 et 48K Brut annuels (hors avantages Scalian)\nAvantages Scalian : Accord d'entreprise t√©l√©travail, Tickets restaurants, Mutuelle groupe, accord am√©nagement temps de travail, compte √©pargne temps, accord de participation et int√©ressement groupe, programme cooptation et apports d'affaires, accompagnement parentalit√©, avantages CSE\nVous √™tes data engineer ou vous souhaitez le devenir !\nQuel sera votre r√¥le ?\nLa port√©e de la mission comprend (sans toutefois s'y limiter) :\nScience des donn√©es\nIng√©nierie des donn√©es\nAnalyse des donn√©es\nG√©nie logiciel\nCe que cette exp√©rience va vous apporter\nVous √™tes autonome, vous avez le sens du service et de l‚Äôanalyse, vous √™tes impliqu√©, nous vous offrons une ouverture sur des projets complexes et une rapide √©volution de carri√®re. Vous rejoignez notre business unit √† Sophia Antipolis compos√©e d'environ 50 consultants, avec possibilit√© de t√©l√©travail en fonction des sujets.\nNous co-construisons votre trajectoire professionnelle et assurons votre mont√©e en comp√©tences.\nNous nous inscrivons ensemble dans la dur√©e, nous assurons votre mont√©e en comp√©tences et disposons d'une vari√©t√© de sujets passionnants.\nCe que nous recherchons chez vous\nDe formation sup√©rieure (Bac+5, √©cole ou universit√©), vous poss√©dez id√©alement une premi√®re exp√©rience r√©ussie dans ce domaine (d√©butants accept√©s), vous aimez le travail en √©quipe.\nComp√©tences requises\n:\nEtape d‚Äôanalyse : Comprendre l‚Äôarchitecture technique, les sources de donn√©es, les objectifs fonctionnels.\nEtape de conception : Solution de conception avec un fort centrage sur les pipelines de donn√©es et les mod√®les ML et l‚Äôexposition des KPI via API\nMise en ≈ìuvre : Apr√®s les phases d‚Äôanalyse et de conception, proc√©der √† a mise en ≈ìuvre dans des technologies s√©lectionn√©es (Java,Scala,Python,Spark)\nCr√©er un code test√© et document√©\nTechno : Linux, Shell, Hadoop, Scrum, Python, Spark, Scala\nPourquoi feriez-vous le grand saut ?\nParce que Scalian vous accompagne dans le d√©veloppement de votre carri√®re :\nProgramme d'onboarding complet sur 1 an avec votre manager et votre RH\nProgramme de formation (Scalian Academy, e-learning, webinaires et formations externes)\nCommunaut√©s techniques (Squads, Practices) afin de valoriser et d√©velopper votre expertise\n√âv√©nements internes (Afterworks, Awards Dinner, Kick Off, Live Event du COMEX, Stand Up) et externes (participation √† des salons et forums sp√©cialis√©s dans nos domaines d‚Äôactivit√©s‚Ä¶)\nDispositif d‚Äôacc√©l√©ration d‚Äôacc√®s √† la mobilit√© interne et √† des √©changes internationaux type Erasmus\nParce que Scalian favorise la Qualit√© de Vie au Travail :\nCertifications Great Place to Work¬Æ et Best Workplaces for Women¬Æ\nPrime de cooptation, prime vacances, prise en charge par l‚Äôemployeur de 60% des titres-restaurant, Accord t√©l√©travail (jusqu‚Äô√† 2,5 jours par semaine indemnis√©s), RTT (dont une partie mon√©tisable), CSE (activit√©s ludiques, ch√®ques-cadeaux, ch√®ques vacances)\nBerceaux en cr√®ches inter-entreprises\nDon ou r√©ception de jours de cong√©s en cas de difficult√©s personnelles\nParce que Scalian d√©veloppe une politique RSE concr√®te et ambitieuse :\nMobilit√© durable (indemnit√© kilom√©trique v√©lo, leasing de v√©los √† assistance √©lectrique)\nActions environnementales (Fresque du Climat, Reforest'Action, Clean Up Day, m√©c√©nat ONF)\nPostes ouverts aux personnes en situation de Handicap\nDiverses politiques de diversit√©, d‚Äôinclusion et d‚Äôint√©gration mises en place\nScalian c‚Äôest aussi :\nUne entreprise en tr√®s forte croissance qui, cr√©√©e en 1989, compte aujourd‚Äôhui plus de 5500 personnes\nDes r√©f√©rences clients √† forte valeur ajout√©e aupr√®s de grands industriels fran√ßais (du CAC40) et internationaux\nUn terrain de jeu o√π l‚Äôexpertise se conjugue avec audace, libert√© d‚Äôentreprendre et convivialit√©\nSi vous aspirez √† un environnement de travail qui valorise autant votre bien-√™tre que votre d√©veloppement professionnel,\nrejoignez-nous et exprimez pleinement votre talent !\nEnvie d'√©largir le cadre ?\nJe suis Liza Djehel, Talent Acquisition Officer.\nSi votre CV est retenu, je vous contacte pour un premier √©change t√©l√©phonique de 15 √† 20 minutes.\nNous d√©terminons ensemble si ce poste est en ad√©quation avec vos comp√©tences et surtout, avec vos attentes.\nL'√©change est positif ? Nous convenons d'un entretien de 1h (en pr√©sentiel ou en visio) avec Lucas Daunar, Business Manager √† Sophia-Antipolis. Cet √©change permet de revenir en d√©tail sur vos comp√©tences, vos attentes, de vous pr√©senter le poste plus en d√©tail, et d'√©voquer d'autres opportunit√©s.\nNous pr√©voyons ensuite un rendez-vous technique de 1h (en pr√©sentiel ou en visio) avec un de nos responsable technique.\nEnfin, nous vous pr√©sentons notre proposition d'embauche.\nNotre processus de recrutement dure entre 15 et 30 jours\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "40",
            "Level": "Bac+5",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "OS": [
                "Linux"
            ],
            "Other": [
                "ML"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Action for Market Transformation - A4MT",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-action-for-market-transformation-a4mt-3910049004?position=10&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=TK2pmr%2BnPCY9%2FL7EfnBCng%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "A4MT ‚Äì Action pour la Transformation des March√©s\nA4MT con√ßoit et impl√©mente des programmes d‚Äôengagement et de ¬´ Market Transformation ¬ª qui visent √† g√©n√©raliser des pratiques vertueuses ‚Äì au sens environnemental et soci√©tal ‚Äì en modifiant la donne du march√©, en reconfigurant le jeu d‚Äôacteurs, g√©n√©ralement via des actions collectives.\nCes programmes agissent sur la demande en suscitant de nouvelles pratiques individuelles et collectives. A4MT assure le r√¥le de pilote, orchestrant les plans d‚Äôaction des parties prenantes gr√¢ce √† une √©quipe de qualit√© √† caract√®re international, un savoir-faire sur la mise en ≈ìuvre des programmes, une connaissance technico-√©conomique experte des sujets trait√©s, et une capacit√© √† interpeller les d√©cideurs √† bon niveau.\nChampionnat de France des √©conomies d‚Äô√©nergie\nA4MT avec ses partenaires op√®re l‚Äôensemble des concours CUBE en France (Championnat de France des Economies d‚ÄôEnergies) et assure son d√©veloppement international (Europe, Asie, etc.). CUBE est un concours original d‚Äô√©conomies d‚Äô√©nergie et de CO2 pour les b√¢timents tertiaires et r√©sidentiels qui acc√©l√®re fortement l‚Äôaction de terrain gr√¢ce √† une intelligence collective sur le terrain.\nLe concours est aujourd‚Äôhui pr√©sent dans 8 pays et se d√©veloppe encore. Au-del√† des √©conomies les plus faciles, il s‚Äôagit de mettre en ≈ìuvre la trajectoire de gestion immobili√®re et d‚Äôinvestissement qui permettra, au-del√† des avanc√©es dans ce programme √† faible investissement, de progresser sur la trajectoire de la neutralit√© carbone.\nhttps://championnatdefrancedeseconomiesdenergie.org/\nMISSION\nRendant compte au directeur d‚ÄôA4MT et en √©troite collaboration avec le directeur technique A4MT, vous √™tes Data Engineer, vous serez responsable de la conception, du d√©veloppement et de la maintenance des bases de donn√©es, et des outils de reporting. Vous travaillerez en √©troite collaboration avec l'√©quipe de d√©veloppement (prestataire externe) et vous participez √† la structuration d‚Äôune √©quipe IT interne pour cr√©er des solutions innovantes r√©pondant aux besoins de l'entreprise.\nVotre mission s'articule autours des 3 axes ci-dessous:\n1/ Pilotage et et d√©veloppement\nd√©velopper et d√©ployer des reporting robustes et √©volutifs.\nle planning de d√©veloppement et le budget allou√©.\navec les √©quipes d‚Äôanimation et le back office technique du programme et avec les prestataires externes pour comprendre les exigences et les sp√©cifications du projet.\n√† la conception de l'architecture des bases de donn√©es et √† la prise de d√©cisions techniques.\nla qualit√© des donn√©es en effectuant des contr√¥les qualit√©.\nles performances des applications pour garantir une exp√©rience utilisateur fluide.\nla maintenance et les mises √† jour r√©guli√®res des applications existantes.\n√† l'aff√ªt des tendances et des technologies √©mergentes.\nVous serez responsable du process, de la ma√Ætrise d‚Äôouvrage li√©e √† la Data et garant(e) de la qualit√© de service.\n2/ Implication des √©quipes et de la sous-traitance\nVous serez impliqu√© dans une √©quipe informatique naissante et dans une √©quipe projet avec les diff√©rentes fonctions m√©tiers. Vous devrez faire le suivi de votre implication avec le responsable de programme et le directeur technique d‚Äô A4MT :\n3/ Gestion de projet\nVous tiendrez le tableau de bord des outils : budgets, engagements, planning, r√©sultats, d√©veloppements.\nPROFIL\nVous avez une exp√©rience significative d‚Äôau moins 3 ann√©es dans l‚Äô√©cosyst√®me de big data, des serveurs et bases de donn√©es dans des contextes de projets, d‚Äôexploitation de migration.\nCOMPETENCES\nBac +5 dipl√¥m√©(e) d‚Äôune grande √©cole d‚Äôing√©nieur ou √©quivalent, vous √™tes :\n+5 dipl√¥m√© (e) d‚Äôune √©cole d‚Äôing√©nieurs ou √©quivalent, en Data science, Informatique, g√©nie logiciel ou domaine connexe.\nprofessionnelle d√©montr√©e de 3 ans ou plus en tant que Data Engineer\ndes langages structur√©s (JavaScript, Scala, Python‚Ä¶),\navec les bases de donn√©es relationnelles (MySQL, PostgreSQL) et non relationnelles (MongoDB, Firebase).\nau moins un outil de reporting (Power BI, Tableau ‚Ä¶)\ndes services de d√©ploiement et d'h√©bergement cloud comme AWS, Azure ou Google Cloud Platform.\ncomp√©tences en d√©veloppement back-end avec des technologies comme Node.js, Python, Ruby on Rails, ou Java. et notamment en PHP sont recommand√©es\ndes langages de programmation front-end tels que HTML5, CSS3 et JavaScript (notamment frameworks: comme React, Angular ou Vue.js).\n√† travailler en √©quipe, √† communiquer efficacement et √† r√©soudre les probl√®mes de mani√®re autonome.\ndes principes de s√©curit√© des applications web et des meilleures pratiques en mati√®re de d√©veloppement s√©curis√© ainsi que le respect du RGPD.\nDate d‚Äôentr√©e et conditions\nLe poste est √† pourvoir imm√©diatement; il est bas√© au 54, rue de Clichy, Paris (IX√®me). Niveau de r√©mun√©ration selon exp√©rience.\nContact\nMerci d‚Äôadresser votre candidature compl√®te (CV, lettre de motivation, pr√©sentation du cursus en cours de conclusions et r√©f√©rences √©ventuelles) √† l‚Äôattention de M. Adrien Brunella sur le mail elisabeth.clement@a4mt.com\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "MongoDB"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "PostgreSQL",
                "MySQL"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Selby Jennings",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/database-developer-at-selby-jennings-3900070880?position=11&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=MmyZ4WJAhpEun%2BieFAUZTQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "I am working on a role with a global quantitative and systematic hedge fund who are looking for a Database Developer to join their team in Paris to work closely with trading and research functions.\nThe ideal candidate would need to be proficient in Python, have System Architecture Experience and have proven professional work experience as a database developer.\nKey Responsibilities\n‚Ä¢ Your role involves providing daily trading insights to multiple activities and asset classes.\n‚Ä¢ You will work with traders, quants and other quantitative developers to build robust reporting and analytical tools, thus providing constant feedback on investment strategy performance.\n‚Ä¢ Given the dynamic nature of the business, you will continually seek solutions that are both flexible and robust and which integrate seamlessly into their technology landscape.\n‚Ä¢ Design and maintain storage solutions to help automate daily processes to crunch trading data\n‚Ä¢ Take ownership of production processes to ensure constant alignment with business objectives\n‚Ä¢ Continually expand and upgrade the software infrastructure to accommodate the changing business needs\nYour present skillset\n‚Ä¢ Expertise in\nrelational databases, NoSql databases\nor other storage solutions for large data sets\n‚Ä¢ Proven professional work experience as a\ndatabase developer\n‚Ä¢\nPython, C#\nexperience nice to have\n‚Ä¢\nSystem Architecture Experience\n‚Ä¢ Ability to multitask, set priorities and work in a team\n‚Ä¢ Excellent communication skills and team player\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "Other",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ramify",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-ramify-3896146641?position=12&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=HexetsgIg2qZbxO1WiSWdQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ABOUT\nRamify‚Äôs mission is to help people achieve financial freedom, no matter who they are and how much they have. We are revolutionizing the private wealth management industry by making smart and affordable financial products for everyone.\nNo more standardized solutions, hidden fees and complicated words, Ramify enables everyone to become a smart investor. The team combines elements of its research with technology to design customized investment portfolios composed of world-class financial products.\nThe team comprises around 15 talented individuals. Ramify is looking for talented people in all sectors, who want to have a huge impact, move fast and deliver.\nJOB DESCRIPTION\nThe Quantitative Investment Solutions (QIS) Team is dedicated to designing innovative investment portfolio models and developing cutting-edge investment features within our product suite. Moreover, the QIS Team is at the forefront of driving AI-based solutions for Ramify. This involves conceptualizing and implementing transformative AI solutions tailored to meet the diverse needs of various teams within Ramify.\nAs a Data Engineer on our QIS Team, you will play a pivotal role in shaping the future of investment strategies through data-driven insights and AI-based solutions. Collaborating closely with our talented team of quantitative researchers, and investment experts, your responsibilities will encompass architecting and implementing robust data pipelines. These pipelines will facilitate the seamless integration of diverse data sources, empowering Ramify teams to make informed decision-making.\nKey Responsibilities:\nDesign, build and launch data pipelines at scale to move data across Ramify platform with SQL technologies.\nDesign and implement processes and tools for data onboarding and quality, helping to deliver an industry best-practice solution for managing the data lifecycle.\nProduce stand-alone tools that can be used by other teams to automate data quality and discover faults.\nBuild analytical tools that provide insight into business metrics across Ramify.\nArchitect and lead the implementation of AI based solutions within Ramify\nPREFERRED EXPERIENCE\nWe're looking for people who:\nWant to make a difference. We are a small team effectively reshaping how people look at the industry. We need people who 'get it' and want to play an integral part in helping us accomplish this mission and are persistent in getting the job done.\nSkills we're looking for:\nMaster‚Äôs or upper-year undergraduate-level coursework in either Computer Science, Management Information Systems, Business Information Systems, Mathematics or Finance related field.\n2+ years of professional experience in data engineering.\n2+ years of experience with one or more coding languages such as Python (is a must), Java.\nExperience with data modeling and ETL design, implementation and maintenance.\nDemonstrable mastery of industry best practices in the data lifecycle, including data quality automation and tooling.\nExcellent written and verbal communication skills with ability to communicate complex designs and solutions to non-technical and highly technical audiences alike.\nGood attention to detail.\nStrong analytics and strategic thinking skills\nNice-to-haves :\nUnderstanding of ML/ Generative AI technologies and their applications.\nPossess a passion, curiosity, and energy for finance + investing. You understand the ins and outs of the wealth management, trading, and more importantly - know how to explain these concepts simply\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "ML"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Renault Digital",
        "location": "Boulogne-Billancourt, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-renault-digital-3911202728?position=13&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=qgGV8j9UkcjRkl8hja9q2A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Contexte :\nDans le cadre de son programme Industrie 4.0, Renault d√©veloppe depuis 2017 sa propre plateforme pour connecter et agr√©ger les donn√©es industrielles des 22 sites du Groupe et de plus de 2500 machines.\nFort de partenariats strat√©giques sign√©s avec Google Cloud (stack data full GCP), Renault Digital est √† la recherche d‚Äôun(e) Data Engineer au sein du P√¥le Architecture et Data pour mettre en place des cha√Ænes de traitement de donn√©es r√©pondant √† de nouveaux besoins m√©tiers.\nVous collaborerez au jour le jour avec les √©quipes m√©tiers ainsi qu‚Äôavec les autres fonctions du P√¥le Architecture & Data (Data Analysts et Scientists, architectes, ‚Ä¶), exploitant des t√©raoctets de donn√©es (√©v√©nements en mode streaming, traitements en batch et temps r√©els et les appels aux APIs) afin entre autres d‚Äôalimenter des mod√®les de machine learning (segmentation clients, d√©tection automatiquement des pannes des v√©hicules, ‚Ä¶).\nResponsabilit√©s principales :\nVous participez aux phases de framing, MVP et release des produits, services et APIs orient√©s data ;\nVous argumentez les choix d‚Äôarchitecture des projets et de la plateforme datalake sur GCP ;\nVous contribuez √† la valeur m√©tier des produits orient√©s Data s‚Äôappuyant sur le Datalake, en mettant en place des cha√Ænes bout en bout de traitement de la data, de l‚Äôingestion √† l‚Äôexposition d‚ÄôAPIs et √† la visualisation des donn√©es et des solutions ML/DS ;\nVous √™tes garant de la qualit√© des donn√©es transform√©es dans le Datalake, du bon fonctionnement des cha√Ænes de traitement et de l‚Äôoptimisation de l‚Äôutilisation des ressources des ressources cloud ;\nVous proposez des standards d‚Äôarchitecture et de d√©veloppement ;\nVous √™tes force de proposition, innovant(e) et bienveillant(e).\nEnvironement technique :\nSpark, Scala, Python, Java, Airflow, SQL, Google Cloud Platform (BigQuery, Cloud Storage, PubSub, Beam, Dataflow, Cloud ML, TensorFlow, Kubernetes), Git, Docker, JSON, Bash, Spotfire\nProfil recherch√© :\nVous avez minimum 5 ans d‚Äôexp√©rience en tant que Data Engineer ;\nVous disposez d‚Äôune exp√©rience en d√©veloppement Spark, Scala, Python et requ√™tage SQL sur des gros volumes de donn√©es ;\nVous avez une app√©tence pour la data : validation, transformation, analyse, valorisation ;\nVous poss√©dez une exp√©rience de d√©veloppement et orchestration de chaines ETL complexes via Airflow ou √©quivalent ;\nVous pratiquez la m√©thodologie agile (Agile Scrum et/ou Kanban) ;\nVous utilisez les services cloud (pr√©f√©rablement GCP) ;\nVous √™tes capable d‚Äô√©changer en anglais technique √©crit et oral.\nInformations compl√©mentaires :\nVotre poste sera bas√© √† Boulogne-Billancourt (France) en CDI (temps plein)\nVous b√©n√©ficiez de 2 √† 3 jours de t√©l√©travail par semaine\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Temps plein",
                "Full"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "MachineLearning": [
                "TensorFlow"
            ],
            "DataSerialization": [
                "Json"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git",
                "Docker"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "ML"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Flowdesk",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-emea-f-m-d-at-flowdesk-3860942388?position=14&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=%2FKDndimAKY6wICIB4Jcdkw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Flowdesk is seeking a highly motivated Data Engineer to join the team and managed by the Quentin, the Lead Data!\nThe data engineer will be an integral part of the growing data team, working closely with our trading and quantitative departments to provide a platform and tools to answer their analytical needs. The data platform revolves around Dagster deployed on Kubernetes for the orchestration, dbt on BigQuery for the transformations, and Looker for the visualization.\nAmong our current challenges, we are currently focusing on the real-time ingestion and processing of terabytes of market data.\nResponsibilities\nDesign, build, and maintain efficient, scalable, and reliable batch and streaming data pipelines to support Flowdesk's trading infrastructure.\nDevelop and maintain data warehouses, data lakes, and other data management systems with a strong focus on logical and physical modeling.\nContribute early on to the definition of the data team's data products to maximize ease of access, data quality, and related documentation.\nCollaborate with the software engineering team to integrate data-related functionality into Flowdesk's trading infrastructure and other applications.\nLeverage cutting-edge data engineering technologies to continually improve the speed, reliability, and scalability of Flowdesk's data processing capabilities.\nDevelop and maintain strong processes and procedures for data quality control, data validation, data documentation and, data integration.\nCollaborate with cross-functional teams including traders, developers, and compliance officers to ensure that all data used by Flowdesk is accurate, timely, and compliant with relevant regulations and requirements.\nRequirements\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n3+ years of experience in data engineering or related field.\nAwareness of software, data, and analytics engineering best practices (e.g. programming standards, data modelization, code idempotency....)\nGeneral understanding of systems architecture and concepts (distributed computing, lake-house architecture, ci/cd workflows...)\nExperience optimizing modern data warehousing platforms (BigQuery is a plus).\nStrong communication skills and ability to work collaboratively in a fast-paced international environment.\nKnowledge of the data engineering ecosystem (contribution to open source projects is a plus).\nStrong analytical and problem-solving skills with a keen attention to detail.\nBenefits\nüåç International environment (English is the main language)\nüöÉ 50% of transportation costs & a sustainable mobility agreement\nüçî Swile lunch voucher (‚Ç¨9.25 per day, 60% covered)\nüè• 100% Alan Blue covered for you and your children\nüíª Top of the range equipment{{:}} Macbook, keyboard, laptop stand, 4K monitor & headphones\nüéâ Team events and offsites\nüîú Coming soon {{:}} gym memberships, international mobility & lot of other cool benefits !\nRecruitment process\nüëÄ Are you interested in this job but feel you haven't ticked all the boxes? Don't hesitate to apply and tell us in the cover letter section why we should meet!\nüìù Here's what you can expect if you apply{{:}}\nHR interview (30')\nTechnical test\nTechnical interview (60')\nChat with the Head of People (30') and the Head of Department (30')\nOn the agenda{{:}} discussions rather than trick questions! These moments of exchange will allow you to understand how Flowdesk works and its values. But they are also (and above all) an opportunity for you to present your career path and your expectations for your next job!\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "SFR",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-sfr-3879318123?position=15&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SFaLXT0NP%2F1pgJi2pSe0Og%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "En tant que Data Ing√©nieur exp√©riment√©, vous occuperez un r√¥le essentiel dans notre √©quipe Data Science.\nVous serez responsable de la conception, du d√©veloppement et de la maintenance des pipelines de donn√©es ainsi que de l'int√©gration de sources de donn√©es multiples.\nVotre expertise sera cruciale pour garantir une gestion efficace des flux de donn√©es, ainsi que pour faciliter l'analyse et la visualisation des donn√©es en plus du support aux data scientists vos missions seront les suivantes :\nArchitecture projet des donn√©es\n: Concevoir et d√©velopper des architectures projet de donn√©es robustes, √©volutives et performantes pour int√©grer et g√©rer de grandes quantit√©s de donn√©es provenant de sources multiples. Assurer la fiabilit√©, l'√©volutivit√© et la s√©curit√© des flux de donn√©es entrant d‚Äôun projet Data Science.\nInt√©gration des donn√©es\n: √âlaborer des pipelines de donn√©es efficaces pour l'extraction, la transformation et le chargement des donn√©es (via notre Framework ELT/ETL interne) provenant de diff√©rentes sources. Mettre en place des processus d'int√©gration automatis√©s et veiller √† la qualit√© des donn√©es.\nGestion des bases de donn√©es\n: Concevoir et optimiser des bases de donn√©es pour r√©pondre aux besoins analytiques et de reporting. Assurer la performance, la disponibilit√© et la s√©curit√© des bases de donn√©es, ainsi que la gestion efficace des requ√™tes.\nCollaboration interfonctionnelle\n: Support des Data Scientists, vous travaillerez avec les √©quipes business pour comprendre leurs besoins et fournir des conseils et des recommandations bas√©s sur les donn√©es.\nOptimisation des performances\n: Surveiller et optimiser les performances des pipelines de donn√©es, des bases de donn√©es et des requ√™tes. Identifier les goulots d'√©tranglement et les points d'optimisation, et proposer des am√©liorations pour garantir des performances optimales.\nS√©curit√© et conformit√©\n: Veiller √† ce que les donn√©es soient trait√©es et stock√©es conform√©ment aux normes de s√©curit√© et de confidentialit√©. Mettre en place des m√©canismes de s√©curit√© pour prot√©ger les donn√©es sensibles et garantir la conformit√© aux r√©glementations en vigueur.\nVotre profil :\nVous avez un\nDipl√¥me universitaire en informatique, en g√©nie logiciel, en science des donn√©es ou dans un domaine connexe et vous avez √† minima 5 ans d'exp√©rience en tant que Data Ing√©nieur.\nVous poss√©dez √©galement une solide ma√Ætrise des technologies et des outils suivants :\nHadoop, Spark, SQL, Kafka, GCP BigQuery,\nDe plus vous avez une bonne compr√©hension des architectures, des mod√®les et des concepts de base de donn√©s avec une exp√©rience avanc√©e dans la mise en ≈ìuvre de pipelines ETL et dans la gestion de bases de donn√©es.\nVos connaissances en mati√®re de s√©curit√© des donn√©es, de conformit√© aux r√©glementations ainsi que vos comp√©tences en programmation scripting et en d√©veloppement logiciel seront un plus.\nVos excellentes comp√©tences en communication seront des qualit√©s appr√©ci√©es et\nun niveau d'anglais (appliqu√©e au domaine technique) est un plus.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "RSight¬Æ",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-rsight%C2%AE-3856216625?position=16&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=qLSCutHr%2FcwFsKvRnygi3g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Nous recherchons pour notre client, un\nleader mondial des services et conseils en technologies\n, un\ning√©nieur Databricks et Data Factory\nqui rejoindra une √©quipe qui combine des comp√©tences m√©tiers avec une forte expertise data, analytique et d‚Äôintelligence artificielle pour mettre en ≈ìuvre des solutions qui visent √† am√©liorer la gestion et la valorisation des donn√©es.\nDescriptif des missions:\nVous √™tes int√©ress√© √† travailler sur une solution ayant un impact direct sur les ambitions de notre client en mati√®re de data (datadriven, data d√©mocratisation) ? Alors devenez membre de l‚Äô√©quipe Corporate Data Lake de notre client ! Comme tout autre membre de l'√©quipe, vous :\nParticiper √† la d√©finition des composants informatiques supportant la fourniture de services\nD√©velopper, tester, industrialiser et d√©ployer des composants en minimisant les impacts sur les utilisateurs (automatisation, 0 temps d'arr√™t,...)\nDocumenter la bonne utilisation des services\nD√©ployer et supporter nos fonctionnalit√©s sur la plateforme\nApporter assistance et conseils aux utilisateurs m√©tiers\nOp√©rer la solution en op√©ration courante (incluant le suivi de la qualit√© des services) et intervenir dans la r√©solution des incidents\nParticiper activement √† l'am√©lioration continue des activit√©s de l'√©quipe\nExpliquer aux collaborateurs ce que le Corporate Data Lake peut faire pour eux\nConfigurer des espaces de travail pour eux\nFournir du coaching et de l'expertise lors de r√©unions en face √† face ou sur les canaux communautaires\nParticiper √† l'effort de support de la plateforme dans une approche \"vous la construisez, vous l'ex√©cutez\"\nContribuer aux premi√®res phases de conception d√©finissant l'avenir du Corporate Data Lake\nComp√©tences:\n1er exp√©rience Azure (PaaS et IaaS)\nConnaissance de Databricks et Data Factory\nMa√Ætrise d'un ou plusieurs langages parmi : Python, Scala, Spark, PowerShell\nInt√©gration et livraison continues (Jenkins, Azure Devops, GIT Lab CI, ‚Ä¶)\nPratique des fondamentaux du g√©nie logiciel (Gestion de Configuration, Tests,...)\nAnglais : √† l'aise pour assister √† une r√©union et r√©diger de la documentation technique\nBonne capacit√© d'√©coute, orientation client/utilisateur\nExpression orale et √©crite adapt√©e √† l'interlocuteur\nCuriosit√© et adaptation aux changements technologiques\nB√©n√©fices:\nUn processus de recrutement court, un accompagnement personnalis√©, une √©volution qui s'adapte √† votre trajectoire de carri√®re.\nEn plus de votre quotidien li√© √† votre mission, vous pourrez entreprendre, √™tre form√©, passer des certifications.\nPlan d'√©pargne pour la retraite collectif, mutuelle, tickets restaurant, des cong√©s d'anciennet√©, un catalogue CE, des accords d‚Äôentreprise relatifs au t√©l√©travail et √† la parentalit√© et autres avantages.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Other": [
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "V√©lizy-Villacoublay, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-thales-3908228180?position=17&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=NNrRMFzlfyvjpi8zEBQidg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs pr√©sents sur tous les continents. Le Groupe investit dans les innovations du num√©rique et de la ¬´ deep tech ¬ª ‚Äì big data, intelligence artificielle, connectivit√©, cybers√©curit√© et quantique ‚Äì pour construire un avenir de confiance, essentiel au d√©veloppement de nos soci√©t√©s, en pla√ßant l‚Äôhumain au c≈ìur des d√©cisions.\nThales propose des solutions, services et produits qui aident ses clients ‚Äì entreprises, organisations, Etats ‚Äì dans cinq grands march√©s vitaux pour le fonctionnement de nos soci√©t√©s : identit√© et s√©curit√© num√©riques, d√©fense, a√©ronautique, espace, et transport.\nQUI ETES-VOUS ?\nDipl√¥m√© d‚Äôun Bac+5 en √©cole d‚Äôing√©nieur ou √©quivalent universitaire avec une sp√©cialisation en informatique, vous avez au moins 3 ans d'exp√©rience dans les technologies Big Data.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que Data Engineer, vous jouerez un r√¥le cl√© dans la conception, le d√©veloppement et la maintenance de notre infrastructure de donn√©es, ainsi que dans la transformation et la gestion des flux de donn√©es.\nVOS MISSIONS :\n‚Ä¢ Concevoir, d√©velopper et d√©ployer des solutions Big Data en utilisant les technologies Hadoop.\n‚Ä¢ Mettre en place des pipelines de donn√©es performants pour l'ingestion, le traitement et le stockage des donn√©es massives.\n‚Ä¢ Collaborer √©troitement avec les √©quipes m√©tier pour comprendre leurs besoins en mati√®re d'analyse de donn√©es et proposer des solutions adapt√©es.\n‚Ä¢ Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des donn√©es.\n‚Ä¢ Assurer la qualit√© et la fiabilit√© des donn√©es trait√©es, en mettant en place des processus de validation et de nettoyage.\n‚Ä¢ Identifier et r√©soudre les probl√®mes li√©s √† l'infrastructure Big Data et proposer des am√©liorations.\n‚Ä¢ Travailler en √©troite collaboration avec les Data Scientists et les Data Analysts pour fournir des insights pertinents √† partir des donn√©es.\nInnovation, passion, ambition : rejoignez Thales et cr√©ez le monde de demain, d√®s aujourd‚Äôhui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "BigData": [
                "Hadoop"
            ],
            "Other": [
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "SQLI",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-gcp-h-f-at-sqli-3849296046?position=18&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=Lx91k5AJ27v1%2BCUIKqVSjw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Rejoignez\nSQLI\net faites partie de l'√©quipe Data, au sein d‚Äôune soci√©t√© √† taille humaine, mais avec de grandes ambitions. Nous sommes plus de 2200 talents sur 13 pays et 3 continents.‚Äã\nVotre futur √©cosyst√®me :\nAu sein du p√¥le Data de SQLI\n, pour rejoindre une √©quipe dynamique de +40 passionn√©s.\nDes projets digitaux pour des clients grands comptes\n: notamment dans les secteurs des services financiers, de l'industrie et du retail.\nUne organisation orient√©e delivery\n: vous travaillerez au sein de nos locaux ou en √©quipe int√©gr√©e chez nos clients en mode 100% agile, en mode projet.\nPossibilit√© de t√©l√©travail jusqu'√† 3 jours par semaine\nDes communaut√©s d‚ÄôExperts\n, pour vous aider √† progresser‚Äã, avec des workshops et ateliers techniques favorisant le partage de connaissances.‚Äã\nDes Managers de carri√®re\n, pour √™tre suivi par l'un de vos pairs sur l'entit√© Data, avec un\naccompagnement dans l‚Äôexpression de vos talents\n(certifications et formations notamment via le partenariat Solutions de Microsoft, participation aux √©v√®nements/salons, publications dans la presse...).\nDescription du poste :\nUn poste de\nData Engineer GCP (H/F)\nest ouvert √†\nPARIS ou ROUEN\n(selon votre localisation)\n, pour faire partie de l‚Äô√©quipe Data chez SQLI et vous investir dans un environnement technique innovant.\nVos missions seront :\nL'analyse et la compr√©hension des besoins m√©tiers.\nLa participation √† la d√©finition et √† la conception de l‚Äôarchitecture.\nLa r√©alisation des pr√©sentations, d√©monstrations, POC ou Pilotes pour mettre en lumi√®re les recommandations technologiques.\nLes d√©veloppements de jobs d‚Äôalimentation (pr√©paration, ingestion, traitement et contr√¥le qualit√©) et l'automatisation des flux d‚Äôalimentation du Data Lake et du Datawarehouse\nLes tests de charge, tests unitaires‚Ä¶\nLa maintenabilit√© de la solution Big Data/BI : optimisation et performance des traitements.\nQualifications :\nIng√©nieur(e) de formation, avec minimum\n3 ans d‚Äôexp√©rience sur des projets Google Cloud Platform (BigQuery, Dataflow, ...)\nToujours en veille, √† l‚Äôaffut des nouveaut√©s technologiques et vous aimez √©changer (Events, conf√©rences, meetups, etc‚Ä¶).\nForce de proposition, vous vous sentez libre d‚Äôoser et de vous surpassez en partageant vos id√©es.\nComp√©tences techniques requises :\nMa√Ætrise d'un langage de programmation\n(Python, Java, R, Spark, Scala).\nMa√Ætrise de\nSQL.\nUne exp√©rience sur au moins un ETL/ELT\n(Talend, DBT).\nBonne connaissance des outils et framework d‚Äôindustrialisation\nCI/CD\net/ou gestion de version (Gitlab).\nSerait un plus : une exp√©rience sur Power BI, TIBCO EBX et/ou BO DS + la gestion de Conteners et Kubernetes (GKE).\nVous pensez que ce poste est fait pour vous ? Transmettez-nous votre profil !\nTous nos postes sont ouverts aux personnes en situation de handicap.\nDes questions sur vos donn√©es personnelles ? Retrouvez notre politique de confidentialit√© concernant les candidats :\nhttps://www.sqli.com/sites/default/files/2024-01/SQLI-PRIV-Politique-Confidentialite-Candidats-C0-29012024.pdf\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LCL",
        "location": "Villejuif, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-profils-exp%C3%A9riment%C3%A9s-h-f-at-lcl-3888403052?position=19&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=P1%2BGTBi8FCAuG8zwrP4fXA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üè¶ LCL, c‚Äôest LA banque urbaine du Groupe Cr√©dit Agricole - avec nous, accompagnez la transformation, le d√©veloppement et le maintien technologiques de nos outils avec une vision business et de satisfaction de nos 6 millions de clients.\nEn tant qu‚Äôacteur majeur de la banque de d√©tail, nous nous adaptons chaque jour aux nouveaux modes de consommation et les projets de nos de clients internes et externes tout en garantissant le besoin de s√©curit√© et de d√©veloppement technologique qu‚Äôimpliquent nos activit√©s.\nüí°Organis√©es en mode Agile, les 8 squads de la tribu DATA (6 squads M√©tier et 2 squads transverses) ≈ìuvrent au quotidien pour r√©pondre √† un enjeu majeur pour la banque : la collecte, le stockage, la gestion et l‚Äôusage de la donn√©e. En interaction permanente avec les autres tribus IT et les m√©tiers, elles √©tudient et proposent les solutions et architectures √† d√©ployer pour r√©pondre au mieux aux strat√©gies de d√©veloppement et de pilotage de l‚Äôensemble des m√©tiers de la banque.\nRejoignez-nous si vous souhaitez participer aux r√©flexions et au d√©veloppement de la trajectoire technique et DataCentric du SI LCL et plus largement du Groupe CA. Vous c√¥toierez et serez au c≈ìur de l‚Äôimpl√©mentation de technologies vari√©es telles que les plateformes Teradata, les solutions d‚Äôarchitecture applicative des technologies BigData ou IA, des environnements analytiques ou encore des solutions de datavisualisation. Vous assurerez le traitement de donn√©es en temps r√©el ou en batch et exposerez les donn√©es sous diff√©rentes formes.\nQue vous souhaitiez devenir expert sur les socles technologiques ou relever le challenge de la gestion de projets M√©tier, nous vous aiderons √† atteindre vos propres objectifs.\nVous rejoindrez une √©quipe pluridisciplinaire, clairement orient√©e vers le d√©veloppement de ses collaborateurs √† de nouvelles technologies !\nüéØ En tant que Data Engineer :\n¬∑ Vous aimez analyser les besoins avec les m√©tiers, challenger, identifier les sources de donn√©es dans les diff√©rents univers technologiques, industrialiser des algorithmes, concevoir et d√©velopper des Datalab ou des Datamart sur les plateformes ? Vous saurez relever les challenges propos√©s par les squads m√©tier !\n¬∑ Vous pr√©f√©rez travailler √† l‚Äôarchitecture et au d√©ploiement de nouvelles plateformes, √† la lev√©e de la dette technologique ou encore r√©aliser de la veille au service de notre trajectoire ? La squad Socles Data est faite pour vous !\n¬∑ Au-del√† des projets que vous g√©rerez, garant du bon fonctionnement de votre parc applicatif, vous attacherez une grande attention √† la mise en ≈ìuvre de solutions optimis√©es.\n¬∑ La rigueur, la communication, l‚Äôesprit d‚Äô√©quipe mais aussi la curiosit√© et la cr√©ativit√© font partie de vos soft skills ! ils vous permettront de r√©pondre aux enjeux de s√©curit√©, de qualit√©, de transmission de la connaissance et contribueront √† l‚Äôatteinte des objectifs de l‚ÄôIT et plus largement de LCL, au service de ses clients.\nüíª Voici les principales technologies utilis√©es au sein de la tribu, si certaines vous sont famili√®res, nous vous aiderons √† monter en comp√©tence sur d‚Äôautres !\nLangages utilis√©s : SQL, Python, Scala\nSGBD : Teradata et utilitaires (TPT, BTEQ, ‚Ä¶)\nStreaming : Kafka\nSearch : ElasticSearch, SolR\nEnvironnement : Unix\nSolutions Big Data : Hadoop Cloudera, DataIku, HDFS, Hive, Impala,\nDevops : GitLab, Jenkins, Nexus\nOutils de visualisation : MS BI (SSIS, SSAS, SSRS) Qlik Sens, BO\nMod√©lisation : MEGA\nOutils collaboratifs : GIT, Jira, Confluence, Teams\n‚ö°Si les nouveaux enjeux bancaires vous int√©ressent, que vous souhaitez int√©grer une √©quipe Agile au service des m√©tiers dans laquelle vous serez force de proposition et que vous aimez travailler dans un environnement motivant et dynamique, rejoignez-nous, cette offre est faite pour vous !\nüî• Les + de notre entreprise :\nAcc√®s au Plan d‚Äô√©pargne Groupe, int√©ressement et participation aux b√©n√©fices de l‚Äôentreprise + abondement\nPrix pr√©f√©rentiels bancaires et avantages CSE\nParcours √©volutif dans l‚Äôentreprise et/ou dans le Groupe CA.S.A\nT√©l√©travail (jusqu'√† 2 jours de t√©l√©travail par semaine)\nDe multiples commodit√©s sur le campus (restaurants d'entreprise, salle de sport, cr√®che, centre m√©dical, m√©diath√®que...)\nForfait et avantages pratiques ¬´ mobilit√© durable ¬ª pour les velotafeurs\nDes √©quipes aussi diversifi√©es que structur√©es dans une dynamique de transformation\nLCL s‚Äôengage en faveur de la diversit√© et nous encourageons tout(e) candidat(e) ayant l‚Äôexp√©rience requise √† postuler √† nos offres. Tous nos postes sont ouverts aux personnes en situation de handicap.\nNous avons encore de nombreuses raisons √† vous pr√©senter pour vous convaincre de nous rejoindre mais pour cela, il faudra postuler ici !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL"
            ],
            "BigData": [
                "Hadoop"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "Collaboration": [
                "Confluence",
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "iPepperGroup",
        "location": "Valbonne, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ipeppergroup-3894091360?position=20&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=RFFhGqjJZXW%2B3KaXucElnA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "iPepper recrute pour l'un de ses clients une PME √©diteur de logiciel dans le domaine du voyage, un\nData Engineer (H/F)\npassionn√©(e) et exp√©riment√©(e) pour rejoindre une √©quipe dynamique.\nEn tant qu'Ing√©nieur(e) Data, vous serez en charge d'extraire et de transformer des donn√©es, de construire et d'optimiser des pipelines de donn√©es, ainsi que de concevoir des visualisations de donn√©es intuitives et informatives.\nResponsabilit√©s :\nConcevoir, construire et maintenir des pipelines de donn√©es √©volutifs et efficaces pour transf√©rer des donn√©es entre des bases de donn√©es SQL et NoSQL.\nD√©velopper et mettre en ≈ìuvre des processus ETL pour extraire, transformer et charger des donn√©es √† partir de diff√©rentes sources dans notre entrep√¥t de donn√©es.\nCollaborer avec des √©quipes pluridisciplinaires pour comprendre les besoins en donn√©es et garantir la fourniture r√©ussie de solutions de donn√©es.\nOptimiser et ajuster les pipelines de donn√©es existants pour la performance et la fiabilit√©.\nConcevoir et d√©velopper des visualisations de donn√©es et des tableaux de bord pour fournir des insights exploitables aux parties prenantes.\nSurveiller et r√©soudre les probl√®mes de pipelines de donn√©es, en veillant √† la qualit√© et √† l'int√©grit√© des donn√©es.\nProfil recherch√© :\nDipl√¥me universitaire en informatique, en ing√©nierie ou dans un domaine connexe.\nExp√©rience av√©r√©e en tant que Data Engineer ou dans un r√¥le similaire, avec un accent particulier sur la construction de pipelines de donn√©es et de processus ETL.\nCompr√©hension solide des bases de donn√©es\nSQL\net\nNoSQL\n, y compris la mod√©lisation des donn√©es et la conception de sch√©mas.\nMa√Ætrise des langages de programmation tels que\nPython, Java ou Scala.\nExp√©rience avec des outils de visualisation de donn√©es tels que\nTableau, Power BI.\nSolides comp√©tences en analyse et en r√©solution de probl√®mes, avec la capacit√© de traduire des donn√©es complexes en insights exploitables.\nExcellentes comp√©tences en communication et en collaboration, avec la capacit√© de travailler efficacement dans un environnement d'√©quipe pluridisciplinaire.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "PROXIAD",
        "location": "Nice",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-proxiad-3901014428?position=21&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=JbkEw8N73f2WHgJMdBa5fg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Contexte\nEn tant que Data Engineer, votre r√¥le consistera √† r√©aliser la conception, le d√©veloppement, les tests unitaires, la qualification, l'int√©gration continue et la mise en production d'√©volutions sur les projets du p√¥le produits scoring.\nCes projets Big Data GCP ont pour objet de d√©velopper des traitements de croisement de donn√©es, exploration data en mode agile (scrum et Safe), industriel (respect de normes), sur l'environnement Google Cloud Platform.\n1 : Conception\nSp√©cification et conception d'une solution se basant sur les d√©veloppements existants.\nMettre en question les choix techniques dans le but de concevoir un logiciel r√©pondant au mieux √† la demande au moindre co√ªt et avec la qualit√© demand√©e.\nConception de l'expression de besoins, de la r√©ponse √† l'expression de besoins √† l'aide des besoins m√©tiers remont√©s par le Product Owner.\n2 : R√©alisation\nD√©veloppement de nouvelles fonctionnalit√©s sur les composants des applications du p√¥le produits scoring en environnement GCP (DataProc, GCS, BigQuery, Airflow...)\nTests des d√©veloppements r√©alis√©s\nRevue de code des d√©veloppements des autres d√©veloppeurs\nMise en production via CICD des d√©veloppements\n3 : Suivi du RUN applicatif\nPrendre en charge avec les autres membres de l'√©quipe le RUN des applications du p√¥le produits scoring. Cela inclus les t√¢ches de rapport quotidien, la gestion des probl√®mes applicatifs, le soutien aux utilisateurs.\nComp√©tences attendues\nMa√Ætrise op√©rationnelle :\nConfluence\nImpl√©mentation de l‚Äôint√©gration continue (Utilisation de la chaine CI/CD existante )\nConnaissance des principes DevOps\nJira\nAnglais (lu, √©crit)\nMa√Ætrise avanc√©e :\nElaborer un cahier de recette\nBig Query\nSp√©cifications technique et documentation\nD√©veloppement :Python, SQL, Scala, Javascript, GitLab\nExpertise\nGCP : Exp√©rience significative en tant que Data Engineer Cloud. Mise en pratique des produits GCP et en particulier Dataproc, Big Query, composer, workflow, PubSub\nD√©veloppement : Java\nCompr√©hension g√©n√©rale des travaux BigData et du profiling\nInformations compl√©mentaires :\nT√©l√©travail 2 jours par semaines\nR√©mun√©ration aux alentours des 45K‚Ç¨\nExp√©rience requise : 6 ans\nLocalisation : Mougins\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "6 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DBMS": [
                "Big Query",
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "Collaboration": [
                "Confluence",
                "JIRA"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Chantelle",
        "location": "Cachan, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-chantelle-3909775663?position=22&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=6A0p8tASG3TahyMB1aX0EA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La Direction des Syst√®mes d'Information et du Digital du groupe Chantelle recherche son/sa futur.e Data Engineer H/F, pour le lancement du grand chantier de r√©novation de l'architecture Data : la bascule de l'int√©gralit√© de son Data Warehouse vers Google Big Query.\nNous souhaitons recruter un Data Engineer H/F confirm√©.e, charg√©.e de contribuer √† la d√©finition de la feuille de route de la Chantelle Data Plaform. En tant que Data Engineer vous travaillerez au sein de l'√©quipe Data Int√©gration en charge de la Chantelle Data Platform.\nVos Missions :\n- Mettre en ≈ìuvre une infrastructure autour de Google Cloud Platform permettant de collecter (airbyte, API, ...) , transformer (dataform, Bigquery ...), exposer (dataviz, API, applications, ...) et historiser les donn√©es g√©n√©r√©es par l'entreprise.\n- Travailler en √©troite proximit√© avec les responsables des diff√©rents domaines fonctionnels (R√©f√©rentiels, Supply Chain, Manufacturing, B2B, Retail & e-commerce, Finance, ...), avec notre √©quipe de Data Analysts ainsi qu'avec l'√©quipe technique en charge des infrastructures transverses\n- √ätre force de proposition sur tous les sujets d'architecture et de mod√©lisation (choix de mise en place de pipeline temps r√©els ou au contraire de flux de donn√©es en mode batch, ou bien encore stockage sur Big Query / Big Table en fonction des cas d'usage).\n- D√©finir les √©l√©ments structurants, en justifiant vos choix, et les mettre en ≈ìuvre.\n- Rationaliser et moderniser notre architecture d'int√©gration inter-applicative; se projeter sur la cr√©ation d'un mod√®le de donn√©es de type Datamesh.\n- Faire la refonte de la BI de nombreux use cases tels que le pilotage de nos stocks, personnalisation de nos sites e-commerce en temps r√©el en fonction de nos profils client, etc‚Ä¶\nStack technique : Google Cloud Platform, BigQuery, DataForm, DataFlow, PubSub, Airbyte, Github ...\nBonne ma√Ætrise des langages Python et SQL\nPourquoi travailler chez Chantelle ?\nUne flexibilit√© dans votre lieu de travail, selon la politique de t√©l√©travail de l'entreprise.\n11 jours de RTT/an ainsi qu'un 13√®me mois.\nUne culture d'entreprise familiale bas√©e sur des valeurs de respect, de cr√©ativit√©, de durabilit√© et de transparence\nUne aventure dans laquelle vous pourrez vous √©panouir, apprendre et entreprendre, avec une grande vari√©t√© de missions et beaucoup d'autonomie\nDes √©quipes ressources humaines et des managers √† votre √©coute pour vous accompagner dans votre parcours professionnel\nDes r√©ductions sur nos produits et des ventes au personnel\nDes avantages dans votre qualit√© de vie au travail : une conciergerie compl√®te proposant un large panel de services, des activit√©s en interne, un CSE.\nVous souhaitez rejoindre un Groupe familial, innovant, engag√© et leader dans son secteur en France comme √† l'international et vous souhaitez apporter votre expertise et authenticit√© pour guider votre √©quipe vers le succ√®s : postulez et rejoignez le Groupe Chantelle !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirm√©"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DBMS": [
                "Big Query",
                "BigQuery"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "EarthDaily Agro",
        "location": "Balma, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-earthdaily-agro-3883708013?position=23&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=Z1Bn3iRSRmq3lgA1wQOI9w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "About Us\nEarthDaily Agro provides space age data and analytics to the organizations and people who feed the planet!\nWith 35 years of industry experience, EarthDaily Agro provides customers with the data, analysis and knowledge they need to make more efficient and effective decisions. B2B services range from global risk management and monitoring of agricultural commodities to the marketing of inputs and precision agriculture consulting, using the latest research in agronomy, information technologies and remote sensing.\nEarthDaily Agro also develops highly customized business solutions for agricultural lenders, insurers, input suppliers and food companies, with easy-to-use analytics, that help reduce the daily risks of agriculture.\nEarthDaily Agro is headquartered in Minneapolis, MN, USA, with offices in France, Brazil, Australia and Switzerland and is a division of EarthDaily Analytics Corp.\nEarthDaily Analytics Corp., a vertically-integrated data processing and analytics company, is launching a new constellation of earth observation satellites. The EarthDaily satellite constellation will significantly enhance geospatial analytics capabilities in agriculture, forestry, environment, financial services, and intelligence, among many other verticals.\nMain Job Tasks And Responsibilities\nAs a EarthDailyAgro Data Engineer, your primary responsibility will be to design, develop, and manage data pipelines and infrastructure specialized for geospatial and remote sensing applications. You will work closely with data scientists, geospatial analysts, remote sensing experts, software engineers, and DevOps teams to ensure the successful deployment and scaling of data pipeline to feed geospatial data machine learning models. Your role will be crucial in optimizing the geospatial machine learning ecosystem and ensuring the seamless integration of AI-driven geospatial solutions into real-world applications.\nYour Responsibilities Include\nCloud-based data pipeline Conceptualization, Development and Scaling: Build up pipeline to ingest large volumes of geospatial data, pre-process them and meet data scientists‚Äô requirements, in terms of accessibility, speed, format, quality.\nAutomation and CI/CD: Industrialization of pipeline deployment, orchestration, workflows, and versioning.\nCost & Speed Optimization: Collaborate with infrastructure team to develop, optimize, and fine-tune pipeline.\nCloud and Containerization: Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization tools.\nInfrastructure Management: Utilize containerization technologies and cloud-based services to set up and manage infrastructure, enabling seamless deployment and scalability.\nMonitoring and Anomaly Detection: Implement monitoring systems to track pipeline performance and identify anomalies.\nVersion Control and Data Version Control: Proficient with version control systems like Git and DVC.\nSecurity and Compliance: Ensure the security and privacy of geospatial data, adhering to relevant data protection regulations and industry best practices.\nCollaboration and Communication: Collaborate with interdisciplinary teams to integrate data pipeline into existing applications or develop new geospatial products.\nIssue Resolution and Troubleshooting: Identify and resolve promptly technical issues related to geospatial data processing, performance, or infrastructure.\nEducation, Knowledge And Abilities\nRequirements\nEducation: Master's degree in Computer Science, specialisation in Geomatics and/or Remote sensing would be a plus.\nExperience: 3+ years experiences with data pipeline processes and deployment is a must-have. Proven hands-on experience in setting up pipelines and data processes with opensource tools (e.g., MLFlow, Argo, Kubeflow) is desirable.\nProgramming Skills: Proficiency in Python and with data manipulation frameworks (e.g., dataframe, numpy, pandas, xarray, rasterio) and librairies (e.g., Dask).\nProblem-Solving Skills: Autonomous, and strong analytical and problem-solving abilities to address complex geospatial data and analysis challenges.\nCommunication Skills: Excellent communication and interpersonal skills to collaborate effectively with cross-functional teams and stakeholders.\nFrench mandatory (job based in France). Fluent in English (oral and written):‚ÄØmeetings with internal are mostly in‚ÄØEnglish.\nPreferred Additional Skills\nExperience with Earth Observation (EO) data analysis and processing.\nExperience with geospatial data formats (e.g., GeoTIFF, Shapefile, NetCDF).\nSpatial Analysis Techniques: Understanding of spatial analysis techniques and algorithms commonly used in geospatial data manipulation.\nRemote Sensing Integration: Knowledge of remote sensing data sources (e.g., STAC catalog, satellite imagery, LiDAR, SAR) integration into data pipelines for accurate and up-to-date geospatial analysis.\nCONDITIONS\nFull time job based in Balma, near Toulouse, France.\nFixed + Bonuses\nTR / \"Family\" insurance / CSE\nPowered by JazzHR\nWrfSXQ5YJg\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataAnalytics": [
                "NumPy",
                "Pandas"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Interpersonal Skills",
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Digital Waffle",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3913824888?position=24&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=sYvCEinWvVGScYf9pk8tqA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Digital Waffle is proud to have partnered with an innovative tech startup in Paris, who are looking for a talented Data Engineer to join their growing team!\nThey are made up of a powerhouse of experts, combining\ndata engineers, business process gurus, and Project managers\nwho leverage the most advanced solutions available; utilising process mining, automation tools, and smart execution systems.\nLooking for an experienced Data Engineer (3-5 years)\nWhat You'll Do:\nThis is a full-time,\nhybrid role (Paris-based)\nwhere you'll wear many hats: data exploration, system integration, data prep, data modeling, and implementing data solutions.\nExperience:\nExpertise in data engineering, data modeling, and ETL (Extract, Transform, Load) processes\nData warehousing and data analytics skills\nExperience handling large, complex datasets\nProficiency in SQL and programming languages like Python or Java\nStellar problem-solving and analytical skills\nTop-notch communication and collaboration abilities\nBachelor's or Master's in Computer Science, Information Systems, or a similar field (a plus for process mining or intelligent process automation experience)\nIf you are an experienced and driven Data Engineer, please apply here!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896992742?position=25&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=lPc66SjHIzg51G%2BwgpRsRw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Boulanger",
        "location": "Lesquin, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-boulanger-3854554057?position=26&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=M3pdzGobBgvn0n9%2B6lfQ%2Bw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Au sein de la direction informatique, le p√¥le DATA a pour missions de maximiser la mise en valeur des donn√©es de BOULANGER ,ELECTRO-DEPOT et KREFEL/HIFI afin d‚Äôaider nos d√©cideurs √† agir sur les leviers de leur performance par des processus d√©cisionnels efficients.\nAu sein de ce p√¥le, tu prendras en charge un large domaine m√©tier qu'il te faudra maitriser de bout en bout : de la donn√©es brutes, sa transformation jusqu'√† son exposition dans les reporting. Cela afin d'en assurer le bon fonctionnement, les √©volutions constantes et sa p√©rennit√©\nTes t√¢ches principales portent sur :\nLe pilotage et la mise en ≈ìuvre de projets DATA.\nLa collecte, le stockage et l‚Äôexploitation fluides des donn√©es par le d√©veloppement de solutions\nMissions\nMaitriser les r√®gles fonctionnelles et les KPI de ton domaine afin de challenger les m√©tiers dans les √©volutions et les nouveaux projets\nAccompagner des √©quipes m√©tiers dans leurs travaux d‚Äôidentification et expression des besoins sur la data\nParticiper aux ateliers de conception et d√©veloppement des applications data\nMod√©liser la solution √† mettre en ≈ìuvre\nConcevoir et mettre (ou faire mettre) en ≈ìuvre des flux les pipelines d‚Äôint√©gration (en mode batch ou fil de l'eau) de donn√©es structur√©es/semi-structur√©es\nTransformer les donn√©es : consolider, enrichir et optimiser les donn√©es, qui seront exploit√©es par le m√©tier\nCr√©er, faire √©voluer et optimiser les restitutions\nSuivre et animer les d√©veloppeurs (ETL, restitution, self-BI internes ou externes)\nG√©rer le RUN\nMaitrise le SQL et la base de donn√©es (Oracle, Snowflake)\nMa√Ætrise d‚Äôoutils de restitution (tel que Business Object (BO), PowerBI‚Ä¶)\nCapacit√© relationnelle, rigueur et dynamisme\nMa√Ætrise un ou plusieurs outils de pr√©paration et traitement de la donn√©e (DataStage, Stambia, ...)\nCapacit√© √† s‚Äôadapter √† tout type d‚Äôinterlocuteurs (technique, m√©tiers, Direction)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Neosoft",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=27&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=n7iKAdkp4FW8O5QNXJuUyg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Tous nos postes sont ouverts au t√©l√©travail\nGroupe ind√©pendant de conseil en transformation digitale de pr√®s de 1800 collaborateurs, N√©osoft s‚Äôest construit, depuis 2005, sur un mod√®le qui place l‚Äôexcellence, le d√©passement de soi et la RSE au c≈ìur de sa strat√©gie.\nEn nous rejoignant, vous int√©grez des communaut√©s d‚Äôexperts et de talents qui vous permettent de d√©velopper vos comp√©tences et d‚Äôoffrir √† nos clients le meilleur accompagnement possible.\nNotre savoir-faire s‚Äôarticule autour de nos 6 domaines d‚Äôexpertise :\nConseil & Agilit√©\nCybers√©curit√©\nData\nDevOps\nInfrastructures & Cloud\nSoftware Engineering\nNous recherchons pour int√©grer notre\nagence lilloise\nun(e)\nData Engineer confirm√©(e)\n.\nNous aimerions vous voir rayonner au sein de notre communaut√© DATA (+100 collaborateurs) anim√©e par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients √† consolider un patrimoine Data responsable.\nüéØ\nVos missions :\nApr√®s une p√©riode d‚Äôint√©gration, en tant que\nData Engineer\n, voici √† quoi ressembleront vos activit√©s dans des contextes clients Retail ou Banque / Assurance / Finance :\nAnalyser et s'approprier les cas d'usages\nAnalyser et valoriser les donn√©es du patrimoine\nMettre en place des flux de transformation de donn√©es\nR√©aliser les tests permettant de s'assurer la qualit√© du delivery\nContinuer la mise au point de frameworks data\nCr√©er et d√©velopper des modules de d√©ploiement des solutions\nAssurer l'industrialisation de moteurs bas√©s sur l'IA\nAssurer le niveau de performance des pipelines\nImpl√©menter les outils de monitoring du socles de donn√©es\nüìù\nVotre profil :\nNous vous imaginons avec au moins 4 ans d‚Äôexp√©riences sur des projets autour de la\nData\n, une ma√Ætrise des\nbases de donn√©es (SQL)\n, des outils de transformation de la donn√©e\n(Talend, BigQuery, Airflow)\n, et un socle de comp√©tences solides autours des langages\nPython, Spark, Scala, Hadoop, Java.\nüëâ\nVotre carri√®re chez N√©osoft\nDepuis sa cr√©ation, N√©osoft place ses collaborateurs au c≈ìur de sa strat√©gie. Notre culture pourrait se r√©sumer en un mot : le collectif.\nNos communaut√©s d‚Äôexperts vous donnent la possibilit√© d‚Äôapprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.\nNous veillons √† ce que chacun b√©n√©ficie d‚Äôun accompagnement de proximit√© et d‚Äôun suivi de carri√®re personnalis√© aupr√®s de votre manager d√©di√© :\n1 bilan d‚Äôactivit√© trimestriel pour suivre le d√©veloppement de vos comp√©tences\n1 entretien d‚Äô√©valuation qui a lieu chaque ann√©e pour √©valuer votre performance et d√©terminer vos nouveaux objectifs\n1 entretien annuel aupr√®s de votre RH dans le but de cartographier vos nouvelles comp√©tences pour √©changer sur vos projets professionnels et souhaits de formations\nüëâ\nVos avantages\nFormations et d√©veloppement de l‚Äôexpertise :\nVous disposez de temps allou√© et r√©mun√©r√© en contribuant au d√©veloppement de votre expertise technique et de celle du groupe (Participations √† des Tech days, animation d‚Äôune conf√©rence √† l‚Äôinterne ou √† l‚Äôexterne, r√©daction d‚Äôarticles, rencontres avec nos candidats en processus de recrutement‚Ä¶)\nUn abonnement illimit√© LinkedIn Learning offert\nBien-√™tre au travail :\nUn accord de t√©l√©travail flexible jusqu‚Äô√† 100% de t√©l√©travail et personnalisable\nUn partenariat avec Gymlib qui favorise le sport en entreprise\nDes initiatives locales (afterworks, d√©fis sportifs, team buildings, ‚Ä¶)\nEt bien plus encore :\nParce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive r√©mun√©r√©e d√®s l‚Äôarriv√©e du collaborateur\nEn plus de votre salaire : participation, compte √©pargne temps, actionnariat...\nüëâ\nVotre parcours candidat\nNotre processus de recrutement se compose de deux √©tapes cl√©s :\nUn entretien de recrutement RH avec un Talent Acquisition Sp√©cialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carri√®re possibles au sein de notre groupe\nUn entretien d‚Äô√©valuation technique pour r√©aliser un diagnostic de vos comp√©tences techniques et identifier les comp√©tences sur lesquels poursuivre votre √©volution\nVous aurez √©galement la possibilit√© de rencontrer pour compl√©ter votre processus un acteur de notre p√¥le Business ou un pair de votre m√©tier pour √©changer sur son exp√©rience collaborateur.\nNous avons h√¢te de vous rencontrer !\nA bient√¥t,\nL‚Äô√©quipe N√©osoft üñê\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirm√©"
            ],
            "TypeContract": "",
            "Salary": "Salaire",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bordeaux-at-capgemini-3889788624?position=28&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=nIragHs8rRPTeomiah0NXg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise o√π vous serez en mesure de fa√ßonner votre carri√®re selon vos aspirations. Avec le soutien et l'inspiration d'une\ncommunaut√© d‚Äôexperts dans le monde entier, vous pourrez r√©√©crire votre futur. Rejoignez-nous pour red√©finir les limites de ce qui est possible, contribuer √† lib√©rer la\nvaleur de la technologie pour les plus grandes organisations et participer √† la construction d‚Äôun monde plus durable et inclusif.\nVos missions :\nVous √™tes passionn√© par le domaine de la Data, vous souhaitez prendre part √† des projets d'envergure, concevoir des solutions, les impl√©menter et les faire √©voluer?\nAlors rejoignez notre √©quipe Data Engineering Services au sein de Capgemini Cloud Infrastructure Services en tant que Data Engineer.\nVous avez acquis une exp√©rience solide dans le d√©veloppement, la mise en ≈ìuvre et l‚Äôoptimisation de solutions pour le traitement d'un grand volume de donn√©es, vous √™tes capable de cr√©er des solutions qui r√©pondent aux besoins m√©tiers et IT, alors rejoignez notre √©quipe d‚Äôexperts.\nEn qualit√© de Data engineer, vos missions sont les suivantes :\n‚ñ™ Concevoir et d√©velopper des solutions Data/IA.\n‚ñ™ Accompagner les M√©tier dans la compr√©hension et la mise en ≈ìuvre de solution orient√©es donn√©es.\n‚ñ™ Collaborer avec les Dev, les Ops, les experts infrastructures dans la construction de solutions et d‚Äôinfrastructures ax√©es sur les donn√©es.\n‚ñ™ G√©rer un √©cosyst√®me de partenaires data et assurer un haut niveau d'expertise\n‚ñ™ Assurer un r√¥le de veille technologique sur tous les outils autours de la data, de l‚ÄôIA et de la BI.\nVotre profil :\nVous √™tes issu d‚Äôune formation ing√©nieur ou √©quivalent bac+5 informatique sp√©cialis√©e en DATA et vous justifiez d‚Äôune exp√©rience de 3 √† 5 ans dans un r√¥le similaire. Expert dans une technologie de base de donn√©es relationnelle (PostgreSQL, Oracle...)\nExpert dans une technologie de base NoSQL (MongoDB, Cassandra...)\nVous maitrisez un framework de manipulation de donn√©es (Hadoop, Spark, Kafka...)\nVous maitrisez les concepts DevOps et avez de bonnes notions en scripting et d√©veloppement\nVous avez une exp√©rience des outils BI et de data visualisation (Kibana, PowerBI...)\nLa maitrise de l'anglais est n√©cessaire.\n3 raisons de nous rejoindre :\nQualit√© de vie au travail :\naccord de t√©l√©travail en France et √† l‚Äôinternational, accord sur l‚Äô√©galit√© professionnelle, la parentalit√©, l‚Äô√©quilibre des temps et la mobilit√© durable.\nApprentissage en continu :\ncertifications et formations en libre acc√®s, accompagnement sur mesure avec votre career manager, parcours d‚Äôint√©gration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, tarif pr√©f√©rentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culture\nNos engagements et priorit√©s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int√©grez un collectif qui valorise la diversit√©, d√©veloppe le potentiel de ses talents, s‚Äôengage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r√©duire son impact environnemental sur tous ses sites et aupr√®s de ses clients.\n√Ä propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr√®s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d‚Äôexp√©rience, nous sommes un partenaire strat√©gique des entreprises pour la transformation de leurs activit√©s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp√©tuelle √©volution tels que le cloud, la data, l‚ÄôIntelligence Artificielle, la connectivit√©, les logiciels, l‚Äôing√©nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "MongoDB",
                "Cassandra",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DBMS": [
                "PostgreSQL"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "eXalt Value",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=29&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=5UkNpHa5sTYINw6lkcL30g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "eXalt\nest un cabinet de conseil IT\nPure player Data\n& IA bas√© √† Paris.\nNotre offre s‚Äôarticule autour de 4 piliers r√©unis au sein d‚Äôune m√™me communaut√© pour un accompagnement √† 360¬∞ alliant une expertise technique et m√©thodologique √† une approche conseil m√©tier:\nData Gouvernance & Project\nData Engineering & Big Data\nData Performance & Analytics\nData Science & IA\nFiliale du groupe eXalt, cr√©√© en 2018,\nregroupant plus de\n950 collaborateurs en France\n(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)\net √† l‚Äôinternational\n(Colombie, Etats-Unis, Espagne, Belgique),\neXalt Value\napporte une\nexpertise approfondie\ndans le domaine de la Data & IA et conseille les entreprises dans le d√©ploiement de leurs strat√©gies data-driven.\nB√©n√©ficiant du support du groupe eXalt\n(1er dans la cat√©gorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value\nest en pleine croissance et regroupe aujourd‚Äôhui une communaut√© d‚Äôexpertise de plus de 60 collaborateurs en r√©gion parisienne.\nNos consultants interviennent sur d\nes projets d‚Äôenvergure\ndans divers secteurs d‚Äôactivit√©,\nBanque & Assurance, M√©dias, Transports, Retail, Tourisme, etc.\nNous recherchons un\nData Engineer Confirm√© H/F (minimum 4 ans d'exp√©rience dans la fonction)\npour rejoindre notre communaut√© sur le\npilier Data Engineering & Big Data.\nVos missions:\nConcevoir et d√©velopper des pipelines et des flux de donn√©es.\nInt√©grer et transformer des donn√©es provenant de diff√©rentes sources.\nD√©velopper et mettre en ≈ìuvre des algorithmes de traitement de donn√©es avanc√©s.\nCollaborer √©troitement avec les √©quipes clients pour comprendre leurs besoins et fournir des solutions adapt√©es.\nAssurer la qualit√© et la fiabilit√© des solutions d√©velopp√©es.\nConseiller les √©quipes clients sur les solutions √† mettre en place.\nLes Pr√©requis :\nTitulaire d'un Bac+5, Ecole d'Ing√©nieur\nMa√Ætrise d'un ou plusieurs langages de programmation (\nPython, Scala, Spark, etc\n.).\nExp√©rience approfondie des technologies\nBig Data (Hadoop, Spark, Kafka, Talend, etc.)\nExp√©rience av√©r√©e\nen\nenvironnement Cloud (AWS, GCP, ou Azure)\n.\nSolides comp√©tences en conception et en optimisation de pipelines de donn√©es.\nExp√©rience de travail en\nm√©thode Agile\nCapacit√© √† travailler de mani√®re autonome et en √©quipe.\nExcellentes comp√©tences en communication et en r√©solution de probl√®mes.\nMa√Ætrise de l‚Äôanglais (oral & √©crit dans un contexte international professionnel).\nVotre environnement eXalt√©:\nUn environnement de travail Collaboratif\nfavorisant les initiatives et projets transverses √† la Practice Data & IA (Lab IA, Data Hub, etc.).\nUn collectif de consultants passionn√©s,\ns‚Äôint√©ressant aux tendances innovantes du secteur.\nUne Practice de proximit√©,\nprivil√©giant la mont√©e en comp√©tence de ses collaborateurs (formations, coachings, mentorats, etc.)\nUn suivi individualis√© et de proximit√©\npar un.e Data Sales Manager r√©f√©rent du compte client, un.e Charg√©.e RH et un.e Practice Manager\nUne √©quipe ouverte et dynamique,\nqui privil√©gie les moments de partage et de convivialit√© (s√©minaires, eXaltemps, meet-up, d√©jeuners d‚Äô√©quipe, etc.)\nNotre processus de recrutement :\nUn entretien RH avec Estelle,\n√† la suite duquel vous saurez tout (ou presque) d‚ÄôeXalt Value,\nUn entretien technique avec un Manager assorti d‚Äôun test technique,\nlors duquel vous aurez l‚Äôoccasion de d√©montrer vos talents mais aussi d‚Äôapprendre avant m√™me de dire oui,\nUn entretien final avec la Directrice Associ√©e ou le Directeur Op√©rationnel,\npour finir de vous convaincre de nous rejoindre üòä\nNous avons h√¢te de recevoir vos CV, et de faire votre connaissance!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirm√©"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "United Robotics Group",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=30&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=zHBqRypzsrJd0gNx9KSZVA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Bienvenue chez\nAldebaran\n, leader europ√©en de la robotique\nau sein du groupe\nUnited Robotics Group\n.\nNous concevons et industrialisons des robots innovants avec une vision soci√©tale ambitieuse pour fa√ßonner un monde plus humain. Depuis 2005, nous sommes √† l'avant-garde de l'interaction homme-robot avec des produits embl√©matiques tels que NAO et Pepper.\nNotre dernier-n√©,\nPlato\n,\nincarne notre engagement envers la technologie de pointe et la s√©curit√©,\nfabriqu√© en France avec des composants europ√©ens.\nRejoignez nos √©quipes multiculturelles et dynamiques pour √™tre au c≈ìur de la r√©volution de la robotique.\nSi vous √™tes passionn√©.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer √† fa√ßonner l'avenir, nous vous offrons une exp√©rience enrichissante et stimulante.\nEn tant que membre de notre √©quipe, vous b√©n√©ficierez d'une culture d'entreprise ax√©e sur le sens de ce que nous faisons et valorisant la responsabilit√© sociale et environnementale.\nChez Aldebaran, nous valorisons l'innovation, la diversit√© et l'√©galit√© et encourageons chacun.e √† √™tre ouvert.e, authentique, courageux.se, responsable et engag√©.e.\nFinalit√© du poste\nAu sein de l'√©quipe Cloud-Online Services, le Data engineer int√©grera l'√©quipe Data, responsable du d√©veloppement des produits destin√©s √† la collecte, aux process et √† l'exploitation des donn√©es de nos robots.\nIl aura pour r√¥le de d√©finir et d'impl√©menter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui g√®rent les robots du groupe.\nMissions principales\nLe Data engineer aura pour responsabilit√©s de :\n√©valuer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,\nconcevoir et d√©velopper des services Data en respectant la sp√©cification fonctionnelle et la m√©thodologie agile,\nagr√©ger et stocker de grandes quantit√©s de donn√©es,\nmettre en place des solutions de data processing,\nint√©grer/d√©velopper des outils de visualisation de donn√©es et analyser les KPI,\nd√©velopper, tester, s√©lectionner et mettre en production des algorithmes qui permettent de r√©pondre aux besoins,\nr√©aliser des analyses de donn√©es,\nmettre en place des tests de charge et fonctionnels pour les solutions Data,\ninvestiguer et corriger les bugs remont√©s par les utilisateurs,\ncontribuer √† la mise en place de l'infrastructure et outil de d√©ploiement (CI/CD)\nRejoignez-nous pour faire partie d'une aventure passionnante o√π Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !\nRequirements\nPour la bonne ex√©cution des missions confi√©es, vous t√©moignez d'au moins 6 ans d'exp√©rience en tant que d√©veloppeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.\nComp√©tences demand√©es :\nBonne compr√©hension des technologies d'infrastructure et de d√©ploiement,\nComp√©tences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,\nBonne compr√©hension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,\nBonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)\nBonne connaissance et une exp√©rience pratique de Scrum\\Scrumban et des m√©thodes agiles,\nUne certification AWS sera appr√©ci√©e,\nUn niveau de fran√ßais et d'anglais courant est indispensable,\nDes exp√©riences dans des environnements fortement internationaux sont un plus\nBenefits\nNos principaux avantages :\nUne culture du bien-√™tre en entreprise qui a fait ses preuves (budget c√©l√©bration et moments de convivialit√© par √©quipes et directions, restauration collective de qualit√©, environnement de travail agr√©able)\nUn engagement fort en mati√®re de responsabilit√© sociale et environnementale (promotion de l'√©galit√© professionnelle, performance de notre plan diversit√© et inclusion, r√©f√©rent handicap, fresque du num√©rique)\nUne culture du t√©l√©travail encadr√©e de mani√®re appropri√©e !\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "6 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Coders Connect",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-coders-connect-3870419202?position=31&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=beZujM6hc33RzbbSXAlpIw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Coders Connect and Sanofi are joining forces to bring an electrifying twist to the biopharmaceutical world!\nWork with a rhythm that suits your style (2 days remote and 3 days onsite magic).\nLanguage\n: Proficiency in English is required for this role to ensure effective communication within our diverse, global team.\nAbout Sanofi:\nWe're not just a company; we're a global movement, focused on human health and making a real difference. Our mission? To battle pain, ease suffering, and sprinkle a little bit of magic in the process by developing life-changing medicines and vaccines through breakthrough science and wizard-level technology.\nDigital & Data: The Pulse of Our Mission\nAt the heart of our quest lies our digital and data powerhouse. Think of us as the digital healthcare platform of your dreams, where innovation meets speed, and technology shakes hands with medicine. With our scale, deep-rooted connections in health ecosystems worldwide, and a knack for pushing boundaries, we're here to revolutionise medicine, one digital solution at a time.\nThe Role: Data Engineering Virtuoso\nAs our Data Engineering Virtuoso, you're tasked with designing and orchestrating the data pipelines that power our ambitious data analytics initiatives. You'll ensure our data's integrity and accessibility, laying the groundwork for groundbreaking insights and innovations.\nRequirements\nCloud Platforms: Proficient in AWS services, with Azure & GCP knowledge a plus. Your work involves leveraging cloud solutions for scalable data processing and storage.\nData Modeling & Query Performance: Expertise in crafting data models and optimizing SQL queries to enhance performance. Experience with Snowflake or similar data warehousing solutions is highly valued.\nIntegration Services: Skilled in utilizing Integration Services like IICS and Tibco, you facilitate seamless data flow and integration across various platforms.\nScripting & Development: Proficient in scripting languages such as Python and R, enabling you to automate tasks and manipulate data efficiently. Familiarity with GitHub for source code management underscores your commitment to collaborative development and version control.\nVisualization & Reporting: Knowledgeable in creating insightful data visualizations using tools like PowerBI, Tableau, or similar, turning complex data into actionable insights.\nData Governance & Compliance: A keen understanding of data quality, security, and governance standards, especially in healthcare environments subject to regulations like GxP, SOX, and data privacy laws.\nReal World Data & Standard Models: Experience with Real World Data (e.g., EHR, Claims) and familiarity with standard data models (e.g., OMOP, FHIR) enhance your ability to contribute to our healthcare objectives.\nPipeline Frameworks: Proficiency in using frameworks like Apache Airflow or Kedro for crafting efficient, reliable data pipelines that support our AI and ML initiatives.\nThe Reward:\nA chance to play a crucial role in a collaboration that's redefining healthcare through digital transformation.\nA seat at the round table of diversity and inclusion, where different backgrounds and experiences conjure the richness of our culture.\nAn endless horizon of professional growth, learning opportunities, and the chance to contribute to a future where better health is a global reality.\nThe Call to Adventure:\nIf you're ready to join a quest for better ‚Äì better treatments, better outcomes, and better science ‚Äì and believe in the magic of bringing diverse talents together to make miracles happen, we want you. Let's embark on this journey together and transform the future of healthcare.\nBetter is out there. Are you ready to find it with us?\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Web Transition",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=32&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vLfroncagdB3hXjfmeFKmg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Web Transition, c‚Äôest qui ?\nFond√©e en 2011,\nWeb transition\nest une entreprise de services num√©riques op√©rant sur le march√© de l‚ÄôIT/Digital !\nConstituant une part essentielle de\nMoOngy Digital Lab\n, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et √©galement en Data !\nNotre objectif : nous implanter comme un acteur principal sur le march√© de la Transformation Digitale en accompagnant et valorisant les comp√©tences de nos collaborateurs !\nNous sommes convaincus que le succ√®s de MoOngy Digital Lab r√©side dans la somme des potentiels de nos √©quipes ü§ù\nTon √©quipe : La tribu Data\nParce qu‚Äôil est indispensable que tu puisses partager tes connaissances mais aussi en acqu√©rir de nouvelles, tu feras partie de l‚Äôune de nos tribus : celle de la Data. De plus, cela te permettra d‚Äô√™tre acteur dans le d√©veloppement et la strat√©gie de Web Transition. Ce syst√®me de co-r√©flexion et co-construction est un fondement essentiel chez nous !\nDans cette aventure, tu :\nT‚Äôassures\nde la ma√Ætrise de la donn√©e et est garant de la qualit√© de son utilisation (r√©f√©rencement, normalisation, et qualification)\nTravailles\n√† la compr√©hension et l'int√©gration des donn√©es en provenance des diff√©rents formats\ndes interfaces de flux\n√©galement √† la d√©finition de la politique de la donn√©e et √† la structuration de son cycle de vie dans le respect des r√©glementations en vigueur\nla supervision et l'int√©gration des donn√©es de diverse nature qui proviennent de ces sources multiples et v√©rifie la qualit√© des donn√©es qui entrent dans le Data Lake\nGarantis\nl'acc√®s qualitatif aux sources de donn√©es\nFacilites\nl‚Äôacc√®s aux donn√©es pour tes coll√®gues (data scientists, data analysts‚Ä¶)\nAssistes\nles autres √©quipes dans l'acc√®s et la compr√©hension des donn√©es des socles.\nRejoins-nous si tu as :\nExp√©rience d‚Äôau-moins 4 ans dans la Data\nApp√©tence √† la qualit√© des donn√©es.\nConnaissance famili√®re des Datawarehouses.\nMaitrise de Python, Oracle SQL, GCP/Power BI\nAisance avec les indicateurs, tu as une bonne capacit√© d'analyse et de r√©daction.\nTon savoir-√™tre :\nOuvert d‚Äôesprit\nRigoureux\nAutonome\nRespectueux des diff√©rences de chacun\nCurieux\nProactif\nAgile\nPar o√π on commence ?\nUn premier entretien RH d‚Äô1h pour comprendre ton parcours et tes aspirations\nUn second entretien de 45 minutes avec l‚Äôun de nos Business Manager afin de valider tes comp√©tences et qu‚Äôil se projette sur l‚Äôune des missions qu‚Äôil pourrait te proposer\nUn troisi√®me entretien de quelques minutes avec notre responsable d‚Äôagence pour te proposer d‚Äôint√©grer notre superbe Team Web !\n3 entretiens en peu de temps, si ton profil correspond tu int√®greras tr√®s vite nos √©quipes üòâ\nPr√™t pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant √† cette offre ! Voici les avantages qui t‚Äôattendent en tant que Weber :\nü§© Des coll√®gues incroyables\nüèÜ Certifi√©e Great Place to Work\nüéÆ Des bureaux sympas (o√π vous serez toujours les bienvenus)\nüéâ Des teambuilding et √©vents tous les mois\nüíª Des tributs m√©tiers pour √©changer entre Weber du m√™me m√©tier\nDes missions chez le client qui sont accompagn√©es et coach√©es par ton manager\nUn accompagnement dans ton plan de carri√®re et tes envies de re skilling\nü§ì Un catalogue de formations certifiantes ouvert √† tous les salari√©s\nüçΩÔ∏è Une carte tickets restaurant MyEdenred\n‚ù§Ô∏è Une mutuelle GrasSavoye\nüöé Une prise en charge des frais de transport √† 100%\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Oracle SQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Airswift",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-airswift-3909165766?position=33&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=H6EwNhVemE1u%2FM3i1U3DQg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data Engineer\nLocation\n: Paris (Hybrid)\nContract type\n: 12 months +\nYears of Experience\n: 4+\nRecruitment Partner:\nAirswift\nKey Words:\nProject Management | Jira | Digiboard | Banking | Stakeholder Management | Architecture | Cloud | Payments/Credit |ServiceNow | PPM |\nResponsibilities\nDesign, develop, and implement data pipelines to collect, process, and store structured and unstructured data from various sources.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and translate them into technical solutions.\nOptimize and tune data pipelines for performance, scalability, and reliability.\nEnsure data quality and integrity throughout the data lifecycle, implementing data validation and monitoring processes.\nEvaluate and implement new tools and technologies to enhance our data infrastructure and capabilities.\nRequirements\n:\nExtensive experience in Python.\nStrong experience with data processing frameworks and tools such as Apache Spark.\nExperience with cloud platforms such as AWS, Azure, or Google Cloud Platform.\nSolid understanding of data modelling, database design, and SQL\nFrench and English speaking\nFreelancing opportunity\nThe next step\nWe have an exceptional team in place, and we are pleased to be able to appoint a further person to our growing business. We are aware that you may not ‚Äòtick all the boxes‚Äô, but if you believe you can genuinely offer some valuable skills and experience to our business, please in the first instance contact our recruitment partner Airswift.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Beelix",
        "location": "Antibes, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=34&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SE7K53prpCanxUE8Nz3ejA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des probl√©matiques de Product Management, Data et Design Thinking. Beelix contribue √† fa√ßonner le monde de demain en participant aux grandes avanc√©es des secteurs suivants :\nüöóAutomobile\n‚ö°Energie\nüì°M√©dias & T√©l√©coms\nüëóLuxe & Retail\nüí∂ Banque, Finance & Assurance\n‚úàÔ∏èD√©fense\nAujourd‚Äôhui, Beelix compte plus de 200 collaborateurs motiv√©s et dynamiques. Lab√©lis√©e Great Place To work en 2023, Beelix est aussi une entreprise engag√©e o√π il fait bon vivre.\nDans le cadre de notre d√©veloppement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.\nQuelles missions au quotidien ?\n√ätre le leader de la brique Datalakehouse\nD√©velopper les scripts de transformations de donn√©es et les pipelines d‚Äôalimentation\nProposer des √©volutions architecturales ou de fonctionnalit√©s pour am√©liorer le socle technique\n√ätre le back-up du leader technique sur la partie reporting (Power BI)\nOrientation satisfaction client et r√©sultat final forte mais √©galement sensibilit√© au ¬´ comment ¬ª\nInnovation et proposition de nouvelles pratiques pour am√©liorer l‚Äôenvironnement et les conditions de travail des √©quipes\nA propos de vous ?\n5 + ann√©es d'exp√©rience en tant que Data Engineer\nMa√Ætrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Mod√©lisation de donn√©es\nAnalyses et export de donn√©es\nConnaissance de l‚Äôensemble du processus depuis la collecte jusqu‚Äô√† la mise √† disposition des donn√©es en ayant comme point fort la maitrise de sa transformation et mise en forme\nVous avez un bon niveau d‚Äôanglais\nLocalisation : Biot et/ou Carros\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "SQL Server"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Aubay",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=35&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=RBe%2FSfh1XFULSgxcv2128A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Passionn√© par la Data, tu souhaites rejoindre une communaut√© d‚Äôexperts dans le domaine afin de d√©velopper tes comp√©tences en Data Engineering. Aubay renforce ses √©quipes Data et recherche des Data Engineers pour int√©grer des dispositifs de projets pointus et vari√©s.\nTon quotidien en tant que Data Engineer chez Aubay, :\nD√©finition de la strat√©gie de stockage et mise en ≈ìuvre des technologie appropri√©es (base de donn√©es SQL, NoSQL, stockage distribu√©,‚Ä¶)\nIngestion des donn√©es (structur√©es, semi-structur√©es ou non-structur√©es) selon diff√©rentes fr√©quences : batch, micro-batch ou temps r√©el\nConception et mise en ≈ìuvre de pipelines de donn√©es afin de fournir des donn√©es pr√™tes √† l‚Äôemploi aux consommateurs : uniformisation, mise en qualit√©, enrichissement, calcul d‚Äôindicateurs,‚Ä¶\nConception et d√©veloppement d‚ÄôAPI pour exposer les donn√©es aupr√®s d‚Äôapplications tierces\nAppui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning\nPr√©paration et animation d‚Äôateliers de travail avec des interlocuteurs vari√©s : recueil/approfondissement des besoins m√©tiers, avancement/restitution des travaux, transfert de comp√©tences,‚Ä¶\nTon profil :\nTu dispose d‚Äôune formation niveau BAC+5 (Master 2 ou √©cole d‚Äôing√©nieur) sp√©cialis√©e en informatique\nTu as d√©j√† une premi√®re exp√©rience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data\nLes technologies telles que Hadoop, Spark ou Kafka sont tes technologies de pr√©dilection\nLa programmation n‚Äôa plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python\nTu ma√Ætrises les tenants et aboutissants de la philosophie DevOps et des outils orient√©s CI/CD\nTu es soucieux de la qualit√© et de la performance de tes d√©veloppements et tu t'int√©resse √† l‚Äôinnovation frugale\nTu es un expert technique dans ton domaine sans pour autant oublier l‚Äôimportance d‚Äôune communication orale et √©crite de qualit√© et adapt√©e √† chacun de tes interlocuteurs\nTu travaille au quotidien en mode agile et tu en maitrise les fondements\nCe qui nous caract√©rise :\nDes missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs vari√©s (Banque, Assurance, Telecom, Industrie,‚Ä¶) qui permettent √† nos collaborateurs de monter en comp√©tences et de devenir des experts Data reconnus\nDe l‚Äôapprentissage en continu avec des formations et des certifications sur les technologies Data d‚Äôaujourd‚Äôhui et de demain\nDes experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets\nDes communaut√©s de savoir-faire Data proposant de mani√®re r√©guli√®re aux collaborateurs d‚ÄôAubay du contenu et des √©v√®nements de partage (webinar, meetup/afterwork, BBL,‚Ä¶) sur les th√©matiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,‚Ä¶\nAubay encourage la diversit√© sous toutes ses formes et garantit l'√©galit√© des chances √† tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les am√©nagements n√©cessaires.\nTa carri√®re chez Aubay :\nTu auras la possibilit√© de d√©velopper et certifier tes comp√©tences sur les derni√®res technologies Data avec un focus fort sur les plateformes Data Cloud telles qu‚ÄôAzure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks\nTu pourras rejoindre la BU d‚Äôexcellence Data et √©voluer au sein d‚Äôun environnement humain et professionnel de haut niveau. Tu profiteras d‚Äôun management sur-mesure pour t'accompagner dans ta trajectoire de carri√®re\nAu sein de la BU d‚Äôexcellence, de multiples perspectives s‚Äôoffriront √† toi :\nR√¥le de ¬´ Lead ¬ª : Vous pourrez gagner en responsabilit√© sur le plan technologique et devenir un r√©f√©rent aupr√®s de nos clients et des collaborateurs de la communaut√© Data Engineering\nR√¥le de ¬´ Champion ¬ª : Vous repr√©senterez Aubay aupr√®s d‚Äôun ou plusieurs de nos partenaires √©diteurs strat√©giques et vous participerez activement √† l‚Äôanimation de la relation sur le plan technologique\nR√¥le de ¬´ Head ¬ª : Vous pourrez prendre la responsabilit√© du savoir-faire Data Engineering et de ses offres et en assurer le d√©veloppement au sens large (d√©veloppement business, recrutement, management de collaborateurs, d√©finition de la strat√©gie et animation de la communaut√© au sein du groupe Aubay,‚Ä¶)\nBesoin d‚Äôen savoir plus sur le processus de recrutement ?\nUn √©change macro au niveau RH avec Doriane\nUn entretien technique avec Marius ou Peter, deux de nos r√©f√©rents techniques\nUn √©change manag√©rial avec le Directeur de la BU Modern BI & Data\nA savoir que l‚Äôordre des √©tapes peut varier selon tes envies (ex : √©change manag√©rial avec l‚Äô√©change technique)\nAubay encourage la diversit√© sous toutes ses formes et garantit l'√©galit√© des chances √† tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les am√©nagements n√©cessaires.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "CI/CD",
                "Big Data",
                "Machine Learning",
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ADVANCED Schema",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-advanced-schema-3886398270?position=36&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=0IzfiiIAtmjfmcIDT5XGsw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "En tant que Data Engineer, vous aurez les missions suivantes :\nConcevoir\ndes modeÃÅlisations physiques\nConstruire\ndes mappings techniques et r√©daction de sp√©cifications d‚Äôalimentation.\nD√©velopper\ndes flux des donn√©es\nContribuer\nau pilotage de projets, de proof of concepts\nParticiper\naÃÄ des missions d‚Äôexpertise\nComp√©tences professionnelles & niveau d'√©tudes requis :\nVous √™tes titulaire d'un dipl√¥me\nBac +3\nminimum dans le domaine de la\ndata\nVous poss√©dez minimum\n1 an d'exp√©rience\ndans le m√©tier\n√ätre\nenthousiaste\n√† l'id√©e\nd'apprendre de nouvelles technologies\nExp√©rience de la m√©thodologie\nAgile / Scrum\nCapacit√© √†\nplanifier et √† prioriser\nles\nt√¢ches\net les\nactivit√©s confi√©es\nen autonomie\nMa√Ætrise\nde l‚Äôanglais oral et technique obligatoire\nExpeÃÅrience\nav√©r√©e dans l'√©criture de code propre avec 2 ou plusieurs des technologies suivantes :\nBASH, SQL, Java, Python, NoSQL\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ADVANCED Schema",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-advanced-schema-3539059697?position=37&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=vCi%2FAHpg6Emgt0PR%2BhHtxA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ADVANCED SCHEMA\nest une soci√©t√© de services informatiques\nsp√©cialis√©e dans la donn√©e.\nDepuis 20 ans, nous cr√©ons des plateformes data sur mesure pour nos clients, orient√©es usages et alliant qualit√©, performance, s√©curit√© et gouvernance.\nADVANCED SCHEMA\na d√©velopp√© de nouvelles activit√©s pour r√©aliser l'ambition du groupe : devenir\nune entreprise end-to-end,\nen proposant une offre √† 360¬∞ √† nos clients pour les\naccompagner √† chaque √©tape de leurs projets.\n√Ä ce jour, nous sommes pr√®s de 220 passionn√©s r√©partis entre Paris, Lille, Nantes, Lyon mettant √† profit leur expertises aussi bien dans le domaine du retail, de la finance/assurance, du luxe, des m√©dias, de la sant√© et de l'industrie.\nAujourd‚Äôhui, nous souhaitons int√©grer de nouveaux renforts dans nos √©quipes Lilloises.\nEn tant que Data Engineer, vous aurez les missions suivantes :\nConcevoir des modeÃÅlisations physiques\nConstruire des mappings techniques et r√©daction de sp√©cifications d‚Äôalimentation.\nD√©velopper des flux des donn√©es\nContribuer au pilotage de projets, de proof of concepts\nParticiper aÃÄ des missions d‚Äôexpertise\nComp√©tences professionnelles & niveau d'√©tudes requis :\nVous √™tes titulaire d'un dipl√¥me Bac +3 minimum dans le domaine de la data\nVous poss√©dez minimum 2 ans d'exp√©rience dans le m√©tier\nPositif(ve), curieux(se), rigoureux(se) et dot√©(e) d'une bonne aisance relationnelle\n√ätre enthousiaste √† l'id√©e d'apprendre de nouvelles technologies\nExp√©rience de la m√©thodologie Agile / Scrum\nCapacit√© √† planifier et √† prioriser les t√¢ches et les activit√©s confi√©es en autonomie\nMa√Ætrise de l‚Äôanglais oral et technique obligatoire\nExp√©rience av√©r√©e dans l'√©criture de code propre avec 2 ou plusieurs des technologies suivantes : BASH, SQL, Java, Python, NoSQL\nNotre proposition :\nTemps plein en\nCDI\navec un\nsalaire attractif\n+ participation aux b√©n√©fices + prime(s) sur investissement personnel\nMode de\ntravail hybride\n(agence, site, t√©l√©travail selon projets/clients)\nTicket restaurant (Sodexo)\nMutuelle financ√©e √† 50%\nPr√©voyance\nComit√© entreprise\n5 jours d‚Äôonboarding plein temps via la\nADVANCED SCHEMA Academy\nNotre investissement :\nChez\nADVANCED SCHEMA\n, nous t‚Äôoffrons un environnement de travail stimulant et collaboratif ainsi que des possibilit√©s de croissance et de d√©veloppement professionnel. √âgalement un\naccompagnement/support au quotidien\npour te faire grandir et monter en comp√©tences, sur des projets qui r√©pondent √† de\nvrais enjeux pour nos clients\n. Si tu es passionn√©(e) par les donn√©es et pr√™t(e) √† relever de nouveaux d√©fis, alors nous aussi nous aimerions te rencontrer\nProcess de recrutement :\nSi ta candidature retient notre attention, nous te proposons :\nUn premier √©change t√©l√©phonique/visio\nUn entretien physique (+questionnaire d‚Äô√©valuation) avec un senior manager\nUn entretien final √† notre si√®ge Parisien afin de rencontrer le DG\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Temps plein",
                "Senior"
            ],
            "TypeContract": "CDI",
            "Salary": "50",
            "Level": "",
            "Experience": "20 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Bash",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "AFD Technologies",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-afd-technologies-3899625641?position=38&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=bYpfT0MgYlzf9jo6i%2Fxwig%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "AFD.TECH part of Accenture\nest le sp√©cialiste du conseil en transformation digitale des grandes entreprises üöÄ.\nA ce jour, le Groupe est compos√© de 2.000 talents r√©partis dans 3 pays (France, Belgique & Maroc) üåé pour un chiffre d‚Äôaffaires annuel de 125M‚Ç¨ !\nNos Talents d‚Äôabord üòé:\nLes Talents d‚ÄôAFD.TECH part of Accenture sont au c≈ìur de la strat√©gie du Groupe et il est primordial pour nous que chaque collaborateur trouve du sens dans son travail.\nAu-del√† de proposer une carri√®re ambitieuse et personnalis√©e √† nos Talents, nous avons √† c≈ìur de leur offrir un environnement de travail flexible (remote), inclusif et √©panouissant dans tous nos bureaux (Paris, Bruxelles, Rabat, Lyon, Strasbourg, Lille, Nantes, Toulouse, Marseille, Bordeaux et Rennes)üåç.\nAvec 20% de croissance par an et plus de 20 ans d‚Äôexistence, AFD.TECH part of Accenture est devenu l‚Äôacteur incontournable du march√© des infrastructures informatiques, r√©seaux et t√©l√©coms.\nNotre proposition de valeur ? Intervenir sur l‚Äôensemble du cycle de vie de projets complexes, pour des clients grands comptes, venant de secteurs tels que la banque, le ferroviaire, les m√©dias, t√©l√©coms, etc (comme la Soci√©t√© G√©n√©rale, Bouygues Telecom, Orange, Thales et bien d‚Äôautres encore !)üë©üèª‚Äçüíª.\nNous rejoindre est une formidable aventure humaine : nous vous proposons un poste un poste de\nData Engineer en CDI\n, au sein de notre agence Lilloise.\nVos missions ‚úÖ:\nEn tant que Data Engineer pour l'un de nos clients grands comptes, votre r√¥le s‚Äôarticulera autour de diff√©rents axes :\nAppr√©hender le contexte et les enjeux m√©tier du client.\nCollaborer avec les √©quipes m√©tier pour comprendre les exigences en mati√®re de donn√©es.\nD√©finir des architectures data.\nConcevoir et mettre en place des pipelines de donn√©es.\nConstruire des flux de donn√©es complexes.\nVous travaillerez dans une mission √† forte valeur ajout√©e et de longue dur√©e (minimum 1 an et demi).\nVotre profil‚úÖ:\nVous ma√Ætrisez le langage SQL, les ETL et les ELT.\nVous aimez automatiser, mettre en place vos data pipelines et ma√Ætriser les technologies: CI/CD, Terraform, Github, Python, Kafka.\nVous poss√©dez des comp√©tences en data visualisation : Business Objects, Qlikview, Qlik Sense, PowerBI ou Data Studio.\nVous connaissez Google Cloud Platform (GCS, BigQuery).\nVous √™tes dipl√¥m√©(e) d‚Äôune formation BAC + 5.\nVous avez une premi√®re exp√©rience significative dans la data engineering (\nminimum 3 ans\n).\nVous projetez votre carri√®re dans un cabinet de conseil exigent et successful, qui vous permettra de d√©velopper votre esprit entrepreneurial et de r√©pondre √† vos ambitions.\nCe que nous offrons chez AFD.TECH part of Accenture ü§ó:\nUne politique de flexibilit√© dans votre organisation et un bon √©quilibre de vie üèÉ‚Äç‚ôÇÔ∏è.\nDes avantages plus que comp√©titifs üí∞.\nUn accompagnement et un suivi r√©gulier durant tout votre parcours chez AFD.TECH (Launchpad, Linkers, rookies, etc‚Ä¶).\nUn √©tat d‚Äôesprit familial et de la proximit√© entre tous üë®‚Äçüë©‚Äçüëß‚Äçüë¶.\nDes moments de convivialit√© toute l‚Äôann√©e üçæ (√©vent en √©quipe, s√©minaire annuel, sports collectifs etc.).\nUn parcours d‚Äô√©volution sur mesure üîº.\nA tr√®s bient√¥t chez AFD.TECH part of Accenture!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "20 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Thales",
        "location": "Ollioules, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-thales-3902424527?position=39&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=gUaUTnXX4O5aMuZNjUpt4A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "QUI SOMMES-NOUS ?\nThales est un leader mondial des hautes technologies comptant plus de 81 000 collaborateurs pr√©sents sur tous les continents. Le Groupe investit dans les innovations du num√©rique et de la ¬´ deep tech ¬ª ‚Äì big data, intelligence artificielle, connectivit√©, cybers√©curit√© et quantique ‚Äì pour construire un avenir de confiance, essentiel au d√©veloppement de nos soci√©t√©s, en pla√ßant l‚Äôhumain au c≈ìur des d√©cisions.\nThales propose des solutions, services et produits qui aident ses clients ‚Äì entreprises, organisations, Etats ‚Äì dans cinq grands march√©s vitaux pour le fonctionnement de nos soci√©t√©s : identit√© et s√©curit√© num√©riques, d√©fense, a√©ronautique, espace, et transport.\nQUI ETES-VOUS ?\nDipl√¥m√© d‚Äôun Bac+5 en √©cole d‚Äôing√©nieur ou √©quivalent universitaire avec une sp√©cialisation en informatique, vous avez a\nu moins 3 ans d'exp√©rience\ndans les technologies Big Data.\nPassionn√© par le\nsecteur de la D√©fense et du Naval.\nCE QUE NOUS POUVONS ACCOMPLIR ENSEMBLE :\nEn tant que\nData Engineer,\nvous jouerez un r√¥le cl√© dans la conception, le d√©veloppement et la maintenance de notre infrastructure de donn√©es, ainsi que dans la transformation et la gestion des flux de donn√©es.\nVOS MISSIONS :\n‚Ä¢ Concevoir, d√©velopper et d√©ployer des solutions Big Data en utilisant les technologies\nHadoop, Spark, Scala\n.\n‚Ä¢ Mettre en place des pipelines de donn√©es performants pour l'ingestion, le traitement et le stockage des donn√©es massives.\n‚Ä¢ Collaborer √©troitement avec les √©quipes m√©tier pour comprendre leurs besoins en mati√®re d'analyse de donn√©es et proposer des solutions adapt√©es.\n‚Ä¢ Optimiser les performances des clusters Hadoop pour garantir une exploitation efficace des donn√©es.\n‚Ä¢ Assurer la qualit√© et la fiabilit√© des donn√©es trait√©es, en mettant en place des processus de validation et de nettoyage.\n‚Ä¢ Identifier et r√©soudre les probl√®mes li√©s √† l'infrastructure Big Data et proposer des am√©liorations.\nInnovation, passion, ambition : rejoignez Thales et cr√©ez le monde de demain, d√®s aujourd‚Äôhui.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-lille-at-capgemini-3914228495?position=40&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=p4ZL6Pvek5LSJf3xzYmxKw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini en quelques mots\nChoisir Capgemini, c'est choisir une entreprise o√π vous serez en mesure de fa√ßonner votre carri√®re selon vos aspirations, o√π vous serez soutenu et inspir√© par une communaut√© d‚Äôexperts dans le monde entier, o√π vous pourrez r√©√©crire votre futur. Rejoignez-nous pour red√©finir les limites de ce qui est possible, contribuer √† lib√©rer la valeur de la technologie pour les plus grandes organisations et participez √† la construction d‚Äôun monde plus durable et inclusif.\nVos missions\nVous maitrisez au minimum un langage de programmation appliqu√© √† l‚Äôanalyse de donn√©es\n(Java, Python, Scala et les environnements Spark et / ou Hadoop).\nVous √™tes passionn√© par le Big Data et le Machine Learning et l‚Äôanalyse de donn√©es\nVous concevez et mettez en ≈ìuvre des strat√©gies s√©curis√©es d'acquisition et d'int√©gration de donn√©es\nVous configurez des r√©f√©rentiels de donn√©es √† la pointe de la technologie dans des environnements distribu√©s\nVous construisez des pipelines de donn√©es pour collecter, transformer et traiter des donn√©es en collaboration avec des scientifiques de donn√©es afin de r√©pondre aux exigences de la mod√©lisation de donn√©es d'analyse avanc√©e\nVotre profil\nDipl√¥m√©(e) de Bac+5 en informatique\n4 ans d‚Äôexp√©rience\n(au sein d‚Äôune ESN ou chez un int√©grateur) en conseil client√®le\nUne solide culture technologique\nUn bon niveau d‚Äôanglais\n3 raisons de nous rejoindre\nQualit√© de vie au travail :\naccord de t√©l√©travail en France et √† l‚Äôinternational, accord sur l‚Äô√©galit√©\nprofessionnelle, la parentalit√©, l‚Äô√©quilibre des temps et la mobilit√© durable.\nApprentissage en continu :\ncertifications et formations en libre acc√®s, accompagnement sur mesure avec\nvotre carreer manager, parcours d‚Äôint√©gration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, activit√©s √† tarifs pr√©f√©rentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel\nNos engagements et priorit√©s\nLe groupe Capgemini encourage une\nculture inclusive dans un cadre multiculturel et handi-accueillant.\nEn nous rejoignant, vous int√©grez un collectif qui valorise la diversit√©, d√©veloppe le potentiel de ses talents, s‚Äôengage dans des\ninitiatives solidaires avec ses partenaires, et se mobilise pour r√©duire son impact environnemental sur tous ses sites et aupr√®s de ses clients.\nCapgemini\nest un\nleader mondial\n, responsable et multiculturel, regroupant pr√®s de 350 000 personnes dans plus de 50 pays. Fort de\n55 ans d‚Äôexp√©rience,\nnous sommes un partenaire strat√©gique des entreprises pour la transformation de leurs activit√©s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp√©tuelle √©volution tels que le\ncloud, la data, l‚ÄôIntelligence Artificielle, la connectivit√©, les logiciels, l‚Äôing√©nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Siderlog Conseil",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-%C3%A0-nantes-at-siderlog-conseil-3858540683?position=41&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=HGGdfZPbjaOackab%2BATkIA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Rejoignez une Aventure Passionnante chez Nous !\nüöÄ\nSi vous recherchez une entreprise en pleine croissance o√π votre potentiel peut s'√©panouir pleinement, vous √™tes au bon endroit !\nChez nous, l'humain est au c≈ìur de notre culture d'entreprise. Nous croyons en l'autonomie, la confiance et le partage comme des valeurs essentielles qui guident chacune de nos actions.\nNe perdez plus de temps, rencontrons-nous d√®s maintenant !\nEn tant que membre de notre √©quipe de consultants Siderlog, vous travaillerez en √©troite collaboration avec nos clients. Voici un aper√ßu des missions qui vous attend :\nContribution √† la fabrication de produits dans un environnement Cloudera üõ†Ô∏è\nAccompagnement sur la fabrication des mod√®les de Machine Learning sur des donn√©es √©nerg√©tiques et plus largement ESG. ü§ñ\nAttendu :\nContribuer au sein d'une √©quipe agile √† r√©pondre aux besoins des Caisses r√©gionales.. üåê\nD√©finir les architectures des solutions avec le reste de l‚Äô√©quipe üèóÔ∏è\nFabriquer et tester les solutions üß™\nD√©ployer dans les diff√©rents environnements üöÄ\nGarantir le bon fonctionnement en production üíº\nAccompagner l‚Äô√©volution des pratiques de l‚Äô√©quipe dans une logique d‚Äôam√©lioration continue de la qualit√© du code üìà\nEntrainer et tester des mod√®les de Machine Learning üß†\nProfil :\nUne exp√©rience entre 4 √† 7 ans\nLieu : Nantes\nD√©but : D√®s que possible\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "7 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "Other": [
                "Machine Learning"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ASTRELYA",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-astrelya-3910760230?position=42&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=xS5BufbehfAwbLJ4GkTZBQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ASTRELYA est un groupe de conseil et d‚Äôexpertise IT fond√© en 2017, pr√©sent en France (Paris et r√©gions) et en Suisse (Gen√®ve). Aujourd'hui plus de 280 collaborateurs accompagnent nos clients dans l‚Äôacc√©l√©ration et la transformation de leurs organisations.\nDans le cadre de notre d√©veloppement, nous recherchons un\nData Engineeer F/H\n.\nVos r√¥les et responsabilit√©s :\nD√©veloppements Java Spark\nOptimisation et gestion des √©volutions de l&#39;architecture pour int√©grer des calculs sur des volum√©tries de plus en plus importantes\nSupport technique aupr√®s des √©quipes de d√©veloppement et du responsable applicatif\nConception des solutions applicatives coh√©rentes avec l&#39;ensemble du SI et avec les normes et standards\nD√©velopper et garantir les pratiques de d√©veloppement et de documentation associ√©s (DevOps\nL‚Äôenvironnement technique dans lequel vous √©voluerez :\nJava, Scala, Spark, √©cosyst√®me Hadoop, environnement DevOps\nLes comp√©tences recherch√©es :\nFormation : √âcole d‚Äôing√©nieur ou √©quivalent Bac+5\nExp√©riences : Minimum 5 ans d‚Äôexp√©rience\nLangues : Anglais technique\nExcellent relationnel, force de proposition, autonome\nPourquoi rejoindre ASTRELYA ?\nUne gestion de carri√®re personnalis√©e et un management de proximit√©\nUne politique active de formations / certifications (technique, m√©tier, leadership)\nUne offre vari√©e de missions d‚Äôexpertise\nUn engagement RSE fort : Ecovadis Gold, Signataire de la charte pour la diversit√©, du Pacte des Nations Unies et mise en place du M√©c√©nat de comp√©tences\nUn programme de cooptation attractif\nAfterworks, conf√©rences techniques et activit√©s sportives r√©guliers\nCette annonce vous correspond ? Postulez !\nüöÄ\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "Other": [
                "DevOps"
            ],
            "EnSoftSkils": [
                "Leadership"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Beelix",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-beelix-3865239426?position=43&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=X3Z%2FKDHugw6kJ06w9HTF%2Bg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des probl√©matiques de Product Management, Data et Design Thinking. Beelix contribue √† fa√ßonner le monde de demain en participant aux grandes avanc√©es des secteurs suivants:\nüöóAutomobile\n‚ö°Energie\nüì°M√©dias & T√©l√©coms\nüëóLuxe & Retail\nüí∂ Banque, Finance & Assurance\n‚úàÔ∏èD√©fense\nAujourd‚Äôhui, Beelix compte plus de 200 collaborateurs motiv√©s et dynamiques. Lab√©lis√©e Great Place To work en 2023, Beelix est aussi une entreprise engag√©e o√π il fait bon vivre.\nDans le cadre de notre d√©veloppement, nous recherchons un Data Engineer en √éle-de-France.\nQuelles missions au quotidien ?\nVous aurez pour missions principales de d√©velopper les projets Big Data demand√©s par le m√©tier, et notamment :\nPasser de la donn√©e brute √† de la donn√©e exploitable, expos√©e sous forme de tables requ√™tables dans le Datalake\nConsolider ces donn√©es au fur et √† mesure de leur alimentation r√©currente dans le Datalake\nLes exploiter pour atteindre la finalit√© business (exposition de Business View, r√©int√©gration des r√©sultats dans le SI, service de scoring, ‚Ä¶)\nDe mettre en place et de garantir le respect dans la dur√©e d'un processus qualit√© sur l'ensemble du cycle de DEV (documents, tests unitaires / int√©gration / fonctionnels, commentaires, versionning, etc.)\nAccompagner les Data Engineers sur son p√©rim√®tre pour garantir la qualit√© des livrables\nExpertise souhait√©e\nExpertise en SPARK et PySpark\nExpertise sur Databricks\nUne exp√©rience sur un cloud provider public comme Azure (id√©alement), AWS, ou GCP\nConnaissances avanc√©es d'outils de BI comme PowerBI (id√©alement) ou Spotfire\nCapacit√© √† interagir avec des parties prenantes diverses : Business analyst, Architectes, M√©tier\nEtre expert dans les pratiques du Software Craftsmanship (Test Driven Development, Behavior Driven Development, Clean Code, Code Reviews, etc.)\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de donn√©es relationnelles et NoSQL\nUne exp√©rience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nA propos de vous ?\nDipl√¥m√© d'une √©cole d'ing√©nieurs ou √©quivalent\nAu moins 3 ans d'exp√©rience en tant que Data Engineer\nExp√©rience en mode de Delivery Agile (Scrum, Kanban, etc.‚Ä¶)\nVous avez un bon niveau d‚Äôanglais tant √† l‚Äô√©crit qu‚Äô√† l‚Äôoral\nPourquoi nous rejoindre ?\nUn suivi et un accompagnement au quotidien\nUn organisme de formation certifi√© Qualiopi, un abonnement linkedin learning pour chaque salari√© et des partenariats avec des sp√©cialistes pour d‚Äôautres expertises\nDe nombreux √©v√©nements : Afterworks, Communaut√©s m√©tiers, Happy talks‚Ä¶\nune Exp√©rience personnalis√©e bas√©e sur vos besoins gr√¢ce au Pr√©dictive Index\nNotre package ¬´ unBeelievable ¬ª : 100% du titre de transport, Tickets restaurants, CSE, Prime de participation ...\nNombreux √©v√®nements (afterworks, sport, etc) et des communaut√©s m√©tiers dynamiques\nLe processus de recrutement ?\n√âchange t√©l√©phonique (15 min)\nEntretien 1 RH pour apprendre √† vous conna√Ætre\nEntretien 2 avec votre futur N+1 pour appr√©hender la relation manag√©riale\nEntretien 3 avec un Responsable commercial pour avoir la vision strat√©gique\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "100",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MP DATA",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-mp-data-3908719610?position=44&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=2MIiyRrB12%2BwsWUba%2B9GJQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "MP DATA est une soci√©t√© sp√©cialis√©e dans l‚Äôacquisition, le traitement, et la valorisation des donn√©es.\nDepuis sa cr√©ation en 2015, MP DATA accompagne ses clients, majoritairement industriels, dans le management de leur performance et l‚Äôexploitation de leurs donn√©es.\nLes collaborateurs, tous issus de grandes √©coles, incarnent au quotidien les valeurs d‚ÄôExcellence, de Partage et d‚ÄôEngagement.\nIls associent savoir-faire technique, m√©thodologie et passion et mettent leurs comp√©tences au service de missions et projets au sein de grands groupes fran√ßais.\nMP DATA accompagne ses clients sur toute la chaine au travers de 3 p√¥les d‚Äôexpertise : Conseil et Strat√©gie, Infrastructure & CloudOPS, Data Science.\nChez MP DATA, les √©quipes commerciales cherchent des missions en fonction des envies des collaborateurs et non pas l‚Äôinverse. Les consultants sont accompagn√©s dans tous leurs projets, de la mobilit√© g√©ographique, au changement de secteur d‚Äôactivit√© en passant par le d√©veloppement de nouvelles comp√©tences.\nRejoindre MP DATA, c‚Äôest la garantie de travailler sur des sujets passionnants avec un cadre technique fort.\nDescriptif du poste :\nNous recherchons un Data Engineer exp√©riment√© pour rejoindre notre √©quipe.\nEn tant que Data Engineer, vous serez responsable de la conception, du d√©veloppement et de la mise en ≈ìuvre de pipelines de traitement de donn√©es en temps r√©el √† grande √©chelle.\nVous travaillerez avec des technologies telles que Kafka, Flink, Kinesis et vous utiliserez les services du cloud AWS pour stocker et traiter les donn√©es.\nVos responsabilit√©s :\nUtiliser Kafka pour le traitement de flux de donn√©es en temps r√©el √† grande √©chelle, en travaillant avec les producteurs, les consommateurs et les topics.\nMettre en ≈ìuvre des pipelines de traitement de donn√©es en streaming avec Flink, en appliquant des transformations complexes et en g√©rant les √©tats.\n√âcrire du code efficace et maintenable en Java / Python pour manipuler et analyser les donn√©es en temps r√©el.\nUtiliser Kubernetes pour d√©ployer et g√©rer des applications conteneuris√©es √† grande √©chelle, en assurant la r√©silience et l‚Äô√©volutivit√© des services.\nUtiliser les services AWS tels que Amazon S3, AWS Lambda, Elastic Kubernetes Service (EKS), Elastic Container Service (ECS) et Elastic Compute Cloud (EC2) pour le stockage, le traitement et le calcul des donn√©es en temps r√©el.\nSuivre les meilleures pratiques pour une utilisation efficace du cloud, en assurant la gestion des co√ªts, la s√©curit√© des donn√©es et la disponibilit√© des services.\nCollaborer avec l‚Äô√©quipe de d√©veloppement logiciel et la gestion de projets pour assurer un flux de d√©veloppement fluide et une livraison efficace des fonctionnalit√©s.\nBon √† savoir :\nCDI / ASAP / Toulouse\nProfil recherch√©:\nNous recherchons un candidat dipl√¥m√© d'une grande √©cole d'Ing√©nieur avec une premi√®re exp√©rience.\nComp√©tences n√©cessaires :\nExp√©rience significative dans un environnement industriel en mode DevOps, avec des outils tels que CICD, gitlab, Jenkins, Sonar, Nexus, XLdeploy, Camunda, etc.\nMa√Ætrise des langages de programmation tels que Python, Java et expertise dans l‚Äô√©criture et l‚Äôoptimisation du code SQL\nMa√Ætrise du fran√ßais et bonne maitrise de l‚Äôanglais.\nCapacit√© √† travailler en √©quipe et esprit d‚Äô√©quipe.\nLe processus de recrutement se d√©roule en 3 entretiens :\nPrise de contact\n1er entretien : Pr√©sentation et projet du candidat + pr√©sentation MP DATA\n2√®me entretien : Entretien de qualification technique\n3√®me entretien : Rencontre avec les √©quipes dans les locaux MP DATA + Proposition de collaboration\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Flink"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Shippeo",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-shippeo-3908268236?position=45&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=DxKRCRuTeTU%2Bx7wYrAc6QQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Founded in 2014, Shippeo is a French based SaaS company providing supply chain visibility. Shippeo has grown from 70 to 220 employees during the last two years and is continuing to rapidly scale after an additional $40M fundraising round in October 2022.\nShippeo is an exceptionally diverse company with colleagues from 27 different nationalities and speaking 29 languages. With offices throughout Europe, North America and recently Asia, Shippeo provides global coverage to all of our clients.\nOur product is composed of a mission critical SaaS web platform (API everywhere), with high traffic inbound/outbound integrations.\nOur mission is to anticipate problems and proactively alert end-customers so they can efficiently manage exceptions. We achieve this by collecting and matching millions of theoretical and real data from different stakeholders.\nThe Data Intelligence Tribe is responsible for leveraging Shippeo‚Äôs data from our large shipper and carrier base, to build data products that help our users (shippers and carriers alike) and ML models to provide predictive insights. This tribe‚Äôs typical responsibilities are to:\nget accurately alerted in advance of any potential delays on their multimodal flows or anomalies so that they can proactively anticipate any resulting disruptions\nextract the data they need, get direct access to it or analyze it directly on the platform to gain actionable insights that can help them increase their operational performance and the quality and compliance of their tracking\nprovide best-in-class data quality by implementing advanced cleansing & enhancement rules\nAs a Data Engineer at Shippeo, your objective is to ensure that data is available and exploitable by our Data Scientists and Analysts on our different data platforms. You will contribute to the construction and maintenance of Shippeo‚Äôs modern data stack that‚Äôs composed of different technology blocks:\nData Acquisition (Kafka, KafkaConnect, RabbitMQ),\nBatch data transformation (Airflow, DBT),\nCloud Data Warehousing (Snowflake, BigQuery),\nStream/event data processing (Python, docker, Kubernetes) and all the underlying infrastructure that support these use cases.\nQualifications\nRequired:\nYou have a degree (MSc or equivalent) in Computer Science.\n3+ years of experience as a Data Engineer.\nExperience building, maintaining, testing and optimizing data pipelines and architectures\nProgramming skills in Python and experience with asynchronous event processing (asyncio).\nAdvanced working knowledge of SQL, experience working with relational databases and familiarity with a variety of databases.\nWorking knowledge of message queuing and stream processing.\nKnowledge of Docker and Kubernetes.\nKnowledge of a cloud platform (preferably GCP).\nExperience working with workflow management systems such as Airflow.\nDesired:\nExperience with cloud based data warehouse solutions (BigQuery, Snowflake).\nExperience with Kafka and KafkaConnect (Debezium).\nExperience with Infrastructure as code (Terraform/Terragrunt).\nExperience building and evolving CI/CD pipelines with Github Actions.\nMonitoring and alerting on Grafana / Prometheus.\nExperience working on Apache Nifi.\nInformations suppl√©mentaires\nWe are looking for talents who share our values:\nüöÄ Ambition\nüíô Care\nüéØ Deliver\nü§ù Collaboration\nFind out more about our values in\nOur Culture Book\nIf you identify with our values and enjoy working in a fast-paced and international environment, Shippeo is just the place for you!\nWe are committed to fostering diversity and inclusion within our workplace as we value the unique perspectives and experiences that individuals from all backgrounds bring to our team. We are dedicated to providing equal employment opportunities to all candidates, regardless of their background or abilities, and our commitment to inclusion is reflected in our policies, practices, and workplace culture.\nWe understand that candidates may have unique needs or questions related to disability inclusion. To facilitate this, you can reach our dedicated Disability Advisor at\ninclusion@shippeo.com\nwith any inquiries or requests for accommodations during the application process.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery",
                "Snowflake"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "ML",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Withings",
        "location": "Issy-les-Moulineaux, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/stage-data-engineer-ml-h-f-at-withings-3613476264?position=46&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=%2F8DZLqwtmjFyUqn3vcgywQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Chez Withings, nous d√©veloppons des appareils de sant√© connect√©e : nos balances connect√©es, montres hybrides, tensiom√®tres, moniteurs de sommeil et tous les dispositifs de notre gamme sont aujourd'hui utilis√©s par des millions d'utilisateurs. Notre objectif est de permettre la pr√©vention, le d√©pistage et l'accompagnement d'un certain nombre de maladies chroniques via des produits et des services innovants afin de r√©volutionner la mani√®re dont on prend soin de notre sant√©.\nAu sein de l'√©quipe Machine Learning, nous d√©veloppons des algorithmes pour extraire des informations physiologiques et m√©dicales pour nos utilisateurs tels que le SPO2, la fr√©quence cardiaque, la d√©tection de diverses pathologies comme la fibrillation atriale, l'apn√©e du sommeil...\nInt√©gr√©.e au sein de l'√©quipe Machine Learning, tu auras une ou plusieurs des responsabilit√©s suivantes :\nD√©velopper un outil de monitoring de la dette technique, des mauvaises pratiques de code, des failles de s√©curit√© ;\nConstruire des dashboards de visualisation ;\nConstruire un syst√®me d'alerte pour notifier les contributeurs d'√©ventuels probl√®mes ;\nD√©velopper des outils permettant de corriger les √©ventuels probl√®mes de fa√ßon automatis√©e ;\nRequirements\n√Ä la recherche d'un stage d'une dur√©e de 3 √† 6 mois ;\nPr√©paration d'un Master en √©cole d'ing√©nieur ou √©quivalent / ann√©e de c√©sure possible ;\nMa√Ætrise de Python ;\nMa√Ætrise de Debian ou de Ubuntu, de Shell et de l'environnement Linux ;\nPremi√®re exp√©rience sur du d√©veloppement logiciel ;\nCulture DevOps (omnipr√©sence du monitoring, automatisation des t√¢ches, ...)\nCompr√©hension de la culture et des besoins des diff√©rents membres de l'√©quipe ;\nRigueur, autonomie, prise d'initiative, curiosit√©\nBenefits\nRejoindre l'aventure Withings, c'est :\nInt√©grer un des pionniers et leaders mondiaux de la sant√© connect√©e, plusieurs fois prim√© au Consumer Electronic Show\nContribuer √† des projets innovants et ambitieux pour la sant√© de demain dans un environnement agile et en constante √©volution\nInt√©grer une entreprise internationale, membre de la FrenchTech 120, dont les √©quipes sont bas√©es √† Issy-les-Moulineaux, Boston, Hong-Kong et Shenzhen\nParticiper √† l'am√©lioration continue de nos produits et services en les b√™ta-testant avant leur sortie, notamment lors de nos nombreuses sessions sportives entre coll√®gues\nParticiper √† la Withings Med Academy en assistant √† des conf√©rences de professionnels de sant√© afin de renforcer ses connaissances dans le domaine m√©dical\nCollaborer avec des coll√®gues passionn√©s et c√©l√©brer ensemble chacune de nos r√©ussites !\nToutes les candidatures re√ßues sont √©tudi√©es ind√©pendamment de l'origine ethnique, des croyances, de la religion, du genre, de l'orientation sexuelle ou de la sant√© des candidats. Withings aspire √† offrir et garantir l'√©galit√© des chances aux candidats et seules les personnes habilit√©es (RH et Management) auront acc√®s aux informations concernant votre candidature.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "OS": [
                "Linux"
            ],
            "Other": [
                "Machine Learning",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "AXA en France",
        "location": "Hauts-de-Seine, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-senior-at-axa-en-france-3884386043?position=47&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=SDubjYBbL7PRNB7yLnRk%2Bg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nSenior Data Engineer F/H\n, vous allez contribuer directement aux projets des directions m√©tier (ex : fraude sant√©, multi√©quipements, pricing IARD, optimisation du lead management, fragilit√© auto, ‚Ä¶) d‚ÄôAXA France et √† la construction du socle technique Big Data.\nVous allez int√©grer une √©quipe d'une dizaine de personne compos√©e de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus m√©tier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :\n- Une organisation agile en feature teams : tribus, guildes, squads\n- Des projets sur des applications innovantes √† fort trafic (web, mobile‚Ä¶)\n- Des m√©thodologies craft (TDD, BDD, clean code, code review‚Ä¶) et DevOps\n- Une communaut√© de partage de bonnes pratiques (BBL, dojo, meetup, conf‚Ä¶)\nVotre r√¥le et vos missions\nVous aurez pour missions principales de d√©velopper les projets Big Data demand√©s par le m√©tier, et notamment :\nPasser de la donn√©e brute √† de la donn√©e exploitable, expos√©e sous forme de tables requ√™tables dans le datalake\nConsolider ces donn√©es au fur et √† mesure de leur alimentation r√©currente dans le data lake\nLes exploiter pour atteindre la finalit√© business (exposition de business view, r√©int√©gration des r√©sultats dans le SI, service de scoring, ‚Ä¶)\nDe travailler √† la cr√©ation du socle technique Big Data et industrialiser le cycle de d√©veloppement de l'√©quipe\nDe mettre en place et de garantir le respect dans la dur√©e d'un processus qualit√© sur l'ensemble du cycle de DEV (documents, tests unitaires / int√©gration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nVous justifiez de plusieurs exp√©riences significatives (+ de 5 ans) sur du\nd√©veloppement big data, en particulier sur du PySpark.\nComp√©tences techniques :\nConnaissances avanc√©es en d√©veloppement en\nPySpark\n(Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avanc√©es d'outils de BI comme\nPowerBI\nComp√©tences transverses :\nCapacit√© √† interagir avec des parties prenantes diverses : Business analyst, Architectes, M√©tier\nExp√©rience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer junior sur les aspects technique\nEt Id√©alement :\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de donn√©es relationnelles et NoSQL\nUne exp√©rience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nQui sommes nous ?\nAXA est un des leaders de l‚Äôassurance et de la gestion d‚Äôactifs dans le monde.\nNous aidons nos 108 millions de clients √† traverser les petites et grandes difficult√©s de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure mani√®re de les prot√©ger et voulons donner √† chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuad√©s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C‚Äôest pour cette raison que nous menons une politique RH engag√©e qui favorise la diversit√©, qui pr√©serve l‚Äô√©quilibre vie priv√©e-vie professionnelle et acc√©l√®re le d√©veloppement des comp√©tences et des carri√®res.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une v√©ritable culture d‚Äôexpertise, acc√©l√©rant le d√©veloppement des comp√©tences de chacun et proposant une r√©mun√©ration attractive.\nPourquoi nous rejoindre ?\nVous √™tes porteur d‚Äôid√©es et d‚Äôinitiatives innovantes ? Vous proposez des solutions et √™tes au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunit√©s de carri√®res int√©ressantes\nUne entreprise qui donne une place de choix √† l‚Äôinnovation, √† l‚Äôinitiative et aux actions solidaires (notamment via l‚Äôassociation AXA Atout C≈ìur)\nUn environnement inclusif √† tous les niveaux (mixit√©, handicap, initiatives pour favoriser l‚Äôinsertion des jeunes, orientation sexuelle, etc.)\nUn acc√®s √† de multiples avantages (cong√©s, temps partiel, t√©l√©travail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d‚Äôenrichir ses comp√©tences\nVictime ou t√©moin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination √† alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Junior",
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Equativ",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/analytics-data-engineer-internship-at-equativ-3901954759?position=48&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=YVUggz38DDZrNjhBOH0OuQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Your mission\nHelping the company be fully data-driven is not just a buzzword, it is our mission at Equativ. We, the Data Analytics team at Equativ, maximize efficiency by enabling easy and permanent access to quality data, valuable insights & rigorous thinking. Our responsibility is to ensure our local and central teams, clients and partners, make informed business decisions. To do so, we leverage huge volumes of data (Equativ handles 150Bn Auctions per Day) in a state-of-the-art tech stack (AirFlow, Snowflake, Tableau‚Ä¶) in order to provide actionable insights to all teams at Equativ!\nReporting to the manager of the Analytics team, your mission will be to maintain and upgrade our data pipeline.\nWhat you'll do\nDay-to-day maintenance of our data pipeline:\nEnsure data pipeline ingestion accuracy in due time\nFollow-up on data quality issues raised by internal customers\nImprovement of sourcing processes:\nMigrate from Talend data flows to Python scripts for our sourcing jobs\nDevelop resources to make Data Analysts autonomous in sourcing data in our Snowflake database (through ready-to-use scripts or small interfaces)\nDeveloping new projects on our main platforms (Tableau & Snowflake)\nLeverage new resources to make the most out of Snowflake (Streamlit, Snowpark‚Ä¶)\nIdentify new ways to structure our data sources in Tableau while reducing the loading time for the user\nParticipate in the restructuring of our data marts (schemas, stages & permissions)\nCommunication:\nSync with Data Analysts to make sure that their requests are properly prioritized\nSynchronize with other technical teams (Core-Data, Infra) to gather requirements of the migration and ensure a smooth transition\nUnderstand business needs to suggest the most efficient technical solution\nAbout You\nPragmatic & hands-on mindset is required: you‚Äôll have latitude to explore different options, but you need to go for the most effective solution\nTechnical knowledge of Python & SQL is a must\nKnowledge of collaboration platforms (Gitlab) & Agile processes is a plus\nYou can demonstrate your ability to solve problems end to end\nYou are fluent in English\nüëã About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus ‚Äî four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times‚Äô FT 1000: Europe‚Äôs Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nEquativ is an equal opportunity employer. Equal access to employment, services, and programs are available to everyone, regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you require reasonable accommodation throughout the application and/or interview process, please contact the recruitment team at ta-team@equativ.com\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Collaboration": [
                "Teams"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Technology & Strategy",
        "location": "Lyon, Auvergne-Rh√¥ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-technology-strategy-3881556102?position=49&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=V5yopRJgKm1yv0Jrdf2Uag%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "D√©couvrez Novencia\n:\nExpert en Data et Intelligence Artificielle, nous aidons nos clients √† exploiter et √† valoriser leurs donn√©es sous toutes ses formes en les accompagnant sur des projets de Data Analyse, Data Gourvernance, Data Architecture, Data Science, et Data Engineering‚Ä¶\nVous avez une solide exp√©rience de minimum 2 ans dans l'ing√©nierie des donn√©es et vous √™tes √† la recherche de nouveaux d√©fis ? Bouclez votre ceinture, la suite est pour vous !\nType de contrat : CDI\nLieu : Lyon\nEn qualit√© de Data Engineer (H/F), votre r√¥le sera :\nConcevoir et proposer les solutions de d√©veloppement r√©pondant aux besoins fonctionnels et techniques des projets big data.\nTu participes √† la conception de solutions permettant le traitement de volumes importants de pipelines donn√©es.\nR√©aliser ces solutions par l‚Äô√©criture de code, en respectant les m√©thodes et proc√©dures qualit√©s d√©finies au sein du d√©partement Technique.\nMise √† disposition s√©curis√© et lisible de la data.\nS‚Äôassurer de la conformit√© fonctionnelle et technique de ces r√©alisations en effectuant les tests automatis√©s n√©cessaire et la mise en place de monitoring (syst√®me et qualit√©).\nAssurer la maintenance des applicatifs / plateforme data science\nAssurer une veille technologique\nVous disposez des comp√©tences suivantes :\nMaitrise des plateformes Cloud (AWS, GCP ou Azure), de Scala et de SQL.\nUn.e touche √† tout : poss√©dant des comp√©tences en langage Python/Spark, de bonnes capacit√©s de mod√©lisation, une forte app√©tence pour le Big Data\nFin.e connaisseur.euse : Data Engineer convaincu, tr√®s peu de secrets pour les clusters et pour les calculs parall√®les\nExplorateur.trice : d√©couvre de nouvelles technos gr√¢ce √† une veille r√©guli√®re\nD√©brouillard.e : rel√®ve de nouveaux d√©fis\nNotre objectif commun est de co-construire votre carri√®re en fonction de vos aspirations et de vos comp√©tences.\nContactez-moi en message priv√© ou par mail √† s.ziki@technologyandstrategy.com !\nLet's make it possible #together\n*Nos postes sont ouverts aux personnes b√©n√©ficiant d‚Äôune Reconnaissance de la Qualit√© de Travailleur Handicap√© (RQTH). T&S Groupe encourage la diversit√© et l‚Äô√©galit√© sur le lieu de travail. Tous les candidats qualifi√©s H/F/* sont pris en consid√©ration pour un emploi sur un m√™me pied d'√©galit√©.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Exotec",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/stage-data-engineer-at-exotec-3918170659?position=50&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=4yuUcRj%2F0x7kYt4sveJZSg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Chez Exotec, nous mettons l'excellence technologique au service de la red√©finition des relations entre humains et robots. A travers le monde, nos solutions r√©volutionnent la fa√ßon dont nos clients d√©livrent leurs produits aux consommateurs finaux. Nous contribuons au succ√®s des plus grandes marques du commerce et de l'industrie, tout en am√©liorant les conditions de travail de leurs salari√©s.\nPar l'alliance de l'intelligence artificielle et d'un hardware performant, nos robots sont d√©sormais d√©ploy√©s dans le monde entier et leur succ√®s a fait de nous la premi√®re licorne industrielle fran√ßaise.\nRejoindre Exotec, c'est l'opportunit√© de donner du sens √† vos comp√©tences. Grandissez avec plus de 800 ExoPeople dans le monde entier pour faire de vos id√©es des r√©alit√©s.\nLa r√©volution robotique port√©e par Exotec ne fait que commencer, vous en √™tes ?\nAu sein du p√¥le Data, de la DSI d'Exotec, votre r√¥le sera de participer au d√©veloppement de l'environnement et de l'infrastructure Data d'Exotec.\nPour cela :\nVous participez √† la mise en ≈ìuvre des composants techniques de la plateforme de donn√©es d'Exotec\nVous travaillez sur la collecte dans la plateforme de donn√©es provenant de sources multiples : Salesforce, ERP, logiciels d√©velopp√©s en interne\nVous nettoyez, mettez en qualit√© et pr√©parez les donn√©es afin de les rendre disponibles pour les diff√©rents cas d'usage qui en ont besoin\nVous migrez des reportings existants vers la plateforme de donn√©es et mettez en ≈ìuvre de nouveaux cas d'usage pour r√©pondre aux besoins de l'entreprise\nVous travaillerez au sein de l'√©quipe data et en √©troite collaboration avec la software factory, ainsi qu'avec les utilisateurs des m√©tiers qui ont besoin de rendre intelligibles les donn√©es disponibles\nRequirements\nVous √™tes √©tudiant(e) d'une √©cole d'Ing√©nieur g√©n√©raliste avec une sp√©cialisation programmation ou informatique\nVous recherchez un stage de fin d'√©tudes d'une dur√©e de 4 √† 6 mois\nVous avez id√©alement une premi√®re exp√©rience en Data Engineering et le d√©veloppement de pipeline de donn√©es\nVous maitrisez Python, l'ETL et SQL,\nCurieux(se) et rigoureux(se), vous souhaitez rejoindre une √©quipe jeune et dynamique ainsi que vous investir dans des projets complexes et excitants\nVous avez un niveau d'anglais courant\nChez Exotec, nous garantissons l'√©galit√© des chances dans notre processus de recrutement. L'ensemble des candidatures re√ßues sont √©tudi√©es ind√©pendamment de l'√¢ge, du genre, de l'origine, de la religion, de la couleur de peau, de la nationalit√©, du sexe, du handicap, de l'orientation sexuelle ou de toute autre distinction prot√©g√©e par la loi. Nous mettons en place un environnement de travail inclusif et respectueux de toutes les diff√©rences. En rejoignant le Pacte Parit√©, Exotec s'engage pour un √©cosyst√®me French Tech plus paritaire.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LVMH",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=51&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=k9JSETDfWfypotKWZEAbAw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.\nWe are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.\nIf you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,\nIf you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,\nIf you like building tech solutions having direct impacts on billion-dollar-valued businesses,\nIf you have good communication skills and like sharing your knowledge,\nApply now, and join us!\nThe mission\nThe Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.\nOur team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.\nYou will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).\nMain responsibilities\nYou will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).\nYou will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.\nYou will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.\nApplying the quality and security standards. Making them evolve if necessary.\nProducing realistic, understandable and documented solutions following the group guidelines.\nSharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.\nTaking responsibility as member of the team on the product performances (delivery and long term usage)\nRequired expertise and knowledge\nAbility to build technical solutions answering concrete usage (User Stories) and communicate them to the team.\nDimension and evaluate complexity for technical solution productions.\nExtensive knowledge and experience with good learning and sharing abilities.\nEvaluate quickly risks and opportunities about technical choices.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProblem-solving skills on Data and AI, coding and software development\nTech lover\nFeedback taker and giver\nTeam player\nKey benefits to join our team\nAttractive packages\nOffices in the 8th arrondissement near the Champs Elys√©es\nFlexibility on the working hours\nRemote work possible (~40%)\n7 weeks of holidays (cong√©s pay√©s + RTT)\nLVMH brands exclusive private sales\nGreat employee committee and health insurance (CE, mutuelle)\nLast generation MacBooks\nPart of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!\nYou‚Äôre eligible if\nYou have a strong experience (3+ years) in cloud data architecting or consultancy.\nYou graduated from an engineering (or equivalent) with a master‚Äôs degree. Computer Science knowledge is mandatory.\nExperience on data stacks and/or Google Cloud (built in components) is a huge plus.\nFrench and English both written and oral (Maisons are all over the world)\nYou‚Äôre thrilled to support the #1 luxury group to get even better.\nHiring Process\nCall with our HR partner dedicated to the Tech Team\nTechnical interview with the Solution Engineering Manager\nTechnical test\nInterview with the Head of Engineering\nStill here? Apply now!!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "40",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Flexibility",
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-gcp-f-h-at-apside-2902806697?position=52&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=CwTgaf7bJVvvYa%2B8by%2FfjQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre une entreprise apprenante ? Engag√©e pour t‚Äôaccompagner dans ton √©volution professionnelle et dans tes projets personnels ?\nRejoins Apside pour travailler sur les projets de demain !\nLe poste ?\nPour le compte de notre\nclient acteur mondial de la beaut√© et cosm√©tique,\ntu interviendras dans la\ntransformation d‚Äôun projet worlwide,\no√π tu devras\nd√©velopper la Data Platform et l'ensemble des services Data qui seront expos√©s aux diff√©rentes √©quipes du client. Aussi, tu seras amen√© √† d√©velopper des use cases data.\nDans ce sens, tes missions seront les suivantes :\nDesigner l'architecture et d√©velopper la solution\nD√©finir et d√©velopper les Data Model\n√ätre garant de la qualit√© du code\n√ätre DevOps (Utilisation/mise en place de chaine CI/CD et support niveau L3 des d√©veloppements)\nEnvironnement technique :\nGCP (BigQuery, Cloud Run, Cloud Build)\nSQL\nPython\nDevOps (Github)\nAPI Development\nTerraform\nM√©thodologie Agile\nToi ?\nTu as d√©j√† travaill√© sur\nGoogle Cloud Platform (GCP)\n?\nTu es\nautonome\n,\nrigoureux\n, et\nbon communiquant\n?\nTu souhaites participer √† un\nprojet d‚Äôenvergure associant cloud et Big Data\n?\nEt la suite ?\nTu rencontres d‚Äôabord l‚Äô√©quipe RH pour parler de tes attentes, ton projet, ton futur !\nPuis les managers pour parler concret : missions, projets, parcours de carri√®re, et bien s√ªr salaire et avantages J\nEt tu discutes avec un de nos Tech Leads, pour √©valuer tes comp√©tences/ te challenger.\nLes infos en plus !\nT√©l√©travail ! üòä\nUn salaire attractif en fonction de ton exp√©rience + diff√©rents avantages\nUn groupe en pleine croissance avec un management bienveillant\nEt une √©volution personnalis√©e avec la possibilit√© de se former via une plateforme interne\nTu souhaites donner un nouvel √©lan √† ta carri√®re ? Rejoins la vie Apsidienne !\nPour en savoir plus √†\nwww.apside.com\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Salaire",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Siderlog Conseil",
        "location": "Niort, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-developpeur-talend-at-siderlog-conseil-3861714639?position=53&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=IHXQMJ2Ij4wKcXuVkn7anQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Siderlog est un cabinet de conseil sp√©cialis√© implant√© √† Niort depuis 2004 qui accompagne les directions m√©tiers et SI sur des projets de:\n- Business et Data Analyse\n- Management de projets\n- Conduite du changement\nPour soutenir notre croissance, nous pr√©voyons √† Niort le recrutement de 20 consultants d'ici 2025.\nNos consultants b√©n√©ficient d'un mod√®le qui favorise l'√©panouissement professionnel et le bien √™tre:\nüçÉUn processus d'int√©gration sp√©cifique et un suivi r√©gulier\nüçÉUne √©coute active des attentes, notamment en terme de formations, certifications\nüçÉDes d√©jeuners et √©v√®nements mensuels\nüçÉUn management et un accompagnement de proximit√©\nüçÉUn package salarial attractif\nüçÉLa possibilit√© de contribuer aux projets d'entreprise ( RSE, communaut√©s m√©tiers, p√¥le conseil et expertise)\nüçÉEntreprise labellis√©e Happy At Work, charte T√©l√©travail...\nüçÉDe nombreux autres avantages que nous vous invitons √† venir d√©couvrir\nSiderlog recherche pour renforcer son √©quipe, √† Niort un(e) consultant(e) Data Engineer / Developpeur Talend.\nDans ce cadre vous devrez :\n‚úîÔ∏èConcevoir et d√©velopper des traitements/job de donn√©es complexes √† l'aide de Talend pour l'ingestion, le nettoyage, la transformation et la distribution des donn√©es.\n‚úîÔ∏èCollaborer √©troitement avec les √©quipes m√©tier pour comprendre les besoins en mati√®re de donn√©es et concevoir des solutions adapt√©es.\n‚úîÔ∏èMettre en ≈ìuvre des bonnes pratiques de d√©veloppement ETL, y compris la documentation, les tests unitaires et l'int√©gration continue.\n‚úîÔ∏èAssurer la surveillance et la maintenance des traitements/job de donn√©es en production, en r√©solvant les incidents et en effectuant des mises √† jour si n√©cessaire.\nüìã Qualifications et comp√©tences :\nüëâExp√©rience av√©r√©e dans le d√©veloppement de solutions de gestion et d'int√©gration de donn√©es, sur Talend.\nüëâMa√Ætrise des langages de requ√™te SQL pour l'extraction et la manipulation des donn√©es.\nüëâConnaissance approfondie des bases de donn√©es relationnelles et des entrep√¥ts de donn√©es.\nüëâComp√©tences en programmation avec Java, Python ou d'autres langages similaires.\nüëâCapacit√© √† travailler de mani√®re autonome tout en collaborant efficacement avec les membres de l'√©quipe.\nüëâExcellentes comp√©tences en communication √©crite et verbale.\nüëâMaitrise de l'outil ETL Talend.\nüëâExp√©rience avec d'autres outils d'int√©gration de donn√©es tels que Informatica, BODS, Alt√©ryx.\nüëâCertification Talend serait un plus.\nüëâExp√©rience dans le domaine de l'assurance souhait√©e\nCette offre vous int√©resse ! Postulez !\nüèÜüôèüöÄüéâ !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "bsport",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/full-stack-data-engineer-at-bsport-3848363421?position=54&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=zVntVhMlT7QiaTnDEAUXjA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Do you know about bsport?\nWe are a Barcelona based company that offers a platform combining boutique fitness and advanced technology. Our all-in-one features cover bookings, payroll, marketing and more, helping our partners streamline operations and boost their commercial success.\nWe have more than 2‚Äô000 clients in 40+ countries and continue to expand rapidly.\nWe provide our partners with:\nOur platform - the heart of the system (B2B)\nA white label iOS and Android mobile application (B2C)\nAn integrated Video on Demand tool\nOur self-built Smart Marketing Suite\nA Webshop to up- and cross-sell different products\nOur first successes\nSince we launched in 2019, we have already achieved the following:\nWe‚Äôve built a community of over 6 million users\nFinalised a Series A Fundraising of $4+ million in December 2022\nGrown our team to more than 150 employees\nWe‚Äôre continuing to grow our team to become the #1 tech partner for boutique studios in Europe and the rest of the world!\nWe are seeking a talented and experienced Full Stack Data Engineer to join our dynamic tech team based in Paris. As a Full Stack Data Engineer, you will play a crucial role in enhancing our data platform and driving innovation through data engineering solutions.\nWhat your future position looks like:\nThe primary focus of this position is data engineering, encompassing tasks such as building, optimizing, and maintaining our data platform. You will be responsible for making continuous improvements to our data infrastructure to ensure its reliability, scalability, and performance. As an integral part of the data team, you will collaborate closely with team members to address various data-related challenges and opportunities. This may involve tasks ranging from designing and implementing data pipelines to conducting in-depth data analysis to extract actionable insights.\nThe role will focus on:\nBuild and maintain bsport‚Äôs data architecture\nEnsure the sustainability and scalability of the diverse components by leveraging bsport's cloud provider services and adhering to all DevOps best practices\nYou will enjoy working within an internal team of 25 people, consisting of Tech, Product, and Data experts, directly surrounded by our Senior Leads and CTO.\nOur stack is fully automated with push-to-deploy on both frontend and backend. We use Kubernetes and AWS, and our CI is self-hosted. Our methodology is based on agile principles, with weekly releases to production and staging to iterate, gather feedback, and drive progress.\nYou will be a good fit to join us if you:\nAlready built or maintained a data architecture at scale in a top cloud provider (AWS, GCP or Snowflake)\nAlready deployed data science models in production or built a data ingestion pipeline\nFamiliar with DevOps best practices\nProficiency in SQL, Python and Spark\nExperience with dbt and airbyte\nQualifications\nBachelor‚Äôs degree in Data Science, Computer Science, Engineering, or related field; advanced degree preferred.\nRelevant experience in data engineering\nStrong analytical and problem-solving skills, with the ability to work independently and in a team environment.\nWe'd love to have you join us for many reasons, such as:\nüåç A multicultural and international team!\nüöÄ A stimulating SaaS environment within a supportive and a fast-growing company\nüîã Enjoy 25 days of paid leave to recharge\nüè° Embrace days of remote work\nüè¢ Work from our stunning office in the heart of Bastille\n‚ù§Ô∏è‚Äçü©π\nHealth insurance half covered\nüõµ Public Transportation half covered\nüèÑüèΩ Take part in bsport team building and sport initiatives\nüõåüèΩ Supported by bsport on sick days\nInterview Process\nFirst interview with one of our Talent Acquisition team members (30 min)\nTechnical Interview with our Lead Data (1h30)\nTechnical Interview with our CPO (1h)\nFinal Interview with our CTO (1h)\nAbout our Company Culture:\nAt bsport, we collaborate with passionate individuals who value diverse ideas and backgrounds. We believe that diversity is our most valuable asset.\nOur commitment is to foster a positive and inclusive culture. We achieve this through team-building initiatives, open communication, professional growth opportunities, and by celebrating diversity in all its forms.\nWe value and respect every individual who is eager to make a difference, empowering them to contribute their unique skills and perspectives. Join our dedicated team to help create a thriving and welcoming workplace.\nJoin our team of passionate and committed people who are dedicated to creating a thriving and welcoming environment. Let's make it happen together!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full",
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ippon Technologies",
        "location": "Nantes",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=55&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=mHG%2FOUZA7Y3wljivPZOIeQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre la communaut√© DATA la plus dynamique de France ?\nNotre sp√©cialit√© est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.\nMembre de la Practice Data, le/la futur(e) Data Engineer sera int√©gr√©(e) √† nos √©quipes de conseil et sera suivi(e) par un(e) mentor qui l‚Äôaidera √† monter en comp√©tences.\nVotre champs d‚Äôexpertise :\nIntervenir sur les data platforms de nos clients pour d√©velopper de nouveaux pipelines de donn√©es (ingestion, traitement, exposition).\nTravailler en collaboration avec les m√©tiers et les data scientists pour leur fournir un support √† l‚Äôindustrialisation de leurs travaux (tests, int√©gration continue, scalabilit√© des mod√®les, craftsmanship etc‚Ä¶)\nD√©ployer des infrastructures cloud full\ninfra-as-code\n(Terraform, CloudFormation).\nParticiper aux √©v√®nements internes √† la communaut√© data (BBL, webinar, datap√©ro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).\nCapitaliser sur les missions et les diff√©rents √©v√®nements de la communaut√© au travers d‚Äôarticles de blogs, REX, BBL interne.\nVos connaissances :\nUn framework de calcul distribu√© tel que Spark, Storm, Flink.\nUn ou plusieurs langages de programmation (Python, Scala, Java...)\nDiff√©rents syst√®mes de stockage de donn√©es (SQL ou NoSQL) et bien s√ªr le langage SQL.\nLa connaissance de Snowflake est bienvenue ;-)\nUn framework de streaming de donn√©es tel que Kafka ou Amazon Kinesis.\nUne exp√©rience sur les technologies Cloud : AWS, GCP, Azure\nLe delivery et les projets en production faisant partie de notre ADN, vous √™tes capable de livrer du code de qualit√© dans des environnements agiles.\nDe plus en plus de nos projets se font en remote avec des clients du monde entier, il devient n√©cessaire d‚Äô√™tre √† l‚Äôaise en Anglais.\nIppon technologies c‚Äôest aussi :\nüëç B√©n√©ficier d'un suivi de proximit√© r√©alis√© par votre manager technique : points r√©guliers pour votre suivi en mission, votre formation et votre √©volution de carri√®re\n‚úåÔ∏è Rejoindre une entreprise o√π les valeurs du sport sont nos leitmotiv : d√©passement de soi, travail en √©quipe, bienveillance.\nüóíÔ∏è Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL\nüòÅ Travailler en pair programming ou avec un.e mentor pour gravir les √©chelons !\nüí™ Pouvoir participer √† une aventure humaine au sein de notre Fondation Ippon pour r√©duire la fracture num√©rique dans le monde !\nü§ù Participer √† nos ap√©ros et divers √©v√®nements internes pour consolider la coh√©sion d‚Äô√©quipe\nEt apr√®s ?\nEt oui alors ? Que se passe-t-il une fois que vous √™tes convaincu d‚Äôavoir lu l‚Äôoffre d‚Äôemploi qui vous correspond bien ?\nNous vous proposons de prendre contact et de nous rencontrer !\nLes Next Steps :\n1 call RH\n1 √©change RH\n1 √©change Technique\nSi le match est bon des deux c√¥t√©s : Hadjim√© ! Vous vous lancerez sur le tatami Ippon !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "InfrastructureAsCode": [
                "CloudFormation",
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Equativ",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-equativ-3814251519?position=56&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=d9nf6mGNenymXJjW70Bn6g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üë´ About the team\nAt Equativ, we‚Äôre on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 100 billions requests per day and above 40 Gbps of network traffic.\nOur innovation team based in Paris, Nantes, Limoges, Krak√≥w and Berlin is composed of 90 straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.\nOur data engineering team is composed of 10 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 140+ engineers spread across Paris, Nantes, Limoges, Krak√≥w and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.\nOur mission üëá\nData Engineering team is central to Equativ‚Äôs data centric business and is responsible to ingest, transform, model and redistribute all data coming from our adtech platform.\nWe aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle massive log ingestion (tens of billions per day), short & long term data storage, complex data modelling, real-time and batch ELT as well as providing external access through dedicated APIs.\nData Engineers serve Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science algorithms (clustering and optimization), customer reporting, invoicing and more.\nEquativ Data Engineering team is engaged in an ambitious migration of its main data stack (Hadoop on-premise) to GCP with the objective to increase reporting features, lower maintenance time, improve performances and simplify the access to our raw data.\nWhat you'll do ‚úèÔ∏è\nAs a Big Data Engineer, you‚Äôll primarily focus on maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs\n-Design, develop, test, promote and industrialize all data components from data ingestion to datawarehouse delivery (ClickHouse, BigQuery):\nPropose and develop innovative solutions to achieve the best levels of scalability and performance for our Big Data engines\nAutomate and streamline our real-time and batch data pipelines (on-premise and in the cloud) in order to simplify access to our data by other teams and lower amount of work spent by other teams on ETL processes\nPerform end-to-end monitoring to ensure high availability of production data processing, data quality and reliability\nApply best in class Devops guidelines and secure deployments\n-Brainstorm with other team members working on our data backend (datawarehouse modelling and data exposure through our reporting APIs) on optimizing our architecture and support them in the use of our pipelines\n-Contribute to data roadmap definition in coordination with other R&D and product teams in order to build a best in class data infrastructure that will generate insights for Equativ‚Äôs analytics\n-Take part in improving and deploy data engineering standards, procedures, processes and operational guidelines around target data components at Equativ\nüí™ About you\nMaster degree in Computer Science or similar technical field of study\n3+ years of software development with open source technologies\nFluent in Java and/or in Scala. SQL mastery\nVery good understanding of Devops principles (Gitlab, Docker, Kubernetes, Gradle, ci/cd)\nExperience with large-scale data engineering technologies (ClickHouse, Flink, Kafka, Hadoop, Spark, Hbase)\nExperience on building data pipelines on Google Cloud Platform (BigQuery, Dataflow, GCS, Cloud Run, Airflow ‚Ä¶) would be a big plus\nExperience in working with high QPS Rest APIs is a plus\nEntrepreneurial spirit and know-how to identify opportunities of improvement\nWorking proficiency and communication skills in verbal and written English\nPassion for playing with large volume of data\nüöÄ How you'll grow\nWithin 1 month:\nYou'll be just finishing your onboarding.\nYou'll probably have tackled a few small tasks in peer-coding\nWithin 4 months:\nYou'll have an overview of 50% of the stack, CI/CD and team‚Äôs main processes. You‚Äôll be able to work on more complex developments\nYou'll now have enough knowledge to participate to deployments of chosen applications\nWithin 9 months:\nYou'll be autonomous on most of our stack and will have participated to major projects\nYou‚Äôll be helping the team on production matters\nüëã About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus ‚Äî four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times‚Äô FT 1000: Europe‚Äôs Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala"
            ],
            "DataBase": [
                "SQL",
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-cloud-f-h-at-apside-3904088503?position=57&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=I%2BZTRr6Z9jhcj14PZMK92A%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üí•\nD√©couvrez la Vie Apsidienne\nüìπ\net vous aussi, devenez Apsidien\nOn aurait pu demander √† Chat GPT de vous d√©montrer en quoi\nApside est l‚ÄôESN qu‚Äôil vous faut,\nmais on pr√©f√®re que vous le d√©couvriez vous-m√™mes üëáüòè\nüî•\nD√©couvrez votre future mission\nüëâ\nContexte\nRejoignez notre Practise Cloud/Data, afin d‚Äôintervenir sur des sujets √† haute valeur ajout√©e !\nNotre\nclient migre actuellement toutes ses applications vers le cloud AWS.\nDe plus, dans le cadre du d√©veloppement d'un produit de restitution automatis√©e de donn√©es, ils recherchent actuellement d√©veloppeur data ayant d√©j√† travaill√© sur un projet similaire. La solution produit est techniquement con√ßue en lien avec le Tech Lead validant l'architecture logicielle √† mettre en place sur le cloud AWS.\nSecteur\n: culture/m√©dia\nM√©thode de travail\n: Agile Safe\nüòé Mission\nCapter les donn√©es (structur√©es et non structur√©es) produites dans les diff√©rentes applications\nInt√©grer les √©l√©ments\nStructurer la donn√©e (s√©mantique, etc‚Ä¶)\nCartographier les √©l√©ments √† disposition\nNettoyer la donn√©e (√©limination des doublons, etc‚Ä¶)\nValider la donn√©e\nCr√©er les r√©f√©rentiels de donn√©es\nEnvironnement technique\n:\nPython\nLambda\nStep Function\nAWS / AWS RDS\nPostegreSQL\nSnowflake\nSpark\nüìç\nLocalisation\nLa D√©fense\nüí∞\nLe package salarial que nous vous proposons\nContrat :\nCDI\nAvantages groupe :\ncarte ticket restaurant Swile, prime de mobilit√©, RTT, accord t√©l√©travail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle √† 100% etc‚Ä¶\nAvantages agence :\nCommunaut√© Cloud/Data, afterworks, communaut√© techlead\nFormation :\ncertifications techniques, cours particuliers d‚Äôanglais en interne, acc√®s √† un catalogue de formations gr√¢ce √† notre plateforme e-learning (\nAcademy by Apside\n) ou via nos organismes partenaires.\nüîÆ\n√î vous futur Apsidien, qui √™tes-vous ?\nAu moins 4 ans d'exp√©rience en tant que Data Engineer\nMaitrise de l‚Äôenvironnement cloud AWS\nForce de proposition, bon relationnel et autonome\nüòè\nApside a suscit√© votre curiosit√© ?\nDans un environnement marqu√© par une acc√©l√©ration des √©volutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients √† cr√©er de la valeur et √† adresser leurs enjeux strat√©giques en leur mettant √† disposition des expertises technologiques (\nData / IA, Cloud, Cyber\n) et une exp√©rience sectorielle (\nIndustrie, Banque, Assurance, Service, Secteur Public\n). Pour un accompagnement global, le groupe propose des offres transverses autour du\nHandicap\n(Apsid‚ÄôEA), du\nDigital Learning\n, et du\nConseil\n.\nü§î\nEt votre place dans tout √ßa ?\nüëâ Notre volont√©\nest de vous accompagner dans la construction et l‚Äô√©panouissement de votre carri√®re\nen nous appuyant notamment\nsur 3 piliers :\nUne\nr√©mun√©ration\n√† hauteur de vos investissements et de vos comp√©tences\nUne\ntrajectoire professionnelle\nstimulante sur mesure\nUn\nengagement\nautour des valeurs Apsidiennes : la qualit√© de vie et des conditions de travail au c≈ìur de nos enjeux\nEngag√©e pour\nun monde plus inclusif et plus responsable\n, Apside r√©invente l‚ÄôESN et propose l‚ÄôEngagement Soci√©tal et Num√©rique. D√©couvrez notre d√©marche RSE ainsi que notre vision de l‚ÄôEntreprise Engag√©e.\nConvaincu ? A vous de jouer, envoyez-nous votre CV !\nRejoignez l‚Äôaventure Apsidienne et d√©couvrez notre vision d‚Äôune ESN singuli√®re et r√©siliente\nüöÄ\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "100",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Carrefour",
        "location": "Massy, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-en-alternance-at-carrefour-3884390443?position=58&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=fpp9k2zsbFGYqI5rGtDPMQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le saviez-vous ?\nNous rejoindre, c‚Äôest rejoindre l‚Äôun des leaders mondiaux de la distribution qui met l'accent au quotidien sur la diversit√©, la RSE et le digital, pour satisfaire nos clients et nos collaborateurs. En tant que partenaire premium des Jeux Olympiques et Paralympiques de Paris 2024, nous partageons les valeurs du sport en permettant √† nos √©quipes de se d√©passer et encourageons une alimentation saine au juste prix pour tous.\nVous cherchez √† travailler dans une entreprise dynamique o√π votre travail rime avec impact social et environnemental ? Bienvenue chez nous !\nData Engineer (F/H) - en alternance\nEn tant qu' alternant, vous int√©grerez la plateforme supply chain o√π vous serez amen√© √† appuyer le p√¥le pr√©vision et optimisation particuli√®rement sur des sujets data et d‚Äôanalyse de donn√©es.\nAu sein d'une √©quipe compos√©e de data scientists et de data engineers organis√©e en mode Scrum Agile, vous travaillerez pour am√©liorer au quotidien un outil de calcul de pr√©vision (pr√©vision de la demande des entrep√¥ts Carrefour). Vous participez √† l'√©volution fonctionnelle et technique de l'application.\nüéØ Les missions\nDans ce cadre, vous serez amen√© √†\nExplorer et analyser les donn√©es du datalake carrefour\nParticiper au cadrage des nouvelles fonctionnalit√©s\nD√©velopper les √©volutions des traitements, des mod√®les statistiques et de machine learning de pr√©vision et des reporting\nTester les fonctionnalit√©s d√©velopp√©es\nR√©pondre aux demandes utilisateurs\nüë• Profil\nVous √™tes en √©cole d‚Äôing√©nieur, en master 2 ou √©quivalent avec une sp√©cialisation data science, data engineering, statistique, informatique.\nVous avez une exp√©rience en traitement et analyse de donn√©es.\nVous avez un esprit d‚Äôanalyse et la capacit√© de travailler en √©quipe et √† distance.\nVous √™tes autonome et rigoureux, fluide dans votre communication orale et √©crite et √† l'√©coute des besoins de vos interlocuteurs.\nVous √™tes reconnu pour vos capacit√©s d'anticipation, votre sens de l'initiative et votre r√©activit√©.\nVous avez une bonne connaissance des langages suivants\nSQL\nPython\nUne connaissance de GCP et de Big query serait un plus.\nUne connaissance m√™me th√©orique de la m√©thodologie agile serait un plus\nUne connaissance de GIT serait un plus.\nEncore plus de bonnes raisons de nous rejoindre\nInt√©grer une √©quipe conviviale √† taille humaine au sein d‚Äòun grand groupe\n12 % de remise sur achat\nüìù Informations compl√©mentaires\nDate de d√©but  09 septembre 2024\nDur√©e  1 an\nLieu  Lyon\nD√©placements en magasin et en concurrence dans la r√©gion parisienne\nAvantages 50 % du titre de transport pris en charge par Carrefour\nEnvie de rejoindre l‚Äôaventure ?\nChez Carrefour, nous avons √† c≈ìur de ne passer √† c√¥t√© d‚Äôaucun talent et sommes fiers de compter des √©quipes repr√©sentatives de la soci√©t√© dans son ensemble. Nous encourageons ainsi tous types de profils √† postuler √† cette offre et garantissons un processus de recrutement d√©nu√© de toutes formes de discriminations.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Other": [
                "Statistiques",
                "Machine Learning"
            ],
            "EnSoftSkils": [
                "Initiative",
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=59&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=FOcexJ8yPYHe2I%2F%2FMuM8YA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise o√π vous serez en mesure de fa√ßonner votre carri√®re selon vos aspirations, o√π vous serez soutenu et inspir√© par une communaut√© d‚Äôexperts dans le monde entier, o√π vous pourrez r√©√©crire votre futur. Rejoignez-nous pour red√©finir les limites de ce qui est possible, contribuer √† lib√©rer la valeur de la technologie pour les plus grandes organisations et participez √† la construction d‚Äôun monde plus durable et inclusif.\nVos missions :\nInt√©gr√©(e) au sein d'une √©quipe projets intervenant pour des clients dans des secteurs d'activit√©s vari√©es, vous serez notamment en charge des missions suivantes :\nConcevoir et mettre en oeuvre des strat√©gies s√©curis√©es d'acquisition et d'int√©gration de donn√©es,\nConfigurer des r√©f√©rentiels de donn√©es √† la pointe de la technologie dans des environnements distribu√©s, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),\nConstruire des pipelines de donn√©es pour collecter, transformer et traiter des donn√©es en collaboration avec des scientifiques de donn√©es afin de r√©pondre aux exigences de la mod√©lisation de donn√©es d'analyse avanc√©e.\nVotre profil :\nDipl√¥me d‚Äôing√©nieur ou √©quivalent universitaire\nMinimum 3 ans d'exp√©rience\nAnglais courant\nMa√Ætrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.\nExpertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks‚Ä¶\n3 raisons de nous rejoindre :\nQualit√© de vie au travail : accord de t√©l√©travail en France et √† l‚Äôinternational, accord sur l‚Äô√©galit√©\nprofessionnelle, la parentalit√©, l‚Äô√©quilibre des temps et la mobilit√© durable.\nApprentissage en continu : certifications et formations en libre acc√®s, accompagnement sur mesure avec votre carreer manager, parcours d‚Äôint√©gration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activit√©s √† tarifs pr√©f√©rentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorit√©s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int√©grez un collectif qui valorise la diversit√©, d√©veloppe le potentiel de ses talents, s‚Äôengage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r√©duire son impact environnemental sur tous ses sites et aupr√®s de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr√®s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d‚Äôexp√©rience, nous sommes un partenaire strat√©gique des entreprises pour la transformation de leurs activit√©s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp√©tuelle √©volution tels que le cloud, la data, l‚ÄôIntelligence Artificielle, la connectivit√©, les logiciels, l‚Äôing√©nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Marseille, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896997028?position=60&pageNum=0&refId=h6vkabfUCZSEdA8tbXvPmQ%3D%3D&trackingId=mVauwVbbjFSlaeW9klBsag%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Boulanger",
        "location": "Lesquin, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-boulanger-3854554057?position=1&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=Av0nle7xMrq%2F2medaWCbng%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Au sein de la direction informatique, le p√¥le DATA a pour missions de maximiser la mise en valeur des donn√©es de BOULANGER ,ELECTRO-DEPOT et KREFEL/HIFI afin d‚Äôaider nos d√©cideurs √† agir sur les leviers de leur performance par des processus d√©cisionnels efficients.\nAu sein de ce p√¥le, tu prendras en charge un large domaine m√©tier qu'il te faudra maitriser de bout en bout : de la donn√©es brutes, sa transformation jusqu'√† son exposition dans les reporting. Cela afin d'en assurer le bon fonctionnement, les √©volutions constantes et sa p√©rennit√©\nTes t√¢ches principales portent sur :\nLe pilotage et la mise en ≈ìuvre de projets DATA.\nLa collecte, le stockage et l‚Äôexploitation fluides des donn√©es par le d√©veloppement de solutions\nMissions\nMaitriser les r√®gles fonctionnelles et les KPI de ton domaine afin de challenger les m√©tiers dans les √©volutions et les nouveaux projets\nAccompagner des √©quipes m√©tiers dans leurs travaux d‚Äôidentification et expression des besoins sur la data\nParticiper aux ateliers de conception et d√©veloppement des applications data\nMod√©liser la solution √† mettre en ≈ìuvre\nConcevoir et mettre (ou faire mettre) en ≈ìuvre des flux les pipelines d‚Äôint√©gration (en mode batch ou fil de l'eau) de donn√©es structur√©es/semi-structur√©es\nTransformer les donn√©es : consolider, enrichir et optimiser les donn√©es, qui seront exploit√©es par le m√©tier\nCr√©er, faire √©voluer et optimiser les restitutions\nSuivre et animer les d√©veloppeurs (ETL, restitution, self-BI internes ou externes)\nG√©rer le RUN\nMaitrise le SQL et la base de donn√©es (Oracle, Snowflake)\nMa√Ætrise d‚Äôoutils de restitution (tel que Business Object (BO), PowerBI‚Ä¶)\nCapacit√© relationnelle, rigueur et dynamisme\nMa√Ætrise un ou plusieurs outils de pr√©paration et traitement de la donn√©e (DataStage, Stambia, ...)\nCapacit√© √† s‚Äôadapter √† tout type d‚Äôinterlocuteurs (technique, m√©tiers, Direction)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Neosoft",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-neosoft-3877878521?position=2&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=7ObkquXOFViiSrYhqZHL9w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Tous nos postes sont ouverts au t√©l√©travail\nGroupe ind√©pendant de conseil en transformation digitale de pr√®s de 1800 collaborateurs, N√©osoft s‚Äôest construit, depuis 2005, sur un mod√®le qui place l‚Äôexcellence, le d√©passement de soi et la RSE au c≈ìur de sa strat√©gie.\nEn nous rejoignant, vous int√©grez des communaut√©s d‚Äôexperts et de talents qui vous permettent de d√©velopper vos comp√©tences et d‚Äôoffrir √† nos clients le meilleur accompagnement possible.\nNotre savoir-faire s‚Äôarticule autour de nos 6 domaines d‚Äôexpertise :\nConseil & Agilit√©\nCybers√©curit√©\nData\nDevOps\nInfrastructures & Cloud\nSoftware Engineering\nNous recherchons pour int√©grer notre\nagence lilloise\nun(e)\nData Engineer confirm√©(e)\n.\nNous aimerions vous voir rayonner au sein de notre communaut√© DATA (+100 collaborateurs) anim√©e par Nicolas Huche, son practice leader et Thibaud Blanchard son Technical Officer. Vous aiderez les clients √† consolider un patrimoine Data responsable.\nüéØ\nVos missions :\nApr√®s une p√©riode d‚Äôint√©gration, en tant que\nData Engineer\n, voici √† quoi ressembleront vos activit√©s dans des contextes clients Retail ou Banque / Assurance / Finance :\nAnalyser et s'approprier les cas d'usages\nAnalyser et valoriser les donn√©es du patrimoine\nMettre en place des flux de transformation de donn√©es\nR√©aliser les tests permettant de s'assurer la qualit√© du delivery\nContinuer la mise au point de frameworks data\nCr√©er et d√©velopper des modules de d√©ploiement des solutions\nAssurer l'industrialisation de moteurs bas√©s sur l'IA\nAssurer le niveau de performance des pipelines\nImpl√©menter les outils de monitoring du socles de donn√©es\nüìù\nVotre profil :\nNous vous imaginons avec au moins 4 ans d‚Äôexp√©riences sur des projets autour de la\nData\n, une ma√Ætrise des\nbases de donn√©es (SQL)\n, des outils de transformation de la donn√©e\n(Talend, BigQuery, Airflow)\n, et un socle de comp√©tences solides autours des langages\nPython, Spark, Scala, Hadoop, Java.\nüëâ\nVotre carri√®re chez N√©osoft\nDepuis sa cr√©ation, N√©osoft place ses collaborateurs au c≈ìur de sa strat√©gie. Notre culture pourrait se r√©sumer en un mot : le collectif.\nNos communaut√©s d‚Äôexperts vous donnent la possibilit√© d‚Äôapprendre, mais aussi de transmettre et de partager vos savoirs pour faire progresser les autres.\nNous veillons √† ce que chacun b√©n√©ficie d‚Äôun accompagnement de proximit√© et d‚Äôun suivi de carri√®re personnalis√© aupr√®s de votre manager d√©di√© :\n1 bilan d‚Äôactivit√© trimestriel pour suivre le d√©veloppement de vos comp√©tences\n1 entretien d‚Äô√©valuation qui a lieu chaque ann√©e pour √©valuer votre performance et d√©terminer vos nouveaux objectifs\n1 entretien annuel aupr√®s de votre RH dans le but de cartographier vos nouvelles comp√©tences pour √©changer sur vos projets professionnels et souhaits de formations\nüëâ\nVos avantages\nFormations et d√©veloppement de l‚Äôexpertise :\nVous disposez de temps allou√© et r√©mun√©r√© en contribuant au d√©veloppement de votre expertise technique et de celle du groupe (Participations √† des Tech days, animation d‚Äôune conf√©rence √† l‚Äôinterne ou √† l‚Äôexterne, r√©daction d‚Äôarticles, rencontres avec nos candidats en processus de recrutement‚Ä¶)\nUn abonnement illimit√© LinkedIn Learning offert\nBien-√™tre au travail :\nUn accord de t√©l√©travail flexible jusqu‚Äô√† 100% de t√©l√©travail et personnalisable\nUn partenariat avec Gymlib qui favorise le sport en entreprise\nDes initiatives locales (afterworks, d√©fis sportifs, team buildings, ‚Ä¶)\nEt bien plus encore :\nParce que les meilleurs cooptent les meilleurs, une politique de cooptation attractive r√©mun√©r√©e d√®s l‚Äôarriv√©e du collaborateur\nEn plus de votre salaire : participation, compte √©pargne temps, actionnariat...\nüëâ\nVotre parcours candidat\nNotre processus de recrutement se compose de deux √©tapes cl√©s :\nUn entretien de recrutement RH avec un Talent Acquisition Sp√©cialiste pour dresser un bilan de votre parcours professionnel et identifier les trajectoires de carri√®re possibles au sein de notre groupe\nUn entretien d‚Äô√©valuation technique pour r√©aliser un diagnostic de vos comp√©tences techniques et identifier les comp√©tences sur lesquels poursuivre votre √©volution\nVous aurez √©galement la possibilit√© de rencontrer pour compl√©ter votre processus un acteur de notre p√¥le Business ou un pair de votre m√©tier pour √©changer sur son exp√©rience collaborateur.\nNous avons h√¢te de vous rencontrer !\nA bient√¥t,\nL‚Äô√©quipe N√©osoft üñê\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirm√©"
            ],
            "TypeContract": "",
            "Salary": "Salaire",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Bordeaux, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bordeaux-at-capgemini-3889788624?position=3&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=pSrQIJJFQcDAjttBdi%2F%2FHw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise o√π vous serez en mesure de fa√ßonner votre carri√®re selon vos aspirations. Avec le soutien et l'inspiration d'une\ncommunaut√© d‚Äôexperts dans le monde entier, vous pourrez r√©√©crire votre futur. Rejoignez-nous pour red√©finir les limites de ce qui est possible, contribuer √† lib√©rer la\nvaleur de la technologie pour les plus grandes organisations et participer √† la construction d‚Äôun monde plus durable et inclusif.\nVos missions :\nVous √™tes passionn√© par le domaine de la Data, vous souhaitez prendre part √† des projets d'envergure, concevoir des solutions, les impl√©menter et les faire √©voluer?\nAlors rejoignez notre √©quipe Data Engineering Services au sein de Capgemini Cloud Infrastructure Services en tant que Data Engineer.\nVous avez acquis une exp√©rience solide dans le d√©veloppement, la mise en ≈ìuvre et l‚Äôoptimisation de solutions pour le traitement d'un grand volume de donn√©es, vous √™tes capable de cr√©er des solutions qui r√©pondent aux besoins m√©tiers et IT, alors rejoignez notre √©quipe d‚Äôexperts.\nEn qualit√© de Data engineer, vos missions sont les suivantes :\n‚ñ™ Concevoir et d√©velopper des solutions Data/IA.\n‚ñ™ Accompagner les M√©tier dans la compr√©hension et la mise en ≈ìuvre de solution orient√©es donn√©es.\n‚ñ™ Collaborer avec les Dev, les Ops, les experts infrastructures dans la construction de solutions et d‚Äôinfrastructures ax√©es sur les donn√©es.\n‚ñ™ G√©rer un √©cosyst√®me de partenaires data et assurer un haut niveau d'expertise\n‚ñ™ Assurer un r√¥le de veille technologique sur tous les outils autours de la data, de l‚ÄôIA et de la BI.\nVotre profil :\nVous √™tes issu d‚Äôune formation ing√©nieur ou √©quivalent bac+5 informatique sp√©cialis√©e en DATA et vous justifiez d‚Äôune exp√©rience de 3 √† 5 ans dans un r√¥le similaire. Expert dans une technologie de base de donn√©es relationnelle (PostgreSQL, Oracle...)\nExpert dans une technologie de base NoSQL (MongoDB, Cassandra...)\nVous maitrisez un framework de manipulation de donn√©es (Hadoop, Spark, Kafka...)\nVous maitrisez les concepts DevOps et avez de bonnes notions en scripting et d√©veloppement\nVous avez une exp√©rience des outils BI et de data visualisation (Kibana, PowerBI...)\nLa maitrise de l'anglais est n√©cessaire.\n3 raisons de nous rejoindre :\nQualit√© de vie au travail :\naccord de t√©l√©travail en France et √† l‚Äôinternational, accord sur l‚Äô√©galit√© professionnelle, la parentalit√©, l‚Äô√©quilibre des temps et la mobilit√© durable.\nApprentissage en continu :\ncertifications et formations en libre acc√®s, accompagnement sur mesure avec votre career manager, parcours d‚Äôint√©gration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, tarif pr√©f√©rentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culture\nNos engagements et priorit√©s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int√©grez un collectif qui valorise la diversit√©, d√©veloppe le potentiel de ses talents, s‚Äôengage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r√©duire son impact environnemental sur tous ses sites et aupr√®s de ses clients.\n√Ä propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr√®s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d‚Äôexp√©rience, nous sommes un partenaire strat√©gique des entreprises pour la transformation de leurs activit√©s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp√©tuelle √©volution tels que le cloud, la data, l‚ÄôIntelligence Artificielle, la connectivit√©, les logiciels, l‚Äôing√©nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "MongoDB",
                "Cassandra",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DBMS": [
                "PostgreSQL"
            ],
            "SoftBigDataProcessing": [
                "Cassandra"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "eXalt Value",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-exalt-value-3897767649?position=4&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=aJYgYrY%2FQ2YWFPS1t2rvFA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "eXalt\nest un cabinet de conseil IT\nPure player Data\n& IA bas√© √† Paris.\nNotre offre s‚Äôarticule autour de 4 piliers r√©unis au sein d‚Äôune m√™me communaut√© pour un accompagnement √† 360¬∞ alliant une expertise technique et m√©thodologique √† une approche conseil m√©tier:\nData Gouvernance & Project\nData Engineering & Big Data\nData Performance & Analytics\nData Science & IA\nFiliale du groupe eXalt, cr√©√© en 2018,\nregroupant plus de\n950 collaborateurs en France\n(Paris, Lyon, Bordeaux, Lille, Nantes, Marseille)\net √† l‚Äôinternational\n(Colombie, Etats-Unis, Espagne, Belgique),\neXalt Value\napporte une\nexpertise approfondie\ndans le domaine de la Data & IA et conseille les entreprises dans le d√©ploiement de leurs strat√©gies data-driven.\nB√©n√©ficiant du support du groupe eXalt\n(1er dans la cat√©gorie Conseil & Audit au classement des Champions de la Croissance 2024), eXalt Value\nest en pleine croissance et regroupe aujourd‚Äôhui une communaut√© d‚Äôexpertise de plus de 60 collaborateurs en r√©gion parisienne.\nNos consultants interviennent sur d\nes projets d‚Äôenvergure\ndans divers secteurs d‚Äôactivit√©,\nBanque & Assurance, M√©dias, Transports, Retail, Tourisme, etc.\nNous recherchons un\nData Engineer Confirm√© H/F (minimum 4 ans d'exp√©rience dans la fonction)\npour rejoindre notre communaut√© sur le\npilier Data Engineering & Big Data.\nVos missions:\nConcevoir et d√©velopper des pipelines et des flux de donn√©es.\nInt√©grer et transformer des donn√©es provenant de diff√©rentes sources.\nD√©velopper et mettre en ≈ìuvre des algorithmes de traitement de donn√©es avanc√©s.\nCollaborer √©troitement avec les √©quipes clients pour comprendre leurs besoins et fournir des solutions adapt√©es.\nAssurer la qualit√© et la fiabilit√© des solutions d√©velopp√©es.\nConseiller les √©quipes clients sur les solutions √† mettre en place.\nLes Pr√©requis :\nTitulaire d'un Bac+5, Ecole d'Ing√©nieur\nMa√Ætrise d'un ou plusieurs langages de programmation (\nPython, Scala, Spark, etc\n.).\nExp√©rience approfondie des technologies\nBig Data (Hadoop, Spark, Kafka, Talend, etc.)\nExp√©rience av√©r√©e\nen\nenvironnement Cloud (AWS, GCP, ou Azure)\n.\nSolides comp√©tences en conception et en optimisation de pipelines de donn√©es.\nExp√©rience de travail en\nm√©thode Agile\nCapacit√© √† travailler de mani√®re autonome et en √©quipe.\nExcellentes comp√©tences en communication et en r√©solution de probl√®mes.\nMa√Ætrise de l‚Äôanglais (oral & √©crit dans un contexte international professionnel).\nVotre environnement eXalt√©:\nUn environnement de travail Collaboratif\nfavorisant les initiatives et projets transverses √† la Practice Data & IA (Lab IA, Data Hub, etc.).\nUn collectif de consultants passionn√©s,\ns‚Äôint√©ressant aux tendances innovantes du secteur.\nUne Practice de proximit√©,\nprivil√©giant la mont√©e en comp√©tence de ses collaborateurs (formations, coachings, mentorats, etc.)\nUn suivi individualis√© et de proximit√©\npar un.e Data Sales Manager r√©f√©rent du compte client, un.e Charg√©.e RH et un.e Practice Manager\nUne √©quipe ouverte et dynamique,\nqui privil√©gie les moments de partage et de convivialit√© (s√©minaires, eXaltemps, meet-up, d√©jeuners d‚Äô√©quipe, etc.)\nNotre processus de recrutement :\nUn entretien RH avec Estelle,\n√† la suite duquel vous saurez tout (ou presque) d‚ÄôeXalt Value,\nUn entretien technique avec un Manager assorti d‚Äôun test technique,\nlors duquel vous aurez l‚Äôoccasion de d√©montrer vos talents mais aussi d‚Äôapprendre avant m√™me de dire oui,\nUn entretien final avec la Directrice Associ√©e ou le Directeur Op√©rationnel,\npour finir de vous convaincre de nous rejoindre üòä\nNous avons h√¢te de recevoir vos CV, et de faire votre connaissance!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirm√©"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "United Robotics Group",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-united-robotics-group-3891680780?position=5&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=va3%2FvkwzPqFEg1ZOXUYHcw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Bienvenue chez\nAldebaran\n, leader europ√©en de la robotique\nau sein du groupe\nUnited Robotics Group\n.\nNous concevons et industrialisons des robots innovants avec une vision soci√©tale ambitieuse pour fa√ßonner un monde plus humain. Depuis 2005, nous sommes √† l'avant-garde de l'interaction homme-robot avec des produits embl√©matiques tels que NAO et Pepper.\nNotre dernier-n√©,\nPlato\n,\nincarne notre engagement envers la technologie de pointe et la s√©curit√©,\nfabriqu√© en France avec des composants europ√©ens.\nRejoignez nos √©quipes multiculturelles et dynamiques pour √™tre au c≈ìur de la r√©volution de la robotique.\nSi vous √™tes passionn√©.e par la robotique et l'intelligence artificielle, et que vous souhaitez contribuer √† fa√ßonner l'avenir, nous vous offrons une exp√©rience enrichissante et stimulante.\nEn tant que membre de notre √©quipe, vous b√©n√©ficierez d'une culture d'entreprise ax√©e sur le sens de ce que nous faisons et valorisant la responsabilit√© sociale et environnementale.\nChez Aldebaran, nous valorisons l'innovation, la diversit√© et l'√©galit√© et encourageons chacun.e √† √™tre ouvert.e, authentique, courageux.se, responsable et engag√©.e.\nFinalit√© du poste\nAu sein de l'√©quipe Cloud-Online Services, le Data engineer int√©grera l'√©quipe Data, responsable du d√©veloppement des produits destin√©s √† la collecte, aux process et √† l'exploitation des donn√©es de nos robots.\nIl aura pour r√¥le de d√©finir et d'impl√©menter des services data, sur une infrastructure Cloud AWS, supportant des services en ligne qui g√®rent les robots du groupe.\nMissions principales\nLe Data engineer aura pour responsabilit√©s de :\n√©valuer les choix d'architecture et de solutions techniques lors de la mise en place de PoC,\nconcevoir et d√©velopper des services Data en respectant la sp√©cification fonctionnelle et la m√©thodologie agile,\nagr√©ger et stocker de grandes quantit√©s de donn√©es,\nmettre en place des solutions de data processing,\nint√©grer/d√©velopper des outils de visualisation de donn√©es et analyser les KPI,\nd√©velopper, tester, s√©lectionner et mettre en production des algorithmes qui permettent de r√©pondre aux besoins,\nr√©aliser des analyses de donn√©es,\nmettre en place des tests de charge et fonctionnels pour les solutions Data,\ninvestiguer et corriger les bugs remont√©s par les utilisateurs,\ncontribuer √† la mise en place de l'infrastructure et outil de d√©ploiement (CI/CD)\nRejoignez-nous pour faire partie d'une aventure passionnante o√π Pepper, NAO, Plato et leurs futurs successeurs attendent votre contribution pour repousser les limites de la technologie robotique !\nRequirements\nPour la bonne ex√©cution des missions confi√©es, vous t√©moignez d'au moins 6 ans d'exp√©rience en tant que d√©veloppeur sur des projets data en Cloud en Python et Spark et avec comme Cloud provider AWS.\nComp√©tences demand√©es :\nBonne compr√©hension des technologies d'infrastructure et de d√©ploiement,\nComp√©tences techniques sur les services AWS : IOT core , Glue, lambda, Kinesis, S3, RDS,\nBonne compr√©hension technique dans la mise en place et l'automatisation de tests de charge et fonctionnels,\nBonne maitrise d'outils BI ou de dashboarding (POWER BI, TABLEAU, QUICKSIGHT)\nBonne connaissance et une exp√©rience pratique de Scrum\\Scrumban et des m√©thodes agiles,\nUne certification AWS sera appr√©ci√©e,\nUn niveau de fran√ßais et d'anglais courant est indispensable,\nDes exp√©riences dans des environnements fortement internationaux sont un plus\nBenefits\nNos principaux avantages :\nUne culture du bien-√™tre en entreprise qui a fait ses preuves (budget c√©l√©bration et moments de convivialit√© par √©quipes et directions, restauration collective de qualit√©, environnement de travail agr√©able)\nUn engagement fort en mati√®re de responsabilit√© sociale et environnementale (promotion de l'√©galit√© professionnelle, performance de notre plan diversit√© et inclusion, r√©f√©rent handicap, fresque du num√©rique)\nUne culture du t√©l√©travail encadr√©e de mani√®re appropri√©e !\nTous nos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "6 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Coders Connect",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-coders-connect-3870419202?position=6&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=JR82LAk%2FERF273UoMx67Kg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Coders Connect and Sanofi are joining forces to bring an electrifying twist to the biopharmaceutical world!\nWork with a rhythm that suits your style (2 days remote and 3 days onsite magic).\nLanguage\n: Proficiency in English is required for this role to ensure effective communication within our diverse, global team.\nAbout Sanofi:\nWe're not just a company; we're a global movement, focused on human health and making a real difference. Our mission? To battle pain, ease suffering, and sprinkle a little bit of magic in the process by developing life-changing medicines and vaccines through breakthrough science and wizard-level technology.\nDigital & Data: The Pulse of Our Mission\nAt the heart of our quest lies our digital and data powerhouse. Think of us as the digital healthcare platform of your dreams, where innovation meets speed, and technology shakes hands with medicine. With our scale, deep-rooted connections in health ecosystems worldwide, and a knack for pushing boundaries, we're here to revolutionise medicine, one digital solution at a time.\nThe Role: Data Engineering Virtuoso\nAs our Data Engineering Virtuoso, you're tasked with designing and orchestrating the data pipelines that power our ambitious data analytics initiatives. You'll ensure our data's integrity and accessibility, laying the groundwork for groundbreaking insights and innovations.\nRequirements\nCloud Platforms: Proficient in AWS services, with Azure & GCP knowledge a plus. Your work involves leveraging cloud solutions for scalable data processing and storage.\nData Modeling & Query Performance: Expertise in crafting data models and optimizing SQL queries to enhance performance. Experience with Snowflake or similar data warehousing solutions is highly valued.\nIntegration Services: Skilled in utilizing Integration Services like IICS and Tibco, you facilitate seamless data flow and integration across various platforms.\nScripting & Development: Proficient in scripting languages such as Python and R, enabling you to automate tasks and manipulate data efficiently. Familiarity with GitHub for source code management underscores your commitment to collaborative development and version control.\nVisualization & Reporting: Knowledgeable in creating insightful data visualizations using tools like PowerBI, Tableau, or similar, turning complex data into actionable insights.\nData Governance & Compliance: A keen understanding of data quality, security, and governance standards, especially in healthcare environments subject to regulations like GxP, SOX, and data privacy laws.\nReal World Data & Standard Models: Experience with Real World Data (e.g., EHR, Claims) and familiarity with standard data models (e.g., OMOP, FHIR) enhance your ability to contribute to our healthcare objectives.\nPipeline Frameworks: Proficiency in using frameworks like Apache Airflow or Kedro for crafting efficient, reliable data pipelines that support our AI and ML initiatives.\nThe Reward:\nA chance to play a crucial role in a collaboration that's redefining healthcare through digital transformation.\nA seat at the round table of diversity and inclusion, where different backgrounds and experiences conjure the richness of our culture.\nAn endless horizon of professional growth, learning opportunities, and the chance to contribute to a future where better health is a global reality.\nThe Call to Adventure:\nIf you're ready to join a quest for better ‚Äì better treatments, better outcomes, and better science ‚Äì and believe in the magic of bringing diverse talents together to make miracles happen, we want you. Let's embark on this journey together and transform the future of healthcare.\nBetter is out there. Are you ready to find it with us?\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Web Transition",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-web-transition-3909147172?position=7&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=84yz5eoQP3z73ZQRUtCzzQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Web Transition, c‚Äôest qui ?\nFond√©e en 2011,\nWeb transition\nest une entreprise de services num√©riques op√©rant sur le march√© de l‚ÄôIT/Digital !\nConstituant une part essentielle de\nMoOngy Digital Lab\n, Web Transition accompagne ses clients grands comptes sur leurs projets de Webmarketing, de Design, Gestion de projet et √©galement en Data !\nNotre objectif : nous implanter comme un acteur principal sur le march√© de la Transformation Digitale en accompagnant et valorisant les comp√©tences de nos collaborateurs !\nNous sommes convaincus que le succ√®s de MoOngy Digital Lab r√©side dans la somme des potentiels de nos √©quipes ü§ù\nTon √©quipe : La tribu Data\nParce qu‚Äôil est indispensable que tu puisses partager tes connaissances mais aussi en acqu√©rir de nouvelles, tu feras partie de l‚Äôune de nos tribus : celle de la Data. De plus, cela te permettra d‚Äô√™tre acteur dans le d√©veloppement et la strat√©gie de Web Transition. Ce syst√®me de co-r√©flexion et co-construction est un fondement essentiel chez nous !\nDans cette aventure, tu :\nT‚Äôassures\nde la ma√Ætrise de la donn√©e et est garant de la qualit√© de son utilisation (r√©f√©rencement, normalisation, et qualification)\nTravailles\n√† la compr√©hension et l'int√©gration des donn√©es en provenance des diff√©rents formats\ndes interfaces de flux\n√©galement √† la d√©finition de la politique de la donn√©e et √† la structuration de son cycle de vie dans le respect des r√©glementations en vigueur\nla supervision et l'int√©gration des donn√©es de diverse nature qui proviennent de ces sources multiples et v√©rifie la qualit√© des donn√©es qui entrent dans le Data Lake\nGarantis\nl'acc√®s qualitatif aux sources de donn√©es\nFacilites\nl‚Äôacc√®s aux donn√©es pour tes coll√®gues (data scientists, data analysts‚Ä¶)\nAssistes\nles autres √©quipes dans l'acc√®s et la compr√©hension des donn√©es des socles.\nRejoins-nous si tu as :\nExp√©rience d‚Äôau-moins 4 ans dans la Data\nApp√©tence √† la qualit√© des donn√©es.\nConnaissance famili√®re des Datawarehouses.\nMaitrise de Python, Oracle SQL, GCP/Power BI\nAisance avec les indicateurs, tu as une bonne capacit√© d'analyse et de r√©daction.\nTon savoir-√™tre :\nOuvert d‚Äôesprit\nRigoureux\nAutonome\nRespectueux des diff√©rences de chacun\nCurieux\nProactif\nAgile\nPar o√π on commence ?\nUn premier entretien RH d‚Äô1h pour comprendre ton parcours et tes aspirations\nUn second entretien de 45 minutes avec l‚Äôun de nos Business Manager afin de valider tes comp√©tences et qu‚Äôil se projette sur l‚Äôune des missions qu‚Äôil pourrait te proposer\nUn troisi√®me entretien de quelques minutes avec notre responsable d‚Äôagence pour te proposer d‚Äôint√©grer notre superbe Team Web !\n3 entretiens en peu de temps, si ton profil correspond tu int√®greras tr√®s vite nos √©quipes üòâ\nPr√™t pour embarquer dans notre grande aventure humaine ? Deviens notre futur Weber en postulant √† cette offre ! Voici les avantages qui t‚Äôattendent en tant que Weber :\nü§© Des coll√®gues incroyables\nüèÜ Certifi√©e Great Place to Work\nüéÆ Des bureaux sympas (o√π vous serez toujours les bienvenus)\nüéâ Des teambuilding et √©vents tous les mois\nüíª Des tributs m√©tiers pour √©changer entre Weber du m√™me m√©tier\nDes missions chez le client qui sont accompagn√©es et coach√©es par ton manager\nUn accompagnement dans ton plan de carri√®re et tes envies de re skilling\nü§ì Un catalogue de formations certifiantes ouvert √† tous les salari√©s\nüçΩÔ∏è Une carte tickets restaurant MyEdenred\n‚ù§Ô∏è Une mutuelle GrasSavoye\nüöé Une prise en charge des frais de transport √† 100%\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "Oracle SQL"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Airswift",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-airswift-3909165766?position=8&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=6kRhAuUBrxZsA0vvv008UA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data Engineer\nLocation\n: Paris (Hybrid)\nContract type\n: 12 months +\nYears of Experience\n: 4+\nRecruitment Partner:\nAirswift\nKey Words:\nProject Management | Jira | Digiboard | Banking | Stakeholder Management | Architecture | Cloud | Payments/Credit |ServiceNow | PPM |\nResponsibilities\nDesign, develop, and implement data pipelines to collect, process, and store structured and unstructured data from various sources.\nCollaborate with data scientists, analysts, and other stakeholders to understand data requirements and translate them into technical solutions.\nOptimize and tune data pipelines for performance, scalability, and reliability.\nEnsure data quality and integrity throughout the data lifecycle, implementing data validation and monitoring processes.\nEvaluate and implement new tools and technologies to enhance our data infrastructure and capabilities.\nRequirements\n:\nExtensive experience in Python.\nStrong experience with data processing frameworks and tools such as Apache Spark.\nExperience with cloud platforms such as AWS, Azure, or Google Cloud Platform.\nSolid understanding of data modelling, database design, and SQL\nFrench and English speaking\nFreelancing opportunity\nThe next step\nWe have an exceptional team in place, and we are pleased to be able to appoint a further person to our growing business. We are aware that you may not ‚Äòtick all the boxes‚Äô, but if you believe you can genuinely offer some valuable skills and experience to our business, please in the first instance contact our recruitment partner Airswift.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Beelix",
        "location": "Antibes, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-beelix-3838611420?position=9&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=tWinPOqjhPLKXwnj%2BfvA6w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Qui sommes-nous ?\nDepuis 2016, nous accompagnons nos clients sur des probl√©matiques de Product Management, Data et Design Thinking. Beelix contribue √† fa√ßonner le monde de demain en participant aux grandes avanc√©es des secteurs suivants :\nüöóAutomobile\n‚ö°Energie\nüì°M√©dias & T√©l√©coms\nüëóLuxe & Retail\nüí∂ Banque, Finance & Assurance\n‚úàÔ∏èD√©fense\nAujourd‚Äôhui, Beelix compte plus de 200 collaborateurs motiv√©s et dynamiques. Lab√©lis√©e Great Place To work en 2023, Beelix est aussi une entreprise engag√©e o√π il fait bon vivre.\nDans le cadre de notre d√©veloppement, nous recherchons un Data Engineer (H/F) pour l'un de nos clients.\nQuelles missions au quotidien ?\n√ätre le leader de la brique Datalakehouse\nD√©velopper les scripts de transformations de donn√©es et les pipelines d‚Äôalimentation\nProposer des √©volutions architecturales ou de fonctionnalit√©s pour am√©liorer le socle technique\n√ätre le back-up du leader technique sur la partie reporting (Power BI)\nOrientation satisfaction client et r√©sultat final forte mais √©galement sensibilit√© au ¬´ comment ¬ª\nInnovation et proposition de nouvelles pratiques pour am√©liorer l‚Äôenvironnement et les conditions de travail des √©quipes\nA propos de vous ?\n5 + ann√©es d'exp√©rience en tant que Data Engineer\nMa√Ætrise des technologies suivantes : Microsoft Azure, Microsoft Azure Synapse Analytics (Spark / Python / Pipeline / Serverless), fichiers parquet / delta, Microsoft Power BI, Microsoft SQL Server, langage SQL, Datawarehousing / Mod√©lisation de donn√©es\nAnalyses et export de donn√©es\nConnaissance de l‚Äôensemble du processus depuis la collecte jusqu‚Äô√† la mise √† disposition des donn√©es en ayant comme point fort la maitrise de sa transformation et mise en forme\nVous avez un bon niveau d‚Äôanglais\nLocalisation : Biot et/ou Carros\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "SQL Server"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Aubay",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-aubay-3573871076?position=10&pageNum=2&refId=L5gzblNVY7DiL8vx2ayO4w%3D%3D&trackingId=ikgRu7W36JbbvXgo9R7FUA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Passionn√© par la Data, tu souhaites rejoindre une communaut√© d‚Äôexperts dans le domaine afin de d√©velopper tes comp√©tences en Data Engineering. Aubay renforce ses √©quipes Data et recherche des Data Engineers pour int√©grer des dispositifs de projets pointus et vari√©s.\nTon quotidien en tant que Data Engineer chez Aubay, :\nD√©finition de la strat√©gie de stockage et mise en ≈ìuvre des technologie appropri√©es (base de donn√©es SQL, NoSQL, stockage distribu√©,‚Ä¶)\nIngestion des donn√©es (structur√©es, semi-structur√©es ou non-structur√©es) selon diff√©rentes fr√©quences : batch, micro-batch ou temps r√©el\nConception et mise en ≈ìuvre de pipelines de donn√©es afin de fournir des donn√©es pr√™tes √† l‚Äôemploi aux consommateurs : uniformisation, mise en qualit√©, enrichissement, calcul d‚Äôindicateurs,‚Ä¶\nConception et d√©veloppement d‚ÄôAPI pour exposer les donn√©es aupr√®s d‚Äôapplications tierces\nAppui aux Data Scientists pour industrialiser et optimiser les algorithmes de Machine Learning\nPr√©paration et animation d‚Äôateliers de travail avec des interlocuteurs vari√©s : recueil/approfondissement des besoins m√©tiers, avancement/restitution des travaux, transfert de comp√©tences,‚Ä¶\nTon profil :\nTu dispose d‚Äôune formation niveau BAC+5 (Master 2 ou √©cole d‚Äôing√©nieur) sp√©cialis√©e en informatique\nTu as d√©j√† une premi√®re exp√©rience significative (a minima 2 ans) en Data Engineering sur des technologies Big Data\nLes technologies telles que Hadoop, Spark ou Kafka sont tes technologies de pr√©dilection\nLa programmation n‚Äôa plus de secret pour toi et tu maitrise parfaitement un ou plusieurs langages de programmation suivants : Java, Scala et Python\nTu ma√Ætrises les tenants et aboutissants de la philosophie DevOps et des outils orient√©s CI/CD\nTu es soucieux de la qualit√© et de la performance de tes d√©veloppements et tu t'int√©resse √† l‚Äôinnovation frugale\nTu es un expert technique dans ton domaine sans pour autant oublier l‚Äôimportance d‚Äôune communication orale et √©crite de qualit√© et adapt√©e √† chacun de tes interlocuteurs\nTu travaille au quotidien en mode agile et tu en maitrise les fondements\nCe qui nous caract√©rise :\nDes missions et projets dans le domaine du Data Engineering en nombre et dans des secteurs vari√©s (Banque, Assurance, Telecom, Industrie,‚Ä¶) qui permettent √† nos collaborateurs de monter en comp√©tences et de devenir des experts Data reconnus\nDe l‚Äôapprentissage en continu avec des formations et des certifications sur les technologies Data d‚Äôaujourd‚Äôhui et de demain\nDes experts Data mobilisables pour accompagner et soutenir techniquement les collaborateurs sur leurs projets\nDes communaut√©s de savoir-faire Data proposant de mani√®re r√©guli√®re aux collaborateurs d‚ÄôAubay du contenu et des √©v√®nements de partage (webinar, meetup/afterwork, BBL,‚Ä¶) sur les th√©matiques suivantes : Data Engineering, Data Viz, Data Science/IA, Data Platform & Architecture,‚Ä¶\nAubay encourage la diversit√© sous toutes ses formes et garantit l'√©galit√© des chances √† tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les am√©nagements n√©cessaires.\nTa carri√®re chez Aubay :\nTu auras la possibilit√© de d√©velopper et certifier tes comp√©tences sur les derni√®res technologies Data avec un focus fort sur les plateformes Data Cloud telles qu‚ÄôAzure Synapse Analytics, Google Cloud Platform, Snowflake et Databricks\nTu pourras rejoindre la BU d‚Äôexcellence Data et √©voluer au sein d‚Äôun environnement humain et professionnel de haut niveau. Tu profiteras d‚Äôun management sur-mesure pour t'accompagner dans ta trajectoire de carri√®re\nAu sein de la BU d‚Äôexcellence, de multiples perspectives s‚Äôoffriront √† toi :\nR√¥le de ¬´ Lead ¬ª : Vous pourrez gagner en responsabilit√© sur le plan technologique et devenir un r√©f√©rent aupr√®s de nos clients et des collaborateurs de la communaut√© Data Engineering\nR√¥le de ¬´ Champion ¬ª : Vous repr√©senterez Aubay aupr√®s d‚Äôun ou plusieurs de nos partenaires √©diteurs strat√©giques et vous participerez activement √† l‚Äôanimation de la relation sur le plan technologique\nR√¥le de ¬´ Head ¬ª : Vous pourrez prendre la responsabilit√© du savoir-faire Data Engineering et de ses offres et en assurer le d√©veloppement au sens large (d√©veloppement business, recrutement, management de collaborateurs, d√©finition de la strat√©gie et animation de la communaut√© au sein du groupe Aubay,‚Ä¶)\nBesoin d‚Äôen savoir plus sur le processus de recrutement ?\nUn √©change macro au niveau RH avec Doriane\nUn entretien technique avec Marius ou Peter, deux de nos r√©f√©rents techniques\nUn √©change manag√©rial avec le Directeur de la BU Modern BI & Data\nA savoir que l‚Äôordre des √©tapes peut varier selon tes envies (ex : √©change manag√©rial avec l‚Äô√©change technique)\nAubay encourage la diversit√© sous toutes ses formes et garantit l'√©galit√© des chances √† tous les candidats. Tous nos postes sont ouverts aux personnes en situation de handicap et nous proposons tous les am√©nagements n√©cessaires.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "CI/CD",
                "Big Data",
                "Machine Learning",
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "LVMH",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-solutions-engineer-data-ai-at-lvmh-3900392289?position=1&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=3ow1m%2BXt5JtMIV2JBi%2FJsw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LVMH is the #1 Luxury group and is currently accelerating rapidly on digitalisation. It is bringing technology and innovation in the core of the established 75+ Maisons by inventing unique and powerful products and services.\nWe are looking for talented solution engineers (Software, Cloud, Data and AI) to join our team and be part of this tech revolution of bringing the Group and its Maisons to the next level.\nIf you believe Data and AI can enhance the retail industry, from the day-to-day operational tasks to the long term customer experience,\nIf you think that the Cloud technologies (we love Google Cloud) is a revolution for Data and AI products,\nIf you like building tech solutions having direct impacts on billion-dollar-valued businesses,\nIf you have good communication skills and like sharing your knowledge,\nApply now, and join us!\nThe mission\nThe Solution Engineer is providing advices and technical assets to the Maisons having Data & AI projects.\nOur team (Group Data team) is building a technical framework for all the Maisons to implement easily and quickly Data and AI use cases. Your mission will be to support the Maisons to convert their use case needs to concrete and production ready technical solutions using our framework and tools.\nYou will cover a portfolio of Maisons, in direct contact with their business analysts, data scientists and IT teams. You will be their dedicated referent on the Data & AI technical topics (Data platform, AI/ML softwares, data transport and transformations, data quality).\nMain responsibilities\nYou will be responsible of providing support and advices to a portfolio of Maisons on Data & AI tech topics (Cloud, Data stacks, Data transformations, Data transfers, ML ops).\nYou will keep a recurrent discussion with the Maisons to accelerate their projects and immediately provide our support when it's needed.\nYou will follow-up the engaged productions in the Maisons and report them to the global group data strategy committees.\nApplying the quality and security standards. Making them evolve if necessary.\nProducing realistic, understandable and documented solutions following the group guidelines.\nSharing and learning from the team by communicating difficulties and successes, taking and bringing honest feedbacks and improving the identified pain points.\nTaking responsibility as member of the team on the product performances (delivery and long term usage)\nRequired expertise and knowledge\nAbility to build technical solutions answering concrete usage (User Stories) and communicate them to the team.\nDimension and evaluate complexity for technical solution productions.\nExtensive knowledge and experience with good learning and sharing abilities.\nEvaluate quickly risks and opportunities about technical choices.\nSolid oral, written, presentation and interpersonal communication and relationship skills.\nProblem-solving skills on Data and AI, coding and software development\nTech lover\nFeedback taker and giver\nTeam player\nKey benefits to join our team\nAttractive packages\nOffices in the 8th arrondissement near the Champs Elys√©es\nFlexibility on the working hours\nRemote work possible (~40%)\n7 weeks of holidays (cong√©s pay√©s + RTT)\nLVMH brands exclusive private sales\nGreat employee committee and health insurance (CE, mutuelle)\nLast generation MacBooks\nPart of a young, motivated and tech savvy team. Get prepared for the Thursday drinks and the tech meet-ups!\nYou‚Äôre eligible if\nYou have a strong experience (3+ years) in cloud data architecting or consultancy.\nYou graduated from an engineering (or equivalent) with a master‚Äôs degree. Computer Science knowledge is mandatory.\nExperience on data stacks and/or Google Cloud (built in components) is a huge plus.\nFrench and English both written and oral (Maisons are all over the world)\nYou‚Äôre thrilled to support the #1 luxury group to get even better.\nHiring Process\nCall with our HR partner dedicated to the Tech Team\nTechnical interview with the Solution Engineering Manager\nTechnical test\nInterview with the Head of Engineering\nStill here? Apply now!!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "40",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Flexibility",
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-gcp-f-h-at-apside-2902806697?position=2&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=ctZFklaEBa2A%2BElqpEb7zA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre une entreprise apprenante ? Engag√©e pour t‚Äôaccompagner dans ton √©volution professionnelle et dans tes projets personnels ?\nRejoins Apside pour travailler sur les projets de demain !\nLe poste ?\nPour le compte de notre\nclient acteur mondial de la beaut√© et cosm√©tique,\ntu interviendras dans la\ntransformation d‚Äôun projet worlwide,\no√π tu devras\nd√©velopper la Data Platform et l'ensemble des services Data qui seront expos√©s aux diff√©rentes √©quipes du client. Aussi, tu seras amen√© √† d√©velopper des use cases data.\nDans ce sens, tes missions seront les suivantes :\nDesigner l'architecture et d√©velopper la solution\nD√©finir et d√©velopper les Data Model\n√ätre garant de la qualit√© du code\n√ätre DevOps (Utilisation/mise en place de chaine CI/CD et support niveau L3 des d√©veloppements)\nEnvironnement technique :\nGCP (BigQuery, Cloud Run, Cloud Build)\nSQL\nPython\nDevOps (Github)\nAPI Development\nTerraform\nM√©thodologie Agile\nToi ?\nTu as d√©j√† travaill√© sur\nGoogle Cloud Platform (GCP)\n?\nTu es\nautonome\n,\nrigoureux\n, et\nbon communiquant\n?\nTu souhaites participer √† un\nprojet d‚Äôenvergure associant cloud et Big Data\n?\nEt la suite ?\nTu rencontres d‚Äôabord l‚Äô√©quipe RH pour parler de tes attentes, ton projet, ton futur !\nPuis les managers pour parler concret : missions, projets, parcours de carri√®re, et bien s√ªr salaire et avantages J\nEt tu discutes avec un de nos Tech Leads, pour √©valuer tes comp√©tences/ te challenger.\nLes infos en plus !\nT√©l√©travail ! üòä\nUn salaire attractif en fonction de ton exp√©rience + diff√©rents avantages\nUn groupe en pleine croissance avec un management bienveillant\nEt une √©volution personnalis√©e avec la possibilit√© de se former via une plateforme interne\nTu souhaites donner un nouvel √©lan √† ta carri√®re ? Rejoins la vie Apsidienne !\nPour en savoir plus √†\nwww.apside.com\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Salaire",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Siderlog Conseil",
        "location": "Niort, Nouvelle-Aquitaine, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-developpeur-talend-at-siderlog-conseil-3861714639?position=3&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=cWPbrcZyyB%2BcGzsdXBIyuw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Siderlog est un cabinet de conseil sp√©cialis√© implant√© √† Niort depuis 2004 qui accompagne les directions m√©tiers et SI sur des projets de:\n- Business et Data Analyse\n- Management de projets\n- Conduite du changement\nPour soutenir notre croissance, nous pr√©voyons √† Niort le recrutement de 20 consultants d'ici 2025.\nNos consultants b√©n√©ficient d'un mod√®le qui favorise l'√©panouissement professionnel et le bien √™tre:\nüçÉUn processus d'int√©gration sp√©cifique et un suivi r√©gulier\nüçÉUne √©coute active des attentes, notamment en terme de formations, certifications\nüçÉDes d√©jeuners et √©v√®nements mensuels\nüçÉUn management et un accompagnement de proximit√©\nüçÉUn package salarial attractif\nüçÉLa possibilit√© de contribuer aux projets d'entreprise ( RSE, communaut√©s m√©tiers, p√¥le conseil et expertise)\nüçÉEntreprise labellis√©e Happy At Work, charte T√©l√©travail...\nüçÉDe nombreux autres avantages que nous vous invitons √† venir d√©couvrir\nSiderlog recherche pour renforcer son √©quipe, √† Niort un(e) consultant(e) Data Engineer / Developpeur Talend.\nDans ce cadre vous devrez :\n‚úîÔ∏èConcevoir et d√©velopper des traitements/job de donn√©es complexes √† l'aide de Talend pour l'ingestion, le nettoyage, la transformation et la distribution des donn√©es.\n‚úîÔ∏èCollaborer √©troitement avec les √©quipes m√©tier pour comprendre les besoins en mati√®re de donn√©es et concevoir des solutions adapt√©es.\n‚úîÔ∏èMettre en ≈ìuvre des bonnes pratiques de d√©veloppement ETL, y compris la documentation, les tests unitaires et l'int√©gration continue.\n‚úîÔ∏èAssurer la surveillance et la maintenance des traitements/job de donn√©es en production, en r√©solvant les incidents et en effectuant des mises √† jour si n√©cessaire.\nüìã Qualifications et comp√©tences :\nüëâExp√©rience av√©r√©e dans le d√©veloppement de solutions de gestion et d'int√©gration de donn√©es, sur Talend.\nüëâMa√Ætrise des langages de requ√™te SQL pour l'extraction et la manipulation des donn√©es.\nüëâConnaissance approfondie des bases de donn√©es relationnelles et des entrep√¥ts de donn√©es.\nüëâComp√©tences en programmation avec Java, Python ou d'autres langages similaires.\nüëâCapacit√© √† travailler de mani√®re autonome tout en collaborant efficacement avec les membres de l'√©quipe.\nüëâExcellentes comp√©tences en communication √©crite et verbale.\nüëâMaitrise de l'outil ETL Talend.\nüëâExp√©rience avec d'autres outils d'int√©gration de donn√©es tels que Informatica, BODS, Alt√©ryx.\nüëâCertification Talend serait un plus.\nüëâExp√©rience dans le domaine de l'assurance souhait√©e\nCette offre vous int√©resse ! Postulez !\nüèÜüôèüöÄüéâ !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "bsport",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/full-stack-data-engineer-at-bsport-3848363421?position=4&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=TlXXliAVNig68JS53ybBYg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Do you know about bsport?\nWe are a Barcelona based company that offers a platform combining boutique fitness and advanced technology. Our all-in-one features cover bookings, payroll, marketing and more, helping our partners streamline operations and boost their commercial success.\nWe have more than 2‚Äô000 clients in 40+ countries and continue to expand rapidly.\nWe provide our partners with:\nOur platform - the heart of the system (B2B)\nA white label iOS and Android mobile application (B2C)\nAn integrated Video on Demand tool\nOur self-built Smart Marketing Suite\nA Webshop to up- and cross-sell different products\nOur first successes\nSince we launched in 2019, we have already achieved the following:\nWe‚Äôve built a community of over 6 million users\nFinalised a Series A Fundraising of $4+ million in December 2022\nGrown our team to more than 150 employees\nWe‚Äôre continuing to grow our team to become the #1 tech partner for boutique studios in Europe and the rest of the world!\nWe are seeking a talented and experienced Full Stack Data Engineer to join our dynamic tech team based in Paris. As a Full Stack Data Engineer, you will play a crucial role in enhancing our data platform and driving innovation through data engineering solutions.\nWhat your future position looks like:\nThe primary focus of this position is data engineering, encompassing tasks such as building, optimizing, and maintaining our data platform. You will be responsible for making continuous improvements to our data infrastructure to ensure its reliability, scalability, and performance. As an integral part of the data team, you will collaborate closely with team members to address various data-related challenges and opportunities. This may involve tasks ranging from designing and implementing data pipelines to conducting in-depth data analysis to extract actionable insights.\nThe role will focus on:\nBuild and maintain bsport‚Äôs data architecture\nEnsure the sustainability and scalability of the diverse components by leveraging bsport's cloud provider services and adhering to all DevOps best practices\nYou will enjoy working within an internal team of 25 people, consisting of Tech, Product, and Data experts, directly surrounded by our Senior Leads and CTO.\nOur stack is fully automated with push-to-deploy on both frontend and backend. We use Kubernetes and AWS, and our CI is self-hosted. Our methodology is based on agile principles, with weekly releases to production and staging to iterate, gather feedback, and drive progress.\nYou will be a good fit to join us if you:\nAlready built or maintained a data architecture at scale in a top cloud provider (AWS, GCP or Snowflake)\nAlready deployed data science models in production or built a data ingestion pipeline\nFamiliar with DevOps best practices\nProficiency in SQL, Python and Spark\nExperience with dbt and airbyte\nQualifications\nBachelor‚Äôs degree in Data Science, Computer Science, Engineering, or related field; advanced degree preferred.\nRelevant experience in data engineering\nStrong analytical and problem-solving skills, with the ability to work independently and in a team environment.\nWe'd love to have you join us for many reasons, such as:\nüåç A multicultural and international team!\nüöÄ A stimulating SaaS environment within a supportive and a fast-growing company\nüîã Enjoy 25 days of paid leave to recharge\nüè° Embrace days of remote work\nüè¢ Work from our stunning office in the heart of Bastille\n‚ù§Ô∏è‚Äçü©π\nHealth insurance half covered\nüõµ Public Transportation half covered\nüèÑüèΩ Take part in bsport team building and sport initiatives\nüõåüèΩ Supported by bsport on sick days\nInterview Process\nFirst interview with one of our Talent Acquisition team members (30 min)\nTechnical Interview with our Lead Data (1h30)\nTechnical Interview with our CPO (1h)\nFinal Interview with our CTO (1h)\nAbout our Company Culture:\nAt bsport, we collaborate with passionate individuals who value diverse ideas and backgrounds. We believe that diversity is our most valuable asset.\nOur commitment is to foster a positive and inclusive culture. We achieve this through team-building initiatives, open communication, professional growth opportunities, and by celebrating diversity in all its forms.\nWe value and respect every individual who is eager to make a difference, empowering them to contribute their unique skills and perspectives. Join our dedicated team to help create a thriving and welcoming workplace.\nJoin our team of passionate and committed people who are dedicated to creating a thriving and welcoming environment. Let's make it happen together!\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full",
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ippon Technologies",
        "location": "Nantes",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-ippon-technologies-3902436649?position=5&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=CgC9wZD4Kt6bLAagR1%2FJQA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Envie de rejoindre la communaut√© DATA la plus dynamique de France ?\nNotre sp√©cialit√© est de construire des data platform dans le Cloud public avec les meilleurs technos du moment : Snowflake, Databricks, Matillion, DBT.\nMembre de la Practice Data, le/la futur(e) Data Engineer sera int√©gr√©(e) √† nos √©quipes de conseil et sera suivi(e) par un(e) mentor qui l‚Äôaidera √† monter en comp√©tences.\nVotre champs d‚Äôexpertise :\nIntervenir sur les data platforms de nos clients pour d√©velopper de nouveaux pipelines de donn√©es (ingestion, traitement, exposition).\nTravailler en collaboration avec les m√©tiers et les data scientists pour leur fournir un support √† l‚Äôindustrialisation de leurs travaux (tests, int√©gration continue, scalabilit√© des mod√®les, craftsmanship etc‚Ä¶)\nD√©ployer des infrastructures cloud full\ninfra-as-code\n(Terraform, CloudFormation).\nParticiper aux √©v√®nements internes √† la communaut√© data (BBL, webinar, datap√©ro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups).\nCapitaliser sur les missions et les diff√©rents √©v√®nements de la communaut√© au travers d‚Äôarticles de blogs, REX, BBL interne.\nVos connaissances :\nUn framework de calcul distribu√© tel que Spark, Storm, Flink.\nUn ou plusieurs langages de programmation (Python, Scala, Java...)\nDiff√©rents syst√®mes de stockage de donn√©es (SQL ou NoSQL) et bien s√ªr le langage SQL.\nLa connaissance de Snowflake est bienvenue ;-)\nUn framework de streaming de donn√©es tel que Kafka ou Amazon Kinesis.\nUne exp√©rience sur les technologies Cloud : AWS, GCP, Azure\nLe delivery et les projets en production faisant partie de notre ADN, vous √™tes capable de livrer du code de qualit√© dans des environnements agiles.\nDe plus en plus de nos projets se font en remote avec des clients du monde entier, il devient n√©cessaire d‚Äô√™tre √† l‚Äôaise en Anglais.\nIppon technologies c‚Äôest aussi :\nüëç B√©n√©ficier d'un suivi de proximit√© r√©alis√© par votre manager technique : points r√©guliers pour votre suivi en mission, votre formation et votre √©volution de carri√®re\n‚úåÔ∏è Rejoindre une entreprise o√π les valeurs du sport sont nos leitmotiv : d√©passement de soi, travail en √©quipe, bienveillance.\nüóíÔ∏è Apprendre via notre programme de formation BlackBelt : https://bit.ly/3ByqcIL\nüòÅ Travailler en pair programming ou avec un.e mentor pour gravir les √©chelons !\nüí™ Pouvoir participer √† une aventure humaine au sein de notre Fondation Ippon pour r√©duire la fracture num√©rique dans le monde !\nü§ù Participer √† nos ap√©ros et divers √©v√®nements internes pour consolider la coh√©sion d‚Äô√©quipe\nEt apr√®s ?\nEt oui alors ? Que se passe-t-il une fois que vous √™tes convaincu d‚Äôavoir lu l‚Äôoffre d‚Äôemploi qui vous correspond bien ?\nNous vous proposons de prendre contact et de nous rencontrer !\nLes Next Steps :\n1 call RH\n1 √©change RH\n1 √©change Technique\nSi le match est bon des deux c√¥t√©s : Hadjim√© ! Vous vous lancerez sur le tatami Ippon !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "InfrastructureAsCode": [
                "CloudFormation",
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Equativ",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-equativ-3814251519?position=6&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=ya1Qp5kjARTzCi3eNWECoA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üë´ About the team\nAt Equativ, we‚Äôre on a mission to develop advertising technologies that empower our customers to reach their digital business goals. This means that we rely on massively scalable, widely distributed, highly available, and efficient software systems; the platform deals with over 100 billions requests per day and above 40 Gbps of network traffic.\nOur innovation team based in Paris, Nantes, Limoges, Krak√≥w and Berlin is composed of 90 straightforward and energetic engineers working in an Agile environment and ready to tackle the most complex technical challenges.\nOur data engineering team is composed of 10 skilled engineers and is based in Paris. We are part of the R&D department which is composed of 140+ engineers spread across Paris, Nantes, Limoges, Krak√≥w and Berlin all working in an Agile environment and ready to tackle the most complex technical challenges.\nOur mission üëá\nData Engineering team is central to Equativ‚Äôs data centric business and is responsible to ingest, transform, model and redistribute all data coming from our adtech platform.\nWe aim at building scalable and robust Big Data platforms from ingestion to business actionable consumption. Our Big Data ecosystem must handle massive log ingestion (tens of billions per day), short & long term data storage, complex data modelling, real-time and batch ELT as well as providing external access through dedicated APIs.\nData Engineers serve Equativ data directly to our customers and throughout the company whether it is for BI analysis, data science algorithms (clustering and optimization), customer reporting, invoicing and more.\nEquativ Data Engineering team is engaged in an ambitious migration of its main data stack (Hadoop on-premise) to GCP with the objective to increase reporting features, lower maintenance time, improve performances and simplify the access to our raw data.\nWhat you'll do ‚úèÔ∏è\nAs a Big Data Engineer, you‚Äôll primarily focus on maintaining and enhancing the operationality of our on-premise and cloud data pipelines which feed our warehouses and APIs\n-Design, develop, test, promote and industrialize all data components from data ingestion to datawarehouse delivery (ClickHouse, BigQuery):\nPropose and develop innovative solutions to achieve the best levels of scalability and performance for our Big Data engines\nAutomate and streamline our real-time and batch data pipelines (on-premise and in the cloud) in order to simplify access to our data by other teams and lower amount of work spent by other teams on ETL processes\nPerform end-to-end monitoring to ensure high availability of production data processing, data quality and reliability\nApply best in class Devops guidelines and secure deployments\n-Brainstorm with other team members working on our data backend (datawarehouse modelling and data exposure through our reporting APIs) on optimizing our architecture and support them in the use of our pipelines\n-Contribute to data roadmap definition in coordination with other R&D and product teams in order to build a best in class data infrastructure that will generate insights for Equativ‚Äôs analytics\n-Take part in improving and deploy data engineering standards, procedures, processes and operational guidelines around target data components at Equativ\nüí™ About you\nMaster degree in Computer Science or similar technical field of study\n3+ years of software development with open source technologies\nFluent in Java and/or in Scala. SQL mastery\nVery good understanding of Devops principles (Gitlab, Docker, Kubernetes, Gradle, ci/cd)\nExperience with large-scale data engineering technologies (ClickHouse, Flink, Kafka, Hadoop, Spark, Hbase)\nExperience on building data pipelines on Google Cloud Platform (BigQuery, Dataflow, GCS, Cloud Run, Airflow ‚Ä¶) would be a big plus\nExperience in working with high QPS Rest APIs is a plus\nEntrepreneurial spirit and know-how to identify opportunities of improvement\nWorking proficiency and communication skills in verbal and written English\nPassion for playing with large volume of data\nüöÄ How you'll grow\nWithin 1 month:\nYou'll be just finishing your onboarding.\nYou'll probably have tackled a few small tasks in peer-coding\nWithin 4 months:\nYou'll have an overview of 50% of the stack, CI/CD and team‚Äôs main processes. You‚Äôll be able to work on more complex developments\nYou'll now have enough knowledge to participate to deployments of chosen applications\nWithin 9 months:\nYou'll be autonomous on most of our stack and will have participated to major projects\nYou‚Äôll be helping the team on production matters\nüëã About us\nEquativ is the new single name for Smart Adserver, DynAdmic, LiquidM and Nowtilus ‚Äî four proven innovators in advertising technology. The vertically integrated company provides brand and privacy-safe solutions that empower its clients to achieve maximum impact while respecting the rights of consumers. The union combines client expertise and engineering excellence to serve the interests of both the supply- side and demand-side with equal professionalism and technical sophistication.\nHeadquartered in Paris and New York, Equativ operates globally with a team of more than 550 people in 20 offices. Equativ offers the market its own independent ad server, SSP, buyer tools, and media services to fulfill the promise of advertising technology. Learn more at Equativ.com.\nThe company is ranked on the Deloitte Technology Fast 500 EMEA and in the Financial Times‚Äô FT 1000: Europe‚Äôs Fastest-Growing Companies.\nEquativ (formerly Smart AdServer) has been awarded the HappyIndex@Work label and is proud to be among the best companies in the ChooseMyCompany ranking, recognized for its flexible working environment.\nCome and lead the charge with us in building a transparent ecosystem based on quality!\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala"
            ],
            "DataBase": [
                "SQL",
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Docker"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-cloud-f-h-at-apside-3904088503?position=7&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=Od0kUUGYPxYerGEI1x4MHA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üí•\nD√©couvrez la Vie Apsidienne\nüìπ\net vous aussi, devenez Apsidien\nOn aurait pu demander √† Chat GPT de vous d√©montrer en quoi\nApside est l‚ÄôESN qu‚Äôil vous faut,\nmais on pr√©f√®re que vous le d√©couvriez vous-m√™mes üëáüòè\nüî•\nD√©couvrez votre future mission\nüëâ\nContexte\nRejoignez notre Practise Cloud/Data, afin d‚Äôintervenir sur des sujets √† haute valeur ajout√©e !\nNotre\nclient migre actuellement toutes ses applications vers le cloud AWS.\nDe plus, dans le cadre du d√©veloppement d'un produit de restitution automatis√©e de donn√©es, ils recherchent actuellement d√©veloppeur data ayant d√©j√† travaill√© sur un projet similaire. La solution produit est techniquement con√ßue en lien avec le Tech Lead validant l'architecture logicielle √† mettre en place sur le cloud AWS.\nSecteur\n: culture/m√©dia\nM√©thode de travail\n: Agile Safe\nüòé Mission\nCapter les donn√©es (structur√©es et non structur√©es) produites dans les diff√©rentes applications\nInt√©grer les √©l√©ments\nStructurer la donn√©e (s√©mantique, etc‚Ä¶)\nCartographier les √©l√©ments √† disposition\nNettoyer la donn√©e (√©limination des doublons, etc‚Ä¶)\nValider la donn√©e\nCr√©er les r√©f√©rentiels de donn√©es\nEnvironnement technique\n:\nPython\nLambda\nStep Function\nAWS / AWS RDS\nPostegreSQL\nSnowflake\nSpark\nüìç\nLocalisation\nLa D√©fense\nüí∞\nLe package salarial que nous vous proposons\nContrat :\nCDI\nAvantages groupe :\ncarte ticket restaurant Swile, prime de mobilit√©, RTT, accord t√©l√©travail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle √† 100% etc‚Ä¶\nAvantages agence :\nCommunaut√© Cloud/Data, afterworks, communaut√© techlead\nFormation :\ncertifications techniques, cours particuliers d‚Äôanglais en interne, acc√®s √† un catalogue de formations gr√¢ce √† notre plateforme e-learning (\nAcademy by Apside\n) ou via nos organismes partenaires.\nüîÆ\n√î vous futur Apsidien, qui √™tes-vous ?\nAu moins 4 ans d'exp√©rience en tant que Data Engineer\nMaitrise de l‚Äôenvironnement cloud AWS\nForce de proposition, bon relationnel et autonome\nüòè\nApside a suscit√© votre curiosit√© ?\nDans un environnement marqu√© par une acc√©l√©ration des √©volutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients √† cr√©er de la valeur et √† adresser leurs enjeux strat√©giques en leur mettant √† disposition des expertises technologiques (\nData / IA, Cloud, Cyber\n) et une exp√©rience sectorielle (\nIndustrie, Banque, Assurance, Service, Secteur Public\n). Pour un accompagnement global, le groupe propose des offres transverses autour du\nHandicap\n(Apsid‚ÄôEA), du\nDigital Learning\n, et du\nConseil\n.\nü§î\nEt votre place dans tout √ßa ?\nüëâ Notre volont√©\nest de vous accompagner dans la construction et l‚Äô√©panouissement de votre carri√®re\nen nous appuyant notamment\nsur 3 piliers :\nUne\nr√©mun√©ration\n√† hauteur de vos investissements et de vos comp√©tences\nUne\ntrajectoire professionnelle\nstimulante sur mesure\nUn\nengagement\nautour des valeurs Apsidiennes : la qualit√© de vie et des conditions de travail au c≈ìur de nos enjeux\nEngag√©e pour\nun monde plus inclusif et plus responsable\n, Apside r√©invente l‚ÄôESN et propose l‚ÄôEngagement Soci√©tal et Num√©rique. D√©couvrez notre d√©marche RSE ainsi que notre vision de l‚ÄôEntreprise Engag√©e.\nConvaincu ? A vous de jouer, envoyez-nous votre CV !\nRejoignez l‚Äôaventure Apsidienne et d√©couvrez notre vision d‚Äôune ESN singuli√®re et r√©siliente\nüöÄ\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "100",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Carrefour",
        "location": "Massy, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-en-alternance-at-carrefour-3884390443?position=8&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=x6YzTwVoIs89U8m4IejwAQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le saviez-vous ?\nNous rejoindre, c‚Äôest rejoindre l‚Äôun des leaders mondiaux de la distribution qui met l'accent au quotidien sur la diversit√©, la RSE et le digital, pour satisfaire nos clients et nos collaborateurs. En tant que partenaire premium des Jeux Olympiques et Paralympiques de Paris 2024, nous partageons les valeurs du sport en permettant √† nos √©quipes de se d√©passer et encourageons une alimentation saine au juste prix pour tous.\nVous cherchez √† travailler dans une entreprise dynamique o√π votre travail rime avec impact social et environnemental ? Bienvenue chez nous !\nData Engineer (F/H) - en alternance\nEn tant qu' alternant, vous int√©grerez la plateforme supply chain o√π vous serez amen√© √† appuyer le p√¥le pr√©vision et optimisation particuli√®rement sur des sujets data et d‚Äôanalyse de donn√©es.\nAu sein d'une √©quipe compos√©e de data scientists et de data engineers organis√©e en mode Scrum Agile, vous travaillerez pour am√©liorer au quotidien un outil de calcul de pr√©vision (pr√©vision de la demande des entrep√¥ts Carrefour). Vous participez √† l'√©volution fonctionnelle et technique de l'application.\nüéØ Les missions\nDans ce cadre, vous serez amen√© √†\nExplorer et analyser les donn√©es du datalake carrefour\nParticiper au cadrage des nouvelles fonctionnalit√©s\nD√©velopper les √©volutions des traitements, des mod√®les statistiques et de machine learning de pr√©vision et des reporting\nTester les fonctionnalit√©s d√©velopp√©es\nR√©pondre aux demandes utilisateurs\nüë• Profil\nVous √™tes en √©cole d‚Äôing√©nieur, en master 2 ou √©quivalent avec une sp√©cialisation data science, data engineering, statistique, informatique.\nVous avez une exp√©rience en traitement et analyse de donn√©es.\nVous avez un esprit d‚Äôanalyse et la capacit√© de travailler en √©quipe et √† distance.\nVous √™tes autonome et rigoureux, fluide dans votre communication orale et √©crite et √† l'√©coute des besoins de vos interlocuteurs.\nVous √™tes reconnu pour vos capacit√©s d'anticipation, votre sens de l'initiative et votre r√©activit√©.\nVous avez une bonne connaissance des langages suivants\nSQL\nPython\nUne connaissance de GCP et de Big query serait un plus.\nUne connaissance m√™me th√©orique de la m√©thodologie agile serait un plus\nUne connaissance de GIT serait un plus.\nEncore plus de bonnes raisons de nous rejoindre\nInt√©grer une √©quipe conviviale √† taille humaine au sein d‚Äòun grand groupe\n12 % de remise sur achat\nüìù Informations compl√©mentaires\nDate de d√©but  09 septembre 2024\nDur√©e  1 an\nLieu  Lyon\nD√©placements en magasin et en concurrence dans la r√©gion parisienne\nAvantages 50 % du titre de transport pris en charge par Carrefour\nEnvie de rejoindre l‚Äôaventure ?\nChez Carrefour, nous avons √† c≈ìur de ne passer √† c√¥t√© d‚Äôaucun talent et sommes fiers de compter des √©quipes repr√©sentatives de la soci√©t√© dans son ensemble. Nous encourageons ainsi tous types de profils √† postuler √† cette offre et garantissons un processus de recrutement d√©nu√© de toutes formes de discriminations.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "Big Query"
            ],
            "Other": [
                "Statistiques",
                "Machine Learning"
            ],
            "EnSoftSkils": [
                "Initiative",
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-nantes-at-capgemini-3803998213?position=9&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=DheaFnpiwdjTtO17fS5wAQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Capgemini\nChoisir Capgemini, c'est choisir une entreprise o√π vous serez en mesure de fa√ßonner votre carri√®re selon vos aspirations, o√π vous serez soutenu et inspir√© par une communaut√© d‚Äôexperts dans le monde entier, o√π vous pourrez r√©√©crire votre futur. Rejoignez-nous pour red√©finir les limites de ce qui est possible, contribuer √† lib√©rer la valeur de la technologie pour les plus grandes organisations et participez √† la construction d‚Äôun monde plus durable et inclusif.\nVos missions :\nInt√©gr√©(e) au sein d'une √©quipe projets intervenant pour des clients dans des secteurs d'activit√©s vari√©es, vous serez notamment en charge des missions suivantes :\nConcevoir et mettre en oeuvre des strat√©gies s√©curis√©es d'acquisition et d'int√©gration de donn√©es,\nConfigurer des r√©f√©rentiels de donn√©es √† la pointe de la technologie dans des environnements distribu√©s, majoritairement dans le cloud (Google Cloud Platform, Azure Databricks, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks),\nConstruire des pipelines de donn√©es pour collecter, transformer et traiter des donn√©es en collaboration avec des scientifiques de donn√©es afin de r√©pondre aux exigences de la mod√©lisation de donn√©es d'analyse avanc√©e.\nVotre profil :\nDipl√¥me d‚Äôing√©nieur ou √©quivalent universitaire\nMinimum 3 ans d'exp√©rience\nAnglais courant\nMa√Ætrise des langages Java, Scala ou Python et expertise sur les framework Spark et/ou Hadoop.\nExpertise sur les services Cloud Data Platform suivants : Azure Data Lake, Azure synapse, Azure Data Factory, Azure Data Explorer, GCP, AWS, Snowflake, Databricks‚Ä¶\n3 raisons de nous rejoindre :\nQualit√© de vie au travail : accord de t√©l√©travail en France et √† l‚Äôinternational, accord sur l‚Äô√©galit√©\nprofessionnelle, la parentalit√©, l‚Äô√©quilibre des temps et la mobilit√© durable.\nApprentissage en continu : certifications et formations en libre acc√®s, accompagnement sur mesure avec votre carreer manager, parcours d‚Äôint√©gration sur 9 mois.\nAvantages groupe & CSE : plan actionnariat, activit√©s √† tarifs pr√©f√©rentiels, remboursement partiel\nvacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorit√©s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int√©grez un collectif qui valorise la diversit√©, d√©veloppe le potentiel de ses talents, s‚Äôengage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r√©duire son impact environnemental sur tous ses sites et aupr√®s de ses clients.\nCapgemini\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr√®s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d‚Äôexp√©rience, nous sommes un partenaire strat√©gique des entreprises pour la transformation de leurs activit√©s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp√©tuelle √©volution tels que le cloud, la data, l‚ÄôIntelligence Artificielle, la connectivit√©, les logiciels, l‚Äôing√©nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "BigData": [
                "Databricks",
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Marseille, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3896997028?position=10&pageNum=5&refId=Em6A1%2BpzDAuDIz3tiZOUsw%3D%3D&trackingId=WW8bMRocTD8OQx6MQxnAeA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ntico",
        "location": "Villeneuve-d‚ÄôAscq, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/consultant%C2%B7e-data-engineer-at-ntico-3902424755?position=1&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=M32mBwSUc%2BeWFVPltZpppA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Sois acteur de ta r√©ussite et rejoins notre √©quipe de 140 collaborateurs¬∑trices qui ne font pas que des projets, mais qui vivent une vraie exp√©rience humaine unique !\nüí° Partage, Progr√®s, Plaisir : nos valeurs, ton avenir !\nüåê Pr√©sents √† Lille, Orl√©ans, Montpellier : des expert¬∑e¬∑s partout en France !\nüíº + de 40 clients qui nous font confiance\nüßë‚Äçüíª Recrutement sur profil\nüéØ\nTA MISSION :\n* Tu int√®gres une communaut√© Data, en tant que Data Engineer.\n* Tu con√ßois et mod√©lises les donn√©es et identifies les sources et flux √† r√©aliser.\n* Tu es en lien permanent avec les √©quipes m√©tiers et IT.\n* Tu formes et transmets ton savoir.\n* Tu es garant¬∑e de la qualit√© des livraisons.\nüßë‚Äçüíª\nTES COMP√âTENCES :\nTalend, ODI, Stambia, Kafka, API, Bases SQL, et NoSQL, GCP, AWS\nü•á\nTON PROFIL :\nTu es expert¬∑e des flux de donn√©es.\nLa manipulation et le traitement des donn√©es est une seconde nature.\nTu as le sens du service et tu apportes des solutions innovantes.\nTu aimes transmettre et partager ton savoir.\nTu justifies imp√©rativement d‚Äôau moins 3 ans d‚Äôexp√©rience et tu as d√©velopp√©¬∑e une autonomie sur ton domaine de comp√©tence.\nTu souhaites diversifier tes comp√©tences pour √™tre toujours √† la pointe des cas d‚Äôusages m√©tiers et des nouvelles technologies Data.\nüôå\nNOS AVANTAGES :\n‚ú® Pourquoi nous rejoindre ?\nüí™\nD√©veloppement Continu\n: Chez Ntico, tu montes en comp√©tences gr√¢ce √† nos communaut√©s d‚Äôexperts et nos formations !\nü§ù\nManagement de proximit√©\n: On t'√©coute, on te valorise et on t'accompagne dans ton projet pro, en toute transparence !\nüéâ\nMoments conviviaux\n: Sport, culture, DIY, insolite‚Ä¶ Tu peux participer √† nos √©v√©nements tous les mois, et en proposer ! On n‚Äôest jamais √† court d‚Äôid√©es pour des animations uniques !\nNtico, c'est un cadre de travail bienveillant, un environnement dynamique o√π l'√©panouissement personnel est aussi important que le succ√®s collectif !\nPostule d√®s maintenant et pr√©pare-toi √† vivre une exp√©rience humaine unique ! ‚ú®\nDe notre c√¥t√©, on te contacte dans les 72h suivant ta candidature et on te propose un processus de recrutement rapide. üöÄ\nNtico s'engage activement en faveur de l'inclusion professionnelle des personnes en situation de handicap, tout en promouvant la mixit√©, la diversit√© et l'√©galit√© au sein de son effectif.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "relevanC",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-h-f-at-relevanc-3845776902?position=2&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=ggs4hDgN4d0J%2BA6DT8nBCg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "relevanC est une filiale du Groupe Casino et a √©t√© fond√©e en 2017.\nNous avons des bureaux en France, au Br√©sil et en Colombie et op√©rons √† l'√©chelle mondiale.\nNos solutions de Retail Media permettent √† nos clients de g√©n√©rer de nouvelles sources de revenus publicitaires gr√¢ce √† des annonces pertinentes et personnalis√©es.\nEn tant que Data Engineer tu auras acc√®s aux donn√©es de nos clients internes (enseignes du groupe Casino) et externes √† traiter au sein de notre data warehouse. Tes missions seront les suivantes :\ntravailler en √©troite collaboration avec tous les autres membres de la squad\n√©crire / relire du code en respectant les bonnes pratiques de d√©veloppement ainsi que les tests unitaires et participer\nassurer la co-responsabilit√© du d√©roulement des d√©ploiements, des mises en production et du bon fonctionnement des applications avec les autres membres de la squad\nr√©diger la documentation technique quand cela est n√©cessaire\nmettre en ≈ìuvre les bonnes pratiques relatives au RGPD telles que d√©finies par le tech lead\nCe CDI bas√© √† Paris centre (1er arrondissement) d√©butera d√®s que possible.\nFaire partie de relevanC, qu‚Äôest-ce que √ßa signifie ?\nTravailler sur une stack technologique de pointe (Python, PySpark, Google BigQuery, Apache, Airflow‚Ä¶)\n√ätre membre √† part enti√®re d‚Äôune √©quipe dynamique et passionn√©e aux profils tr√®s vari√©s (chefs de projets, d√©veloppeurs, designers, animations commerciales)\nTravailler dans un environnement stimulant et relever des nouveaux d√©fis chaque jour\nRejoindre une entreprise en pleine expansion avec des opportunit√©s fortes de d√©veloppements et d‚Äôinnovation\nProfil recherch√©\nDipl√¥m√©(e) d‚Äôune grande √©cole d‚Äôing√©nieur ou profil universitaire sp√©cialis√© en Data / Informatique / Math / Stats.\n5 ans (et plus) d‚Äôexp√©rience en Data Engineering\nApp√©tence forte pour le marketing digital et le retail, force de proposition, business oriented et moteur d‚Äôinnovation\nUne maitrise parfaites des bonnes pratiques de d√©veloppement\nSolides comp√©tences en Python, Spark et SQL\nUne exp√©rience sur Google Cloud Platform est un plus\nLien vers notre politique de traitement des donn√©es : https://relevanc.com/fr/politique-de-protection-des-donn%C3%A9es-recrutemen\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "METEOJOB by CleverConnect",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-lille-cdi-at-meteojob-by-cleverconnect-3858145866?position=3&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=tvqEla8Q4uUopai9kdIyDw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nChez LJE Solutions, nous pla√ßons l‚Äôhumain au c≈ìur de chaque projet. Au-del√† des comp√©tences, nous valorisons les\naspirations\net les\nvaleurs\nde chaque individu.\nNous intervenons dans tous les secteurs d'activit√© en France et en Suisse.\nDescription Du Poste\nLJE Solutions recherche pour un de ses clients bas√© √† Lille, un/une Data Engineer.\nNotre client est une\nESN dynamique bas√©e √† Lille, qui se distingue dans l'int√©gration et la restitution de donn√©es. Partenaire privil√©gi√© de technologies de pointe comme Power BI, Tableau et Qlik, il recherche des talents d√©sireux de participer √† notre aventure entrepreneuriale.\nNous recherchons un Data Engineer curieux et motiv√© pour jouer un r√¥le cl√© dans l'organisation et le d√©veloppement de l'agence. Ce poste offre une opportunit√© unique de travailler directement avec les fondateurs, experts en technologies, et de contribuer significativement √† la formation interne et √† l'expertise chez nos clients.\nVos Responsabilit√©s\nTravailler en √©troite collaboration avec les fondateurs sur des projets d'int√©gration et de restitution de donn√©es,\nParticiper activement √† la croissance de l'entreprise en apportant des id√©es innovantes et en prenant part √† des projets vari√©s,\nMonter en comp√©tence techniquement, avec la possibilit√© d'√©voluer vers des r√¥les de Team Lead ou Tech Lead selon vos aspirations.\nCette entreprise offre un environnement convivial et ambitieux, encourageant la prise d'initiative. Leur structure √† taille humaine valorise chaque collaborateur, avec une approche personnalis√©e et une hi√©rarchie plate qui favorise l'expression et la participation active de tous.\nR√©mun√©ration Et Avantages\nPoste bas√© √† Lille, avec possibilit√© de t√©l√©travail partiel,\nR√©mun√©ration comp√©titive bas√©e sur l'exp√©rience, fourchette indicative de 44k √† 48k ‚Ç¨ en fixe, + variables,\nTickets restaurant,\nMutuelle d'entreprise.\nDescription Du Profil\nPassion pour les technologies de la data, avec une expertise ou un int√©r√™t pour XDi et Talend, sans exclure d'autres ETL du march√©,\nPlus de 4 ans d'exp√©rience dans le domaine de la data engineering,\nCuriosit√© intellectuelle, agilit√©, excellent savoir-√™tre, forte capacit√© de travail en √©quipe et de partage de connaissances,\nLocalisation √† Lille ou disposition √† d√©m√©nager, avec une pr√©f√©rence pour les candidats de la r√©gion pour faciliter la collaboration et le partage au sein de notre agence physique.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ],
            "EnSoftSkils": [
                "Initiative",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Klanik",
        "location": "Valbonne, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/ing%C3%A9nieur-data-etl-at-klanik-3918894069?position=4&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=Yal%2Ffuu%2F8kYrTIr2bQuwVw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Nous recherchons un profil Data ETL exp√©riment√© orient√© sur les API pour rejoindre notre √©quipe dynamique. Le candidat id√©al doit avoir une expertise approfondie dans la conception, le d√©veloppement et la gestion d'API, en particulier dans les environnements SOAP, UI et REST. Ce r√¥le n√©cessite la capacit√© de travailler avec des API existantes, de les √©valuer, de les am√©liorer et de proposer des solutions innovantes pour r√©pondre aux besoins de notre client.\nLe profil recherch√© devra g√©rer la configuration de notre outil d'injection de donn√©es, automatiser les scripts de population de donn√©es, coordonner les activit√©s de traitement des rejets et construire des vues de surveillance.\nResponsabilit√©s :\nConcevoir, d√©velopper et impl√©menter des API performantes et √©volutives.\nMettre en place de l'outil de transformation et d'injection interne existant en d√©finissant le s√©quen√ßage des appels API et le mappage des donn√©es avec les appels API en tenant compte des exigences du client.\nCollaborer avec les √©quipes techniques pour int√©grer efficacement les API dans nos applications et syst√®mes.\nAnalyser et am√©liorer les API existantes pour optimiser les performances et la s√©curit√©.\nProposer des solutions innovantes pour r√©soudre les probl√®mes et am√©liorer l'exp√©rience utilisateur.\nAssurer la documentation compl√®te des API d√©velopp√©es ou modifi√©es.\nR√©diger un plan de test principal et concevoir des cas de test pour valider la configuration de l'outil, d√©velopper des scripts de cas de test automatis√©s le cas √©ch√©ant et enrichir les suites de tests de r√©gression sur la base du plan de test d√©fini.\nAnalyser les API des services web et la documentation des √©crans d'interface utilisateur pour √©laborer des documents de cartographie d'interface.\nMaintenir des scripts pour la population et la migration des donn√©es, en utilisant Python et VBA.\nValider et v√©rifier les configurations livr√©es √† nos clients.\nSuivre les donn√©es pour les KPI afin de mesurer l'effort de l'√©quipe, et contribuer √† la cr√©ation de rapports.\nAppliquer le mod√®le de gouvernance concernant la propri√©t√© des donn√©es, l'acc√®s aux donn√©es et le cycle de vie des changements de donn√©es.\nComp√©tences Requises :\nExp√©rience pratique significative dans le d√©veloppement d'API, y compris SOAP, UI et REST.\nMa√Ætrise des langages de programmation courants pour le d√©veloppement d'API (comme Python, Java, Node.js, etc.).\nCompr√©hension approfondie des bonnes pratiques de conception d'API, de la s√©curit√© et de la gestion du cycle de vie des API.\nCapacit√© √† travailler efficacement dans un environnement agile, en √©quipe multidisciplinaire.\nSolides comp√©tences en r√©solution de probl√®mes et capacit√© √† travailler de mani√®re autonome.\nQualifications Additionnelles :\nDipl√¥me (bac+5 ou dipl√¥me d'ing√©nieur) en informatique, g√©nie logiciel, ou exp√©rience √©quivalente.\nExp√©rience pr√©alable dans le d√©veloppement de solutions de donn√©es ou d'int√©gration.\nInformations Compl√©mentaires :\nCe poste offre l'opportunit√© de travailler dans un environnement stimulant, o√π l'innovation et la collaboration sont encourag√©es. Si vous √™tes passionn√© par le d√©veloppement d'API et la gestion de donn√©es, et que vous souhaitez contribuer √† des projets stimulants et agiles, alors vous √™tes la personne que nous recherchons !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "AXA en France",
        "location": "Hauts-de-Seine, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/tech-lead-data-engineer-at-axa-en-france-3905641945?position=5&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=GT9b1dyWWtVSzrqUxtYdmg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nTech Lead Data Engineer F/H\n, vous allez contribuer directement aux projets des directions m√©tier (ex : fraude sant√©, multi√©quipements, pricing IARD, optimisation du lead management, fragilit√© auto, ‚Ä¶) d‚ÄôAXA France et √† la construction du socle technique Big Data.\nVous allez int√©grer une √©quipe d'une dizaine de personne compos√©e de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus m√©tier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transfo. & Tech. d'AXA France en quelques mots :\nUne organisation agile en feature teams : tribus, guildes, squads\nDes projets sur des applications innovantes √† fort trafic (web, mobile‚Ä¶)\nDes m√©thodologies craft (TDD, BDD, clean code, code review‚Ä¶) et DevOps\nUne communaut√© de partage de bonnes pratiques (BBL, dojo, meetup, conf‚Ä¶)\nVotre r√¥le et vos missions\nVous aurez pour missions principales de d√©velopper les projets Big Data demand√©s par le m√©tier, et notamment :\nD‚Äôaccompagner techniquement les Data Engineer de l‚Äô√©quipe (coaching, code review, pair programming‚Ä¶)\nPasser de la donn√©e brute √† de la donn√©e exploitable, expos√©e sous forme de tables requ√™tables dans le datalake\nConsolider ces donn√©es au fur et √† mesure de leur alimentation r√©currente dans le data lake\nLes exploiter pour atteindre la finalit√© business (exposition de business view, r√©int√©gration des r√©sultats dans le SI, service de scoring, ‚Ä¶)\nDe travailler √† la cr√©ation du socle technique Big Data et industrialiser le cycle de d√©veloppement de l'√©quipe\nDe mettre en place et de garantir le respect dans la dur√©e d'un processus qualit√© sur l'ensemble du cycle de DEV (documents, tests unitaires / int√©gration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nD'une formation sup√©rieure en informatique ou scientifique (Master ou Dipl√¥me d'ing√©nieur), vous justifiez de plusieurs exp√©riences significatives (+ de 7 ans)\nsur du d√©veloppement big data, en particulier sur du PySpark.\nComp√©tences techniques :\nConnaissances avanc√©es en d√©veloppement en\nPySpark (Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avanc√©es d'outils de BI comme\nPowerBI\nComp√©tences transverses :\nCapacit√© √† interagir avec des parties prenantes diverses : Business analyst, Architectes, M√©tier\nExp√©rience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer sur le plan op√©rationnel\nEt Id√©alement :\nAvoir une exp√©rience en tant que lead\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming avec Kafka\nMaitrise des Bases de donn√©es relationnelles et NoSQL\nUne exp√©rience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nMais pourquoi AXA France ?\nNous sommes persuad√©s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs ! Les avantages que nous proposons √† nos salari√©s sont nombreux.\nNous choisir, c‚Äôest b√©n√©ficier par exemple :\nD‚Äôun package de r√©mun√©ration complet comprenant un salaire fixe, un compl√©ment de r√©mun√©ration variable, des primes, de la participation et de l‚Äôint√©ressement, la possibilit√© d‚Äôacqu√©rir des actions AXA, ou encore des solutions d‚Äô√©pargne avantageuses ;\nEquilibre vie Pro / Perso. : D‚Äôun cadre de travail flexible jusqu‚Äô√† 3 jours de t√©l√©travail possible par semaine, des tickets restaurant pour les jours t√©l√©travaill√©s ou encore une participation √† l‚Äôachat d‚Äôun √©cran ou fauteuil ergonomique ;\nD‚Äôune politique visant √† concilier vie personnelle et vie professionnelle avec 28 jours de cong√©s pay√©s, entre 14 et 16 RTT selon les ann√©es, des formules de travail √† temps partiel ou encore des jours d‚Äôabsence r√©mun√©r√©es pour la rentr√©e scolaire ou un d√©m√©nagement par exemple ;\nDe la possibilit√© de s‚Äôengager pour une cause qui vous tient √† c≈ìur gr√¢ce √† nos associations telles que AXA Atout C≈ìur, AXA Comp√©tences Solidaires ou encore AXA Pr√©vention ;\nEt bien plus encore ! Perspectives de d√©veloppement des comp√©tences et de carri√®res immenses, CE, conciergerie, offres privil√®ges, soutien en cas d‚Äô√©preuve personnelle‚Ä¶On s‚Äôarr√™te l√†, la liste est longue\nQui sommes nous ?\nAXA est un des leaders de l‚Äôassurance et de la gestion d‚Äôactifs dans le monde.\nNous aidons nos 108 millions de clients √† traverser les petites et grandes difficult√©s de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure mani√®re de les prot√©ger et voulons donner √† chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuad√©s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C‚Äôest pour cette raison que nous menons une politique RH engag√©e qui favorise la diversit√©, qui pr√©serve l‚Äô√©quilibre vie priv√©e-vie professionnelle et acc√©l√®re le d√©veloppement des comp√©tences et des carri√®res.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une v√©ritable culture d‚Äôexpertise, acc√©l√©rant le d√©veloppement des comp√©tences de chacun et proposant une r√©mun√©ration attractive.\nPourquoi nous rejoindre ?\nVous √™tes porteur d‚Äôid√©es et d‚Äôinitiatives innovantes ? Vous proposez des solutions et √™tes au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunit√©s de carri√®res int√©ressantes\nUne entreprise qui donne une place de choix √† l‚Äôinnovation, √† l‚Äôinitiative et aux actions solidaires (notamment via l‚Äôassociation AXA Atout C≈ìur)\nUn environnement inclusif √† tous les niveaux (mixit√©, handicap, initiatives pour favoriser l‚Äôinsertion des jeunes, orientation sexuelle, etc.)\nUn acc√®s √† de multiples avantages (cong√©s, temps partiel, t√©l√©travail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d‚Äôenrichir ses comp√©tences\nVictime ou t√©moin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination √† alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "3, 3",
            "Level": "",
            "Experience": "7 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Akkodis",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-akkodis-3890779946?position=6&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=Oa7lMHaesBbGmnh3YiFQig%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La ligne de service Consulting & Solutions d‚ÄôAkkodis France renforce ses √©quipes en r√©gion Hauts-de-France et recrute un\nData engineer H/F\nen\nCDI\nsur la\nm√©tropole lilloise\n:\nDescription de la mission :\nConcevoir, mettre en oeuvre et maintenir des pipelines de donn√©es efficaces et √©volutifs dans un environnement cloud (comme AWS, Azure, Google Cloud Platform‚Ä¶)\nAssurer la qualit√© des donn√©es et des mod√®les\nD√©finir les bonnes pratiques de d√©veloppement en impl√©mentant des outils de CI/CD\nAssurer une veille technologique sur les technologies Cloud\nCapacit√© √† interagir avec des parties prenantes diverses : business analyst, architecte, m√©tier‚Ä¶\nVeiller au bon fonctionnement des pipelines en production\nProfil :\nDe formation\nBac +4/5 en informatique\nou issu d'une\n√©cole d'ing√©nieur\n, vous poss√©dez une exp√©rience de\n3 ans\nminimum en tant que data engineer ainsi que les comp√©tences suivantes :\nUne bonne connaissance des √©cosyst√®mes li√©s √† la data (Kafka, ETL, base de donn√©es‚Ä¶)\nUne premi√®re exp√©rience sur un cloud provider (AWS, Azure, GCP)\nUne bonne maitrise de langages de programmation tels que SQL, Python, Scala\nAkkodis accompagne ses clients dans la mondialisation de leurs projets, aussi un anglais courant est requis pour l‚Äôensemble de nos collaborateurs.\nProcessus de recrutement :\nUne charg√©e de recrutement vous contacte pour √©changer sur votre projet professionnel\nVous √©changez ensuite avec un.e manager sur les aspects techniques, les projets\nChez Akkodis nous sommes convaincus que de l‚Äôintelligence collective na√Æt le succ√®s. Il n‚Äôexiste pas qu‚Äôun mod√®le, nous valorisons l‚Äôagilit√© et l‚Äôexcellence, l‚Äôaudace et la cr√©ativit√©.\nEt si nous parlions ensemble de vos ambitions pour les prochaines ann√©es ?\nAkkodis est une entreprise handi-engag√©e et inclusive. Tous nos postes sont ouverts aux handicaps et √† la diversit√©. Tous diff√©rents, tous comp√©tents !\nAkkodis, est un acteur mondial de l‚Äôing√©nierie et de l‚ÄôIT et un leader dans la smart industrie. Nous accompagnons nos clients dans leurs projets de transformation digitale via 4 lignes de service : Consulting, Solutions, Talent et Academy. Akkodis est un partenaire technologique de confiance pour ses clients √† l‚Äô√©chelle internationale. Nous co-cr√©ons et nous imaginons des solutions de pointe pour r√©pondre aux d√©fis majeurs de notre soci√©t√©, qu'il s'agisse d'acc√©l√©rer la transition √©nerg√©tique et de d√©velopper la mobilit√© verte, ou encore de construire des approches centr√©es sur les utilisateurs.\nDot√©s d‚Äôune forte culture de l‚Äôinclusion et de la diversit√©, nos 50 000 experts en IT et en ing√©nierie, pr√©sents dans 30 pays, allient les meilleures comp√©tences technologiques √† une connaissance transverse de toutes les industries pour fa√ßonner un futur plus durable. Nous sommes passionn√©s par l‚Äôid√©e d‚Äôinventer ensemble un avenir meilleur.\nAkkodis en France, ce sont pr√®s de 9.000 experts en IT et en ing√©nierie r√©partis sur l'ensemble du territoire, des collaborateurs partageant des valeurs fortes d'honn√™tet√©, de respect, d'√©quit√© et d'inclusion. Notre engagement : leur permettre au quotidien d'√™tre eux-m√™mes au travail, et acteurs de leur vie et de leur d√©veloppement au sein d'Akkodis.\n*Akkodis est une marque commerciale sous laquelle les entit√©s AKKA et Modis op√®rent\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Other": [
                "Cloud",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Mobiskill | WEFY Group",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mobiskill-wefy-group-3907391938?position=7&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=eT3vGYULy5bSpvwf%2FX9gYw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La soci√©t√© ?\nCette startup a √©t√© cr√©√©e en 2018 et vise √† aider la prise de d√©cision de ses clients qui sont principalement dans le secteur du retail ou de l'alimentaire.\nIls permettent d'enrichir la donn√©e afin d'am√©liorer la strat√©gie de vente et marketing d'une entreprise gr√¢ce √† leur plateforme Saas bas√©e sur des algorithmes d'IA.\nIls ont besoin de renforcer leur √©quipe en Data Engineering pour g√©rer au mieux leur volum√©trie.\nLes missions ?\n- Editer le cahier des charges des donn√©es √† collecter aupr√®s de nos partenaires distributeurs\n- Prendre en main la gestion de la donn√©e dans le cloud de la soci√©t√© pour optimiser les co√ªts et l‚Äôefficacit√© des analyses effectu√©es par l‚Äô√©quipe Analytics\n- Anticiper les √©volutions et participer aux choix structurants de la soci√©t√© li√©s √† la gestion de la data\nLe profil recherch√© ?\n- Avoir 2/3 ans d'exp√©rience en Data Engineering (hors stage et alternance)\n- Avoir pu travaill√© en Python comme langage de programmation\nAvoir travaill√© au moins deux ans et si possible sur des sujets d'optimisation avec Spark !\n- La ma√Ætrise des outils tels Airflow, Kafka et Snowflake seraient un plus appr√©ci√©\n- Ma√Ætriser un des cloud providers et si possible avoir une exp√©rience sur Azure\nPourquoi les rejoindre ?\n- Une soci√©t√© stable financi√®rement (fonds propres uniquement)\n- Une startup en pleine croissance\n- Une r√©mun√©ration en fonction de votre s√©niorit√©\n- Volum√©trie de donn√©es incroyable, il y a de quoi s'amuser !\n- Faire parti de l'unique retail-tech qui a un impact √©cologique positif (fin des prospectus, √©viter le g√¢chis alimentaire)\nH√¢te de vous en dire plus rapidement !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "FRG Technology Consulting",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-data-ops-at-frg-technology-consulting-3913842168?position=8&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=cQ8SitVQDCHcJKbOT0F5og%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous √™tes un expert passionn√© par la Data et √† la recherche de d√©fis excitants ? Mon client recherche actuellement un\nData Engineer\n/ Data Ops\ntalentueux pour rejoindre une √©quipe dynamique et humaine.\nMissions principales :\nParticipation active au d√©ploiement de la nouvelle plateforme sur Azure & Snowflake\nForte autonomie et gestion compl√®te des projets data\nAnalyse des besoins actuels et futurs\nCr√©ation de sp√©cifications fonctionnelles et techniques\nMod√©lisation de donn√©es\nD√©veloppement de packages SSIS\nInt√©gration des donn√©es dans SnowFlake & Azure,\nCr√©ation de rapports avec Power BI et Excel\nProfil recherch√© :\n3 √† 4 ans d'exp√©rience\nminimum\ndans la BI (SSIS, SQLServer, SSAS, SSRS) et/ou le cloud (Azure , Snowflake) ainsi qu'en SQL\nComp√©tences en\narchitecture sur Snowflake\nfortement appr√©ci√©es\n1 √† 2 ans d'exp√©rience en tant que DevOps ( CI/CD ; GitLab)\nAutonome, rigoureux et anglais courant\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "3",
            "Level": "",
            "Experience": "4 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud",
                "DevOps",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Extia",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-extia-3599188121?position=9&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=pq5NdvpIJcK61%2FJVu1Ybeg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vous souhaitez rejoindre une entreprise qui place l‚Äôhumain au c≈ìur de ses pr√©occupations ? On vous attend chez\nExtia\n!\nSoci√©t√© de conseil sp√©cialis√©e dans les m√©tiers de l‚ÄôIT, de l‚Äôing√©nierie et du digital, Extia privil√©gie depuis sa cr√©ation en 2007 une approche qui allie performance et bien-√™tre au travail. Une vision de l‚Äôentreprise partag√©e aujourd‚Äôhui par plus de 2 500 Extiens en France et √† l'international et r√©compens√©e par le label Great Place to Work¬Æ depuis 13 ans, notamment en\n2024 o√π les Extiens se hissent √† la premi√®re place du palmar√®s Best Workplaces France\n!\nChez Extia, c‚Äôest ¬´ D‚Äôabord qui, ensuite quoi ¬ª alors, allons-y !\nD'abord qui\nVous √™tes habitu√© √† travailler aussi bien avec des m√©ta-donn√©es qu‚Äôavec des donn√©es non-structur√©es. A cet effet vous maitrisez un ou plusieurs des concepts comme l‚ÄôETL, le Data mining le Machine learning, les Big data ou encore la Th√©orie des graphes par exemple,\nVous maitrisez les bases de l‚Äôanalyse statistique,\nVous √™tes apte √† r√©diger des scripts en Python et/ou R, et une connaissance d'autres langages de programmation comme Java, Scala ou SAS est un plus,\nVous maitrisez Spark et Hadoop\nVous √™tes familiaris√© avec l‚Äôenvironnement Linux,\nUne exp√©rience avec les outils de Stockage de fichiers volumineux (HDFS, Data Lake, S3, stockage Blob), la connaissance des infrastructures cloud AWS ou GCP et des bases en streaming temps r√©el seront aussi de r√©els atouts.\nEnsuite quoi\nVous aurez le r√¥le de support technique aux √©quipes d‚Äôanalyse : structurer les donn√©es, r√©aliser des analyses ¬´ statistiques ¬ª ou ¬´ techniques ¬ª sur les donn√©es, d√©velopper des outils d‚Äôanalyse‚Ä¶\nVous m√®nerez des √©tudes afin d‚Äô√©valuer les nouvelles technologies dans le domaine du Big Data, Data Mining ou Machine Learning afin d‚Äôidentifier les solutions les plus pertinentes.\nVous serez en charge de :\nParticiper √† la d√©finition des besoins et √† la r√©daction des User Stories,\nCollaborer avec les Data Scientists au d√©veloppement des modules d‚Äôanalyse de donn√©e,\nConcevoir et construire des architectures de donn√©es,\nInt√©grer des sources de donn√©es,\nVous assurez que les donn√©es sont facilement accessibles et que leur exploitation fonctionne comme demand√©, m√™me dans des circonstances hautement √©volutives,\nEx√©cuter des processus ETL (extraire / transformer / charger) √† partir d'ensembles de donn√©es complexes et / ou volumineux\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "13 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala",
                "Python"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS"
            ],
            "OS": [
                "Linux"
            ],
            "Other": [
                "Statistiques",
                "Machine Learning",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Lyon, Auvergne-Rh√¥ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-snowflake-at-mindpal-3910994899?position=10&pageNum=7&refId=dBp3nd8lT7q7GiEDXjKL6A%3D%3D&trackingId=JBJP3r8cDISpd7T04G1vsQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for experienced\nData Engineers\nwith knowledge of\nSnowflake\nplatform.\nResponsibilities\nCreating and managing data in the Snowflake environment\nDesigning and implementing ETL (Extract, Transform, Load) solutions for transferring data between various sources and platforms\nOptimizing the performance of Snowflake databases, including designing and implementing data structures and using indexes appropriately\nAutomating data processing workflows using tools such as Airflow or other workflow management tools\nDeploying and configuring tools to monitor and report on the performance of the Snowflake system\nRequirements\nMinimum 1 year of experience as a Data Engineer\nAbility to use Snowflake\nVery good knowledge of SQL and programming in Python\nAbility to work with databases, including the Snowflake platform\nKnowledge of ETL tools and data integration\nAbility to work in a team and good communication skills\nFluent English in speaking and writing\nWe Offer\nB2B contract type\nFull-time job\nRemote and flexible working hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Orange Business",
        "location": "Lille",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-talend-f-h-at-orange-business-3916552363?position=1&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=B7xw7hTucHearR4LMUncOg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "L‚Äôambition d‚ÄôOrange Business est de devenir l‚Äôint√©grateur r√©seau et num√©rique de r√©f√©rence en Europe, en nous appuyant sur nos forces autour des solutions de connectivit√© nouvelle g√©n√©ration, du cloud et de la cybers√©curit√©.\nNos 30 000 femmes et hommes pr√©sents dans 65 pays, dont chaque voix compte, sont tous anim√©s par la m√™me d√©termination et le m√™me esprit d‚Äô√©quipe, pour construire les solutions digitales d‚Äôaujourd‚Äôhui et de demain et cr√©er un impact positif pour nos clients, pour leurs salari√©s et pour la plan√®te.\nNous offrons des opportunit√©s passionnantes gr√¢ce √† des projets innovants dans la data et le digital, le cloud, l‚ÄôIA, la cybers√©curit√©, l‚ÄôIoT, ou encore le digital workspace et le big data.\nVenez vivre cette aventure avec nous !\nAfin de d√©velopper notre √©quipe lilloise, nous recherchons aujourd'hui, un Ing√©nieur DATA √† m√™me d‚Äôaccompagner nos clients dans la structuration de leurs SI autour de la donn√©e.\nVos principales missions seront les suivantes\n:\n- Concevoir des solutions de traitement et collecter des volumes importants de donn√©es.\n- Participer √† des √©tudes de cadrage pour collecter le besoin m√©tier et concevoir les solutions qui r√©pondent au besoin du client.\n- Apporter son expertise sur des probl√©matiques pr√©cises rencontr√©es chez les clients.\n- Participer √† la veille technologique\n- R√©aliser les\nd√©veloppements TALEND\n- Rester inform√© et former sur les nouvelles solutions DATA\n- Contribuer aux phases d'avant-vente et au d√©veloppement business.\n- Participer √† la conception, l'√©volution et la pr√©sentation de nos offres DATA.\nVous\n:\n- √ätes issu(e) de formation bac+5 ?\n- Vous justifiez d'au moins 3 ans d'exp√©riences en qualit√© d'Ing√©nieur DATA sur la solution TALEND Enterprise (Data Integration) et avez id√©alement une connaissance des solutions Cloud d'AWS et d'AZURE ?\n- Vous √™tes intervenu sur des projets int√©grant des pratiques DevOps et AGILE ?\nAlors postulez, ce poste est fait pour vous !\nVos comp√©tences cl√©s\n:\n- Expertise sur l'outil\nETL TALEND\nEnterprise (administration et d√©veloppement)\n- Fortes connaissances des solutions de bases de donn√©es (SQL, NoSQL‚Ä¶)\n- Connaissances en langages objets ou scripts (notamment Java mais aussi Javascript, Scala, Python‚Ä¶)\n- Divers syst√®mes d'exploitation : UNIX, Windows\nAutonomie, rigueur, curiosit√©, dynamisme et sens du service sont des qualit√©s n√©cessaires pour ce poste.\nLes comp√©tences compl√©mentaires qui seraient appr√©ci√©es :\n- Connaissances d'autres modules Talend (MDM, ESB, Data Quality, Cloud‚Ä¶)\n- Ma√Ætrise des technologies du Big Data (Hadoop, Spark, Kafka‚Ä¶)\n- Expertise sur d'autres outils ETL (Informatica, SSIS, DataStage...)\n- Notions en architecture des Syst√®mes d'Information\n- Ma√Ætrise de l'anglais (oral et √©crit)\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "MachineLearning": [
                "Orange"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "OS": [
                "Windows"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "√éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-aws-azure-at-apside-3825012802?position=2&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=tCEzMqlfv%2BbUvDPus74BWA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üí•\nD√©couvrez la Vie Apsidienne\nüìπ\net vous aussi, devenez Apsidien\nOn aurait pu demander √† Chat GPT de vous d√©montrer en quoi\nApside est l‚ÄôESN qu‚Äôil vous faut,\nmais on pr√©f√®re que vous le d√©couvriez vous-m√™mes üëáüòè\nüî•\nD√©couvrez votre future mission\nüëâ\nContexte\nRejoignez notre Practise Cloud/Data, afin d‚Äôintervenir sur des sujets √† haute valeur ajout√©e !\nSecteur\n: Banque/Finance\nM√©thode de travail\n: Agile\nL‚Äô√©quipe Data de l‚Äôun de nos clients grands compte du secteur bancaire vise √† faciliter la construction de parcours transversaux par les DSI en proximit√© des M√©tiers en leur proposant des solutions technologiques sur √©tag√®re, qu'elles pourront assembler rapidement, de mani√®re agile.\nDes solutions informatiques bas√©es sur les technologies BigData sont donc mises en place dont une qui est un framework de contr√¥les. Celle-ci est propos√©e aux applications du groupe afin de les aider dans l'impl√©mentation des contr√¥les de Data Quality sur les plates formes Bigdata on premise, ainsi que cloud Azure et AWS.\nOr ils souhaitent aujourd‚Äôhui mettre en oeuvre plusieurs √©volutions de son socle technique.\nüòé Mission\nL'ajout de fonctionnalit√©s sur le moteur de calcul\nIHM de param√©trage\nCompatibilit√© avec la plateforme Azure et AWS\nD√©veloppements des √©volutions sur le moteur des contr√¥les\nTests (en TDD) en mode Agile\nContribution √† la validation de l'usage de solution en production pour les nouvelles applications utilisatrices du framework.\nEnvironnement technique\n:\nAmazon Web Services\nGitHub\nHadoop\nKubernetes\nMS Azure\nPython\nScala\nSpark\nüí∞\nLe package salarial que nous vous proposons\nContrat :\nCDI\nAvantages groupe :\ncarte ticket restaurant Swile, prime de mobilit√©, RTT, accord t√©l√©travail, Mutuelle, prime de cooptation, avantages CE, prise en charge de la mutuelle √† 100% etc‚Ä¶\nAvantages agence :\nint√©gration de la Practise Cloud/Data, afterworks, communaut√© techlead\nFormation :\ncertifications techniques, cours particuliers d‚Äôanglais en interne, acc√®s √† un catalogue de formations gr√¢ce √† notre plateforme e-learning (\nAcademy by Apside\n) ou via nos organismes partenaires.\nüîÆ\n√î vous futur Apsidien, qui √™tes-vous ?\nAu moins 5 ans d'exp√©rience en tant que Data Engineer\nMaitrise de l‚Äôenvironnement cloud AWS ou Azure\nForce de proposition, bon relationnel et autonome\nüòè\nApside a suscit√© votre curiosit√© ?\nDans un environnement marqu√© par une acc√©l√©ration des √©volutions technologiques, de transformations des usages et de disruptions majeures, Apside est un partenaire de confiance qui accompagne ses clients √† cr√©er de la valeur et √† adresser leurs enjeux strat√©giques en leur mettant √† disposition des expertises technologiques (\nData / IA, Cloud, Cyber\n) et une exp√©rience sectorielle (\nIndustrie, Banque, Assurance, Service, Secteur Public\n). Pour un accompagnement global, le groupe propose des offres transverses autour du\nHandicap\n(Apsid‚ÄôEA), du\nDigital Learning\n, et du\nConseil\n.\nü§î\nEt votre place dans tout √ßa ?\nüëâ Notre volont√©\nest de vous accompagner dans la construction et l‚Äô√©panouissement de votre carri√®re\nen nous appuyant notamment\nsur 3 piliers :\nUne\nr√©mun√©ration\n√† hauteur de vos investissements et de vos comp√©tences\nUne\ntrajectoire professionnelle\nstimulante sur mesure\nUn\nengagement\nautour des valeurs Apsidiennes : la qualit√© de vie et des conditions de travail au c≈ìur de nos enjeux\nEngag√©e pour\nun monde plus inclusif et plus responsable\n, Apside r√©invente l‚ÄôESN et propose l‚ÄôEngagement Soci√©tal et Num√©rique. D√©couvrez notre d√©marche RSE ainsi que notre vision de l‚ÄôEntreprise Engag√©e.\nConvaincu ? A vous de jouer, envoyez-nous votre CV !\nRejoignez l‚Äôaventure Apsidienne et d√©couvrez notre vision d‚Äôune ESN singuli√®re et r√©siliente\nüöÄ\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "100",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "HarfangLab",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-harfanglab-3849760187?position=3&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=skdkpLczYGSCzfAm1n2LAw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Who we are?\nHarfangLab\nis a\ncybersecurity scale-up\n, and we have developed an\nEndpoint Detection and Response\n(EDR) software to\ndetect and mitigate modern cyberattacks\non a company's workstations and servers. Our algorithms detect abnormal behaviors and generate security alerts or block program execution.\nFrom 50 to 100 employees in 2023, HarfangLab is experiencing hypergrowth and has already achieved several significant milestones: winners of the Ministry of Defense's cyber challenge in 2019, recipients of the BPI's I-Nov competition in 2020, and software certified by ANSSI in 2021.\nOur initial clients include CAC40 industrial companies and government entities. We completed our\nfirst funding round of ‚Ç¨5 million in 2021 and our second funding round of ‚Ç¨25 millions in 2023\n, which will enable us to strengthen our teams, and to expand internationally in Europe.\nOur mission is to\nprotect businesses and government agencies from modern cybersecurity threats\n(cybercrime, data theft, influence)\nthat endanger the economic health of companies and the security of the nation\n.\nWhat you will do with us?\nYou will work within the\nArtificial Intelligence team\n, consisting of 5 individuals, under the direct and daily supervision of the team lead.\nThis team designs, implements, and deploys supervised algorithms for detecting malicious behavior.\nAs a\nData Engineer\nyou will:\nGather requirements from stakeholders,\nManage data for the AI and CTI departments,\nDesign, develop, and maintain the existing data warehouse,\nImplement a data lake if deemed appropriate,\nCreate data pipelines using ELT processes,\nDesign tools for data visualization.\nAbout You\nHard Skills\nMaster‚Äôs degree in Computer Science, Engineering, or a related field,\nProven experience as a Data Engineer, 2 years minimum,\nProficient in Python,\nSQL: Strong in SQL syntax and query optimization, NoSQL will be a big plus,\nCompetence in data warehousing and data lake architecture,\nProficiency in at least one ELT tool and strong understanding of related processes.\nSoft Skills\nStrong communication and teamwork skills,\nExcellent problem-solving and attention to detail,\nYou enjoy learning and sharing your knowledge with others,\nYou demonstrate initiative - when an opportunity arises to improve existing processes, you seize it.\nAbout Us\nOur office and Team Life:\nOffices located in the heart of Paris, near Bourse (75002),\nHigh-quality equipment based on preferences and needs (PC, Mac, additional screens, etc.),\nThanks to our Office Manager, we regularly organize events such as seminars, happy hours, themed evenings, and more,\nAn onboarding process to welcome each new colleague with an explanation of the roles and a mentor to support you during your early days!\"\nA great team that always seeks to improve their skills\nAnd more:\nAn attractive package: Base salary + profit sharing,\nFlexible remote work options,\nA mentor to guide you throughout your probationary period,\nHealth insurance: The best health insurance with Alan and Moka Care, a mental health at work app,\nMeal vouchers: We use the Swile card and also have access to a discount platform through our works council,\n7 to 11 additional days off (RTT) per year, in addition to the 25 days of paid vacation. GymLib subscription, covered 80% by HarfangLab,\nAccess to training and events of your choice and according to your professional needs.\nThe recruitment process\nA 30-minutes call with our Talent Acquisition Manager,\nA 30-minutes visio interview with the Hiring Manager,\nA 1 hour on-site interview + 30 minutes with the team for a team fit assessment,\nA psychometric test to assess your motivations and soft skills,\nA final HR video appointment to review your soft skills and motivations.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "7, 7",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "Collaboration": [
                "Teams"
            ],
            "EnSoftSkils": [
                "Initiative",
                "Teamwork",
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Ippon Technologies",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/lead-data-engineer-at-ippon-technologies-3851535052?position=4&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=SDvZzVRJfCa5AUR0lcL0eQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Cabinet de conseil et d'expertise en technologie, international et ind√©pendant.\nEn quelques mots : 700 passionn√©s de tech, 12 agences dans le monde, 6 communaut√©s d‚Äôexpertise d‚Äôexcellence, contributeur actif et sponsor de l'√©cosyst√®me num√©rique, des publications soutenues et reconnues sur nos r√©seaux.\nRejoignez notre communaut√© de 70 experts en data, dont 30 √† Paris, o√π la collaboration dynamique entre data engineers, data analysts et data architects est le moteur de notre succ√®s. Avec une communication proactive sur des canaux internes, restez constamment inform√© des derni√®res tendances, participez √† des discussions stimulantes et contribuez √† l'organisation d'√©v√©nements passionnants (dataday, datap√©ro, datalunch‚Ä¶).\nFaites partie d'une √©quipe o√π l'innovation et l'engagement sont les cl√©s de notre excellence collective !\nNotre sp√©cialit√© ? construire des data platforms dans le cloud public avec les meilleures technos du moment.\nEn tant que tech lead, tu interviendras sur la cr√©ation d'un entrep√¥t de donn√©es pour les KPIs d‚Äôun grand groupe dans le secteur de l‚Äô√©nergie. Le but √©tant de leur permettre de superviser leurs activit√©s afin de supporter leurs d√©cisions strat√©giques.\nTon r√¥le :\nIntervenir sur l‚Äôarchitecture et le d√©veloppement d‚Äôune pipeline d'alimentation de donn√©es\nTravailler sur la mod√©lisation et l‚Äôimpl√©mentation de l'entrep√¥t de donn√©es\nConseiller et accompagner les √©quipes dans la r√©alisation des dashboards de suivi des KPIs\nDevOps: projet enti√®rement Terraform√© (ressources + droits), CI/CD Gitlab, administration GCP\nFaire une veille technologique active et partager tes connaissances en interne\nTravailler en collaboration avec les m√©tiers et les data analysts pour leur fournir un support √† l‚Äôindustrialisation de leurs travaux (tests, int√©gration continue, scalabilit√© des mod√®les, craftsmanship etc‚Ä¶)\nSi tu le souhaites, tu pourras √©galement :\nParticiper aux √©v√®nements internes √† la communaut√© data (BBL, webinar, datap√©ro interne, meetup, blog, dojos) et externes (Salon du Big Data, GCP Summit, Spark Summit, AWS Summit, Devoxx, workshop partenaire, meetups)\nCapitaliser sur les missions et les diff√©rents √©v√®nements de la communaut√© au travers d‚Äôarticles de blogs, REX, BBL interne.\nTes connaissances :\nTu ma√Ætrises le d√©veloppement en Python\nTu as de l‚Äôexp√©rience dans la mise en place de pipeline de donn√©es jusqu‚Äôen production (CI/CD Gitlab, Terraform)\nTu as une exp√©rience dans un environnement Cloud (GCP de pr√©f√©rence, AWS, Azure)\nTu as une bonne connaissance d‚Äôun outil de visualisation (Looker Studio, Power BI)\nTu accompagnes des data engineers dans la mise en place des bonnes pratiques\nTu es capable de proposer/challenger la stack technique\nIppon c‚Äôest aussi :\nTravailler en √©quipe au sein d'une communaut√© data √† la pointe des √©volutions\nUn suivi de proximit√© r√©alis√© par ton manager (expert data)\nDevenir ceinture noire en data gr√¢ce √† notre programme d‚Äôaccompagnement de carri√®re Blackbelt\nParticiper √† nos ap√©ros et divers √©v√®nements internes pour consolider la coh√©sion d‚Äô√©quipe\nNotre process de recrutement :\nPr√©qualification t√©l√©phonique - 20 min\nUn entretien RH / Sales - 1H00\nUn entretien technique avec 2 consultants data\nSi le match est bon des deux c√¥t√©s : Hadjim√© ! Tu te lanceras sur le tatami Ippon !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "HUTTOPIA Jobs",
        "location": "Saint-Genis-les-Olli√®res",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-alternance-h-f-at-huttopia-jobs-3902055574?position=5&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=eGiPNGddRzkmDAThGsqSWg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Huttopia, op√©rateur international reconnu du tourisme durable et acteur du d√©veloppement territorial, poursuit un d√©veloppement soutenu en France et √† l‚Äôinternational (Pays-Bas, Espagne, Canada, USA, Chine‚Ä¶). Pr√©sent dans les domaines de l‚ÄôHospitality avec 127 camping-nature exploit√©s, les activit√©s industrielles avec la fabrication d‚Äôh√©bergements en bois et toile, et dans le num√©rique, le groupe Huttopia a g√©n√©r√© en 2023 un chiffre d‚Äôaffaires de 160 M‚Ç¨ avec plus de 700 collaborateurs permanents et plus de 1800 personnes l‚Äô√©t√©.\nPour accompagner son fort d√©veloppement, Huttopia recrute ses talents de demain !\nNous recherchons notre futur\nData Engineer H/F\nen alternance √† compter de\nseptembre 2024.\nVOS MISSIONS :\nSous la responsabilit√© hi√©rarchique du Responsable Data et sous la responsabilit√© technique de la Data Engineer, vous interviendrez comme :\nCo-responsable de\nl‚Äôexploitation des donn√©es\n:\nProduire et livrer des tableaux de bord (sous Tableau)\nParticiper √† l‚Äôadministration de l‚Äôoutil de Data Viz\nAccompagner et former des utilisateurs (lecteurs et d√©veloppeurs)\nProduire des demandes de chiffres ad-hoc en interrogeant la BDD centrale (SQL),\nParticiper aux d√©veloppements des algorithmes d‚Äôexploitation des donn√©es (en Python)\nParticipant actif √†\nla r√©cup√©ration et la structuration des donn√©es\n:\nD√©velopper des flux de r√©cup√©ration des donn√©es entre une source et la BDD centrale\nR√©aliser la maintenance et monitoring des flux d√©j√† d√©velopp√©s\nVOTRE PROFIL :\nVous √™tes le candidat id√©al pour rejoindre notre √©quipe si ‚Ä¶\nVous pr√©parez une formation sup√©rieure en M1 ou M2, sp√©cialis√©e en DATA dans le cadre de votre alternance.\nVous avez des connaissances en langage de programmation : SQL, Python et Java. Vous souhaitez d√©velopper vos comp√©tences sur les ETL type Talend et les outils de Data Viz (Tableau, Power BI, Qlik..) vous sont familiers.\nAu-del√† de vos comp√©tences, nous nous int√©ressons √† vous, √† votre personnalit√©.\nDot√© d‚Äôun bon relationnel, vous √™tes reconnu pour votre rigueur, votre sens du service et votre curiosit√© technique. Vous appr√©ciez travailler en √©quipe et vous √™tes adaptable.\nEnfin, vous √™tes attir√© par le secteur de l‚Äôoutdoor et notamment du tourisme !\nNous sommes l‚Äôentreprise qu‚Äôil vous faut si ‚Ä¶\nVous √™tes pr√™t √† relever de nouveaux d√©fis en rejoignant une entreprise aux collaborateurs engag√©s et autonomes.\nVous souhaitez √©voluer dans des bureaux en bois o√π il fait bon travailler avec des espaces pour se d√©tendre et des √©v√®nements festifs‚Ä¶ Parce que chez Huttopia on aime travailler s√©rieusement sans se prendre au s√©rieux.\nLES PLUS HUTTOPIA :\nTicket restaurant : 9.92‚Ç¨/jour travaill√©\nCh√®ques culture mensuels\nUne bonne mutuelle sant√©\nLES POINTS PRATIQUES :\nContrat apprentissage ou contrat de professionnalisation\nDate de d√©marrage : Septembre 2024\nDur√©e du contrat : 12 mois\nM√©tropole Lyonnaise √† Saint Genis les Olli√®res ‚Äì √† 5 minutes de Tassin-La-Demi-Lune\nPrise en charge √† hauteur de 50% de l‚Äôabonnement de transports en commun.\nNOTRE PROCESS DE RECRUTEMENT :\nB√©rang√®re, notre charg√©e de recrutement vous contactera pour un premier √©change en visio, vous passerez ensuite un entretien dans nos bureaux.\nVous serez inform√© √† chaque √©tape de l‚Äô√©volution de votre candidature.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI",
                "Tableau"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Lincoln France",
        "location": "Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-lincoln-france-3829857168?position=6&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=SmxRFRPcnUVxFqCIJQZpiw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "DATA ENGINEER H/F\nCDI\n3 ans minimum\nChez Lincoln\n, nous formons une communaut√© d'innovateurs passionn√©s qui red√©finissent l'analyse de donn√©es depuis\nplus de 30 ans\n. En tant que\nPure Player Data\n, notre expertise est reconnue dans les domaines\nde la Modern BI, du Big Data et de la Science des donn√©es\n.\nNotre mission ?\nTransformer les donn√©es en solutions concr√®tes pour nos clients grands comptes dans divers secteurs tels que la banque, le retail, les t√©l√©coms, l'industrie, la sant√©, etc.\nDescription de poste\nNous recherchons un\nData Engineer H/F\npour accompagner nos clients dans leurs projets strat√©giques.\nVos missions :\nConcevoir et d√©velopper des pipelines de donn√©es robustes et √©volutifs.\nInt√©grer et transformer des donn√©es provenant de diff√©rentes sources.\nD√©velopper et mettre en ≈ìuvre des algorithmes de traitement de donn√©es avanc√©s.\nCollaborer √©troitement avec les √©quipes clients pour comprendre leurs besoins et fournir des solutions adapt√©es.\nAssurer la qualit√© et la fiabilit√© des solutions d√©velopp√©es.\nPr√©requis :\nMa√Ætrise des langages de programmation (\nPython, Scala, etc\n.).\nConnaissance approfondie des bases de donn√©es et des technologies\nCloud (GCP, AWS, Azure, Snowflake, etc.)\nExp√©rience avec\nMySQL, PostgreSQL, MongoDB.\nMaitrise ETL/ELT (Talend, Stambia, etc.)\nSolides comp√©tences en conception et en optimisation de pipelines de donn√©es.\nExp√©rience de travail en\nm√©thode Agile\npour la gestion de projet et le d√©veloppement de solutions.\nCapacit√© √† travailler de mani√®re autonome et en √©quipe.\nExcellentes comp√©tences en communication et en r√©solution de probl√®mes.\nLes plus du poste :\nEnvironnement Collaboratif\n: projets innovants favorisant le partage des connaissances.\nAccompagnement individualis√© et de proximit√©\n: formations certifiantes, attribution d‚Äôun Career Manager pour vous orienter dans votre trajectoire professionnelle, opportunit√©s d‚Äô√©volution de carri√®re.\nFlexibilit√© du Travail\n: T√©l√©travail et horaires flexibles pour votre √©quilibre vie professionnelle-personnelle.\nR√©mun√©ration Comp√©titive\n: Salaire comp√©titif avec des avantages sociaux attrayants.\nMobilit√©\n: Possibilit√© de mobilit√© √† Paris, Lyon ou Aix-en-Provence offrant des exp√©riences diversifi√©es au sein de Lincoln.\nNotre processus de recrutement :\nun entretien RH (1h) et entretien technique (1h)\nCette annonce n‚Äôest pas faite pour vous si :\nVous √™tes freelance et vous comptez le rester !\nToujours l√† ? Postulez et rejoignez nos\n400 experts en Data\nüòâ.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "400",
            "Level": "",
            "Experience": "3 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "MongoDB"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake",
                "PostgreSQL",
                "MySQL"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Herm√®s",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/alternance-data-engineer-h-f-at-herm%C3%A8s-3889716412?position=7&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=wkNcO1RWrxC1NFezsJWZyA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "El√©ments de contexte\nHerm√®s Digital Ventes et Services recherche pour sa direction Data & Performance :\nUn Alternant Data Engineer (H/F)\nContrat d'alternance de 12 mois\nA partir de Septembre 2024\nBas√© √† Paris\nPrincipales activit√©s\nVous √™tes rattach√© au Data manager.\nVous avez pour principale mission d‚Äôaccompagner l‚Äô√©quipe Data dans les t√¢ches quotidiennes :\nReporting et statistiques de ventes et trafic (notamment via l‚Äôoutil Google Analytics et Google BigQuery)\nAnalyse des leviers d‚Äôacquisition de traffic SEA/SEO/Referral\nCr√©ation de Dashboard via l‚Äôoutil Google Data Studio\nParticipation aux travaux de CRO (Conversion Rate Optimization) et d‚ÄôAB testing\nMise en place d‚Äô√©tude pr√©dictive sur les donn√©es des sites Ecommerce\nProfil\nEtudiant en √©cole d‚Äôing√©nieur poss√©dant une forte culture Internet et une sensibilit√© aux probl√©matiques digitales e-commerce, vous avez une premi√®re exp√©rience en entreprise\nProfil technique ou ais√© avec la technique, une sp√©cialisation en digital est en plus\nOrganis√©, rigoureux, curieux, autonome, bonne expression √©crite et aisance relationnelle\nMa√Ætrise du Pack Office indispensable, ayant d√©j√† utilis√© Google Analytics\nLa connaissance d‚Äôoutils de BI / Datavisualisation serait appr√©ci√©e (Google Data Studio, Tableau Software, Bime ou QlikView/QlikSense, PowerBI), de Base de Donn√©es (SQL, MySQL, BigQuery)\nUne app√©tence pour la Data, ses languages (Python, R) et ses technologies (Notebooks, mod√©lisation statistique, Machine learning) est fortement appr√©ci√©e.\nAnglais courant souhait√©\nSensible aux produits hauts de gamme, vous souhaitez vous investir dans un stage riche et formateur\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "DBMS": [
                "BigQuery",
                "MySQL"
            ],
            "Other": [
                "Statistiques",
                "Machine Learning"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Valeuriad",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-f-h-at-valeuriad-3741223009?position=8&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=vksQPYG8IfOb6g1PzOjDwQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Rejoins la\nTeam Data\ncr√©√©e par\nNicolas Greffard,\nDocteur en Intelligence Artificielle\n, d√©j√† compos√©e de\n20\nData Engineers\net\nDatascientists\ntalentueux üòç\nNous recherchons de\nnouvelles p√©pites\npour rejoindre notre √©quipe de choc et r√©pondre aux\nmultiples probl√©matiques Big Data\nde nos\nclients nantais\nmais √©galement\ncontribuer √† nos projets de R&D\net travailler sur des\nconf√©rences incroyables\n(DevFest, Salon de la Data)\nü§©\nTa future mission si tu l'acceptes\nüòâ\nNous te proposons d'intervenir au sein de nos\ngrandes DSI clientes\n, sur des sujets de\ncollecte\n, d\n'alimentation\net de\ntransformation de donn√©es\nsur un environnement\nBig Data\nApache\n(\nHadoop, Spark, Ambari, Hive\n) sur les technologies suivantes :\nHadoop, Apache Ambari, RabbitMQ, Java, Scala, YarnApplication, Teradata, Squoop, Kudu, Hue, Hive, Impala, Dataiku, Flink, Kafka, Spark, Kibana, Oozie, Git, GitLabCI, Jenkins\n,\nAWS.\nLe job en d√©tail\nü§©\n√âtude, conception et r√©alisation de traitements Big Data ;\n√âchange avec les architectes, les PO et PPO, les d√©veloppeurs et la gouvernance de donn√©es ;\nExploration des donn√©es et des usages des utilisateurs avec Impala ;\nImport de donn√©es (SFTP, Kafka, RabbitMQ) ;\nAlimentation du cluster Hadoop via des composants d√©velopp√© en Java avec le Framework Spark sur IntelliJ ;\nUtilisation d‚ÄôApache Ambari pour g√©rer et surveiller un cluster Hadoop, visualisation des jobs en cours via YarnApplication et des flux Oozie ;\nCollecte des donn√©es depuis Teradata via l‚Äôoutil Sqoop dans une base de donn√©es Hive ;\nTransformation des donn√©es avec Spark (HDFS, Hive, Kafka, Hbase, Phoenix) ;\nUtilisation de Apache Kudu afin d'optimiser les requ√™tes utilisateurs sur les donn√©es chaudes ;\nExposition de donn√©es sur Dataiku pour la cr√©ation de mod√®le de DataScience ;\nR√©alisation en Java ‚Äì Flink pour g√©rer les traitements complexe et volumineux ;\nGestion de configuration sous Git avec GitLab ;\nInt√©gration continue avec Jenkins et Sonar ;\nLecture de fichier parquet depuis un r√©pertoire S3 sous AWS ;\nRequ√™tage de bases de donn√©e depuis l'outil Athena d'AWS ;\nTransformation des donn√©es et calcul d'indicateurs sous Hive ;\nUtilisation de Oozie pour l‚Äôordonnancement de flux ;\nUtilisation de Kibana pour visualiser et mesurer la volum√©trie de traitements quotidien et en streaming.\nPourquoi choisir Valeuriad ?\nüòä\nEn plus d‚Äô√™tre aujourd‚Äôhui un acteur nantais reconnu de l‚Äôexpertise IT, nous nous inscrivons depuis notre cr√©ation dans une d√©marche d'entreprise\nOpale\net\nHolacratique\n, o√π l'ensemble de nos prises de d√©cisions et projets sont r√©alis√©s par et avec l'ensemble de nos\n119 co√©quipiers\nüí™\nRejoindre Valeuriad, c'est\npouvoir s'investir dans la co-construction de l'entreprise\n:\nPar un r√¥le, avec une fiche de poste et un temps d√©di√© (gestionnaire des Ci‚Äôs, porteur des partenariats √©coles, organisateur d‚Äô√©v√©nements, PO des projets internes, gestion de l'Acad√©mie Valeuriad‚Ä¶).\nPar les projets strat√©giques (200 jours mis √† disposition pour les co√©quipiers chaque ann√©e) pour cr√©er et faire grandir des projets structurants (cr√©ation de nouveaux avantages √† l'anciennet√©, cr√©ation d'indicateurs mensuels pour √™tre toujours plus transparents, m√©c√©nat de comp√©tences pour des associations caritatives...).\nPar les projets cagnottes (150‚Ç¨ par co√©quipiers et par an) pour r√©aliser des projets collaboratifs qui te tiennent √† c≈ìur avec d'autres Valeurieux (d√©couverte du c√©cifoot, challenge √©cologique, challenges sportifs pour des dons √† des associations humanitaires, borne photo...).\nPar les ateliers collaboratifs, chaque mois des brainstorming et ateliers de travail sont propos√©s par les diff√©rents porteurs de projets et sont ouverts √† tous les volontaires.\nMais avant-tout nous sommes une\n√©quipe soud√©e\n, des coll√®gues qui appr√©cient\npasser du temps ensemble\nlors de nos\nsoir√©es hebdomadaires\net se cr√©er des\nsouvenirs inoubliables\nü§© C'est pour √ßa que chez Valeuriad, le plus important pour nous reste le\nsavoir-√™tre\n:\ndes passionn√©s, du dynamisme, des sourires, de l'√©coute et le sens de la f√™te\nüòâ\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "R",
                "Java",
                "Scala"
            ],
            "DataBase": [
                "HBase"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Hadoop",
                "Flink",
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git",
                "Jenkins"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Other": [
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Apside",
        "location": "Nantes, Pays de la Loire, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-apside-3909772916?position=9&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=aYYNuTDFLAFR6HvwcvR%2FAg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Offre d'Emploi : DATA ENGINEER H/F chez Apside\nDescription du poste :\nNous sommes √† la recherche d'un Data Engineer passionn√© pour rejoindre notre √©quipe dynamique. Si vous avez une expertise dans le Big Data, la Data Science, l'analyse de donn√©es et l'architecture de donn√©es, cette opportunit√© est faite pour vous. Int√©grer notre communaut√© Data, c‚Äôest l‚Äôassurance de progresser, innover, partager, vous certifier et rendre service √† nos clients.\nVos missions :\nD√©veloppement des jobs Spark pour la collecte et la transformation des donn√©es comptables disponibles dans les bucket S3.\nOptimisation des jobs Spark.\nD√©veloppement des batchs Java et √©criture des donn√©es au formats comptables.\n√âcriture et ordonnancement des DAGs Airflow.\nSupport du d√©veloppement Spark Scala.\nMaintenance applicative.\nProduction des √©v√©nements d√©di√©s √† la plateforme de donn√©es.\n.\nVotre r√¥le, vos comp√©tences :\nVous ma√Ætrisez au minimum un langage de programmation appliqu√© √† l‚Äôanalyse de donn√©es (SQL, Scala, Python, Java).\nVous √™tes passionn√© par le Big Data et le Machine Learning.\nVous concevez et mettez en ≈ìuvre des strat√©gies s√©curis√©es d'acquisition et d'int√©gration de donn√©es.\nVous configurez des r√©f√©rentiels de donn√©es √† la pointe de la technologie dans des environnements distribu√©s, majoritairement dans le cloud (Google Cloud Platform, Azure, AWS) et/ou en environnement Hadoop (distribution MapR, Cloudera, Hortonworks).\nEnvironnement technique :\nSQL\nPython/Spark\nCloud AWS: AWS Glue, AWS Lambda (possibilit√© de vous former sur AWS)\nStockage objet (AWS S3)\nOrchestration et scheduling de t√¢ches (Apache Airflow)\nBases analytiques et bases NoSQL (ElasticSearch, AWS Athena)\nVotre profil :\nFort de 4 ann√©es d‚Äôexp√©rience en Data Engineer/ DATA ANALYST\nTitulaire d‚Äôune formation sup√©rieure IT.\nCapacit√© √† s‚Äôint√©grer dans un cadre technique client tout en √©tant √† m√™me de proposer des pistes d‚Äôam√©liorations pertinentes.\nAutonome dans la gestion des projets.\nCurieux et impliqu√©, vous √™tes bon communicant avec les clients et les acteurs de culture technique diff√©rente.\nDe bonnes raisons de rejoindre Apside ?\nUn esprit start-up avec la stabilit√© d‚Äôun grand groupe, qui favorise l‚Äôagilit√©, le travail d‚Äô√©quipe et la proximit√©. Alors qu‚ÄôApside ne cesse d‚Äôagrandir sa famille d√©j√† forte de plus de 3000 consultants, nous sommes √† la recherche de nos nouveaux talents !\nCDI + package salarial avantageux (Mutuelle offerte, RTT, Tickets Restaurant, Int√©ressement ...)\nParticipez et animez nos soir√©es techniques (Project Lab, Test Lab‚Ä¶),\nDevenez speaker (Devoxx, DevFest, NCraft‚Ä¶),\nFormez vous avec l‚ÄôAcademy By Apside (e-learning, formation, certification).\nD√©veloppez votre r√©seau (Soir√©es trimestrielles, Afterwork, Soir√©es d‚Äôint√©gration‚Ä¶),\nInt√©grez notre Communaut√©s d‚ÄôExperts et testez les derni√®res innovations techniques sur notre Bac √† Sable !\nApside s‚Äôengage en faveur de l‚Äôemploi des personnes en situation de handicap avec sa filiale Apsid‚ÄôEA : 1√®re entreprise adapt√©e totalement int√©gr√©e √† une ESN !\nPour aller plus loin avec APSIDE !\nhttps://www.apside.com/fr/nos-offres-emploi/?_joboffer-agency=17833&_paged=2\nCe poste de DATA ENGINEER est fait pour vous !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "17833",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "Elasticsearch",
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Apache Airflow",
                "Spark"
            ],
            "CloudComputing": [
                "AWS",
                "Google Cloud Platform",
                "Azure"
            ],
            "Automation": [
                "Airflow"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Grenoble, Auvergne-Rh√¥ne-Alpes, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-%E2%80%93-grenoble-at-capgemini-3905836212?position=10&pageNum=10&refId=gpEQfgH%2FUqWr7JKAKdnERQ%3D%3D&trackingId=KDNA8Rx%2B%2FhO7ZsBno6DlMw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise o√π vous serez en mesure de fa√ßonner votre carri√®re selon vos aspirations. Avec le soutien et l'inspiration d'une communaut√© d‚Äôexperts dans le monde entier, vous pourrez r√©√©crire votre futur. Rejoignez-nous pour red√©finir les limites de ce qui est possible, contribuer √† lib√©rer la valeur de la technologie pour les plus grandes organisations et participer √† la construction d‚Äôun monde plus durable et inclusif.\nVos missions :\nEn tant que Data Engineer au sein d'une √©quipe multidisciplinaire, vos responsabilit√©s principales seront les suivantes :\nIntervenir sur les diff√©rentes phases d'un projet dans un environnement Cloud et Agile.\nContribuer √† la gestion de la qualit√© des donn√©es et extraction et analyse de celle-ci, ainsi qu‚Äô√† la pr√©sentation des donn√©es dans leur forme raffin√©e.\nProposer des nouvelles lectures de donn√©es via un travail de fouille sur les gisements d‚Äôinformation, notamment client.\nAdopter une posture de consultant : proposer de nouvelles solutions et accompagner le client dans ses choix.\nVotre profil :\nTitulaire d'un Bac+5 en √©cole d‚Äôing√©nieur ou en universit√©.\nConnaissances approfondies des ETL (Talend, Informatica ou SSIS), du traitement de donn√©es (Spark, Python, Scala) ainsi que des bases de donn√©es (Oracle, SQL Server, Postgres).\nFacult√© pour se montrer curieux, autonome et proactif dans la r√©alisation de ses t√¢ches.\nCapacit√© √† faire preuve de rigueur et √† travailler en √©quipe.\nBon niveau d‚Äôanglais (B2 minimum).\n3 raisons de nous rejoindre :\nQualit√© de vie au travail\n: accord de t√©l√©travail en France et √† l‚Äôinternational, accord sur l‚Äô√©galit√© professionnelle, la parentalit√©, l‚Äô√©quilibre des temps et la mobilit√© durable.\nApprentissage en continu\n: certifications et formations en libre acc√®s, accompagnement sur mesure avec votre career manager, parcours d‚Äôint√©gration sur 9 mois.\nAvantages groupe & CSE\n: plan actionnariat, tarif pr√©f√©rentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorit√©s\n:\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int√©grez un collectif qui valorise la diversit√©, d√©veloppe le potentiel de ses talents, s‚Äôengage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r√©duire son impact environnemental sur tous ses sites et aupr√®s de ses clients.\n√Ä propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr√®s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d‚Äôexp√©rience, nous sommes un partenaire strat√©gique des entreprises pour la transformation de leurs activit√©s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp√©tuelle √©volution tels que le cloud, la data, l‚ÄôIntelligence Artificielle, la connectivit√©, les logiciels, l‚Äôing√©nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5",
            "Experience": "55 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Spark"
            ],
            "DBMS": [
                "SQL Server"
            ],
            "Other": [
                "Cloud"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Groupe INGENA",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-groupe-ingena-3883689479?position=1&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=9Wc3V%2BMHM4IlRePP6%2BcZVA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Le groupe INGENA promeut la transition num√©rique en √©tant acteur d‚Äôun monde souhaitable.\nVotre mission :\nConcevoir, d√©velopper et tester des algorithmes de collecte et de traitement de gros volumes de donn√©es sous Scala, Python ou Java\nAutomatiser et optimiser les flux de donn√©es et leurs visualisations en dashboards\nIndustrialiser les traitements, la qualit√© et l‚Äôint√©grit√© des donn√©es\nParticiper √† la Mod√©lisation et √† la Gouvernance des donn√©es (process, normalisation, r√©f√©rentiel,‚Ä¶)\nContribuer √† la scalabilit√©, la s√©curit√©, la stabilit√© et la disponibilit√© des donn√©es de la plateforme\nAnalyser les donn√©es pour r√©pondre aux questions m√©tiers et participer √† l‚Äô√©volution de l‚Äôarchitecture Big Data\nConcevoir, D√©velopper et Industrialiser des mod√®les de Machine Learning, Deep Learning, en collaboration avec les Data Scientists\nAppliquer une d√©marche CI/CD (Git, Jira, Jenkins)\nLes comp√©tences techniques n√©cessaires sont :\nExp√©rience de 5 ans minimum en d√©veloppements Scala, Python ou Java\nExp√©rience de 2 ans minimum sur SPARK et sur le traitement des flux en streaming\nExpertise sur Hadoop (Hive, HBase, HDFS) sous distributions MapR ou Hortonworks\nExp√©rience souhait√©e sur ELK, Terraform, NoSQL,‚Ä¶\nFort background en Mod√©lisation de donn√©es ou ETL\nMa√Ætrise des briques analytiques des clouds AWS, GCP ou Azure\nSensibilisation √† la d√©marche CI/CD tools (Git, Jenkins)\nLa connaissance de Docker, Kubernetes et Ansible est un plus\nMise en ≈ìuvre des m√©thodes Agile (Scrum, Kanban,‚Ä¶)\nAnglais souhait√©\nGroupe INGENA\n:\nLe Groupe INGENA est sp√©cialis√© en Conseil M√©tier et en Int√©gration pour les march√©s de l‚Äôassurance, de la banque et de la Finance. INGENA intervient notamment sur les projets associ√©s √† la Data, aux Risques et √† la Distribution.\nLe groupe comprend √©galement la soci√©t√© DRiMS sp√©cialis√©e en Finance de March√©.\nNos valeurs : Engagement, Int√©grit√© et Bienveillance.\nLa mise en pratique du monde souhaitable, c‚Äôest pour nous une entreprise √©co-responsable, √©thique, inclusive, sociale, soucieuse du bien-√™tre, de l‚Äô√©volution et de l‚Äô√©panouissement de ses √©quipes. Ce sont aussi des offres pour un monde durable comme la ma√Ætrise des risques ou l‚ÄôESG.\nDans un esprit convivial et engag√©, nous faisons en sorte que chacun puisse √™tre acteur de l‚ÄôINGENA souhaitable.\nBureau √† Paris 9√®me (M√©tro Le Peletier). Clients √† Paris ou tr√®s proche banlieue.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Scala",
                "Python"
            ],
            "DataBase": [
                "HBase",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker",
                "Jenkins"
            ],
            "SoftBigDataProcessing": [
                "HBase"
            ],
            "Automation": [
                "Kubernetes",
                "Ansible"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Machine Learning",
                "Big Data",
                "CI/CD"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Pathway",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-stream-data-processing-distributed-data-processing-at-pathway-3887662141?position=2&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=ikwLbQi4nhCOigonivq9UA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "About Pathway\nDeeptech start-up, founded in March 2020.\nOur primary developer offering is an ultra-performant Data Processing Framework (unified streaming + batch) with a Python API, distributed Rust engine, and capabilities for data source integration & transformation at scale (Kafka, S3, databases/CDC,...)\nThe single-machine version is provided on a free-to-use license (`pip install pathway`)\nMajor data use cases are around event-stream data (including real-world data such as IoT), and graph data that changes over time\nOur enterprise offering is currently used by leaders of the logistics industry, such as DB Schenker or La Poste, and tested across multiple industries. Pathway has been featured in Gartner's market guide for Event Stream Processing\nLearn more at http://pathway.com/ and https://github.com/pathwaycom/\nPathway is VC-funded, with amazing BAs from the AI space and industry. We have operations across Europe and in the US. We are headquartered in Paris, with significant support from the French ecosystem (BPI, Agoranov, WILCO,...).\nThe Team\nPathway is built by and for overachievers. Its co-founders and employees have worked in the best AI labs in the world (Microsoft Research, Google Brain, ETH Zurich), worked at Google, and graduated from top universities (Polytechnique, ENSAE, Sciences Po, HEC Paris, PhD obtained at the age of 20, etc...). Pathway's CTO is a co-author with Goeff Hinton and Yoshua Bengio. The management team also includes the co-founder of Spoj.com (1M+ developer users) and NK.pl (13.5M+ users) and experienced growth leader who has scaled companies with multiple exits.\nThe opportunity\nWe are searching for a person with a Data Processing or Data Engineering profile, willing to work with live client datasets, and to test, benchmark, and showcase our brand-new stream data processing technology.\nThe end-user of our product are mostly developers and data engineers working in a corporate environment. Our development framework is one day expected to become for them a part of their preferred development stack for analytics projects at work - their daily bread & butter.\nYou Will\nYou will be working closely with our CTO, Head of Product, as well as key developers. You will be expected to:\nImplement the flow of data from their location in client's warehouses up to Pathway's ingress\nSet up CDC interfaces for change streams between client data stores and i/o data processed by Pathway; ensuring data persistence for Pathway outputs\nDesign ETL pipelines within Pathway\nContribute to benchmark framework design (throughput / latency / memory footprint; consistency), including in a distributed system setup.\nContribute to building open-source test frameworks for simulated streaming data scenarios on public datasets\nRequirements\nInside-out understanding of at least one major distributed data processing framework (Spark, Dask, Ray,...)\n6 months+ experience working with a streaming dataflow framework (e.g.: Flink, Kafka Streams or ksqldb, Spark in streaming mode, Beam/Dataflow)\nAbility to set up distributed dataflows independently\nExperience with data streams: message queues, message brokers (Kafka), CDC\nWorking familiarity with data schema and schema versioning concepts; Avro, Protobuf, or others\nFamiliarities with Kubernetes\nFamiliarity with deployments in both Azure and AWS clouds\nGood working knowledge of Python\nGood working knowledge of SQL\nExperienced in working for an innovative tech company (SaaS, IT infrastructure or similar preferred), with a long-term vision\nWarmly disposed towards open-source and open-core software, but pragmatic about licensing\nBonus Points\nKnow the ways of developers in a corporate environment\nPassionate about trends in data\nProficiency in Rust\nExperience with Machine Learning pipelines or MLOps\nFamiliarity with any modern data transformation workflow tooling (dbt, Airflow, Dagster, Prefect,...)\nFamiliarity with Databricks Data Lakehouse architecture\nFamiliarity with Snowflake's data product vision (2022+)\nExperience in a startup environment\nBenefits\nWhy You Should Apply\nIntellectually stimulating work environment. Be a pioneer: you get to work with a new type of stream processing framework\nWork in one of the hottest data startups in France, with exciting career prospects\nResponsibilities and ability to make significant contribution to the company' success\nCompensation: annual salary of ‚Ç¨60K-‚Ç¨100K + Employee stock option plan.\nInclusive workplace culture\nFurther details\nType of contract: Permanent employment contract\nPreferable joining date: early 2023\nCompensation: annual salary of ‚Ç¨60K-‚Ç¨100K + Employee stock option plan\nLocation: Remote work from home. Possibility to work or meet with other team members in one of our offices:\nParis - Agoranov (where Doctolib, Alan, and Criteo were born) near Saint-Placide Metro (75006)\nParis Area - Drahi X-Novation Center, Ecole Polytechnique, Palaiseau\nWroclaw - University area\nCandidates based anywhere in the EU, United States, and Canada will be considered.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "60K",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Databricks",
                "Flink",
                "Spark"
            ],
            "DataSerialization": [
                "Avro"
            ],
            "CloudComputing": [
                "AWS",
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Machine Learning"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ALFI : Financial Markets Consultancy Services",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-alfi-financial-markets-consultancy-services-3916559634?position=3&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=gTjggiRztds529DCE7H89w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "ALFI est une soci√©t√© de conseil et services sp√©cialis√©e en syst√®mes d‚Äôinformation. Depuis plus de 20 ans, ALFI est un acteur unique qui m√™le technologie et humain pour accompagner les transformations num√©riques sur les march√©s de l‚ÄôAsset Management, la banque d‚ÄôInvestissement et les Services aux Investisseurs.\nAvec plus de 46 r√©f√©rencements de rang un, ALFI est reconnu comme incontournable sur le secteur BFA. Nous avons plus de 35 clients grands comptes actifs tels que HSBC, Soci√©t√© G√©n√©rale, BNP Paribas, Cr√©dit Agricole, Axa‚Ä¶\nDepuis 2015, ALFI a int√©gr√© le groupe\nMoOngy\n, qui compte plus de 6000 salari√©s dans toute l‚ÄôEurope\nMissions :\nPour l'un de nos clients grands comptes, nous vous proposons d'intervenir sur une fonction de Consultant Data engineer.\nLes principales missions sont :\nComprendre les besoins des utilisateurs et les traduire de mani√®re analytique\nD√©veloppement de solutions permettant de traiter des volumes importants de donn√©es\nConception, collection et fabrication des donn√©es brutes\nD√©velopper des algorithmes permettant de r√©pondre aux probl√®mes pos√©s et veiller √† leur industrialisation\nS√©curisation des Pipelines donn√©es pour les Data Scientists et les Data Analysts\nConstruire des bases de donn√©es robustes\nOrganisation de l‚Äôarchitecture du cloud\nProfil recherch√© :\nVous √™tes issu d'une formation Bac +5 Ecole scientifique ou informatique.\nVous disposez d'une premi√®re exp√©rience en d√©veloppement et dans la data.\nVous disposez d'un niveau d'anglais op√©rationnel.\nJava, Python, C++\nSQL\nDevops (Jenkins, Kubernetes, Docker)\nConform√©ment √† la r√®glementation, et √† notre politique d‚Äô√©galit√© professionnelle, tous nos postes sont ouverts aux personnes en situation de handicap;\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "20 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DevTools": [
                "Jenkins",
                "Docker"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "MindPal",
        "location": "Marseille, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-mindpal-3896991761?position=4&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=mK%2Fm3m7kwlYQFk2YmDT9sA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "We are looking for\nData Engineer!\nResponsibilities\nDesigning, creating, and maintaining data processing systems\nAnalyzing and optimizing data processing workflows\nCollaborating with the team to ensure data quality and efficiency\nTesting and implementing new solutions\nRequirements\nAt least 2 years of experience in designing and creating data processing systems\nProficiency in tools and programming languages related to data engineering (e.g. Hadoop, Spark, Scala, Python)\nExcellent knowledge of databases and SQL language\nAbility to work in a team and communicate effectively with other departments\nCommunicative English skills\nExperience with AWS/AWS Glue is a plus\nWe Offer\nB2B contract\nFull-time job\nRemote work and flexible hours\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Full"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "AWS"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "METEOJOB by CleverConnect",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-meteojob-by-cleverconnect-3895135654?position=5&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=G%2BraCwgpd5v46E1jI26QuA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nQui sommes-nous ?\nNous sommes passionn√©s par les nouvelles technologies, et vous ?\nRejoindre Amiltone, c'est int√©grer des √©quipes dynamiques et soud√©es dans le cadre de projets novateurs et ambitieux. Nous relevons les challenges techniques de nos clients et les accompagnons dans leur transformation digitale.\nPourquoi choisir Amiltone‚ÄØ?\nAmiltone, plus qu'une entreprise, un √©tat d'esprit !\nNotre objectif ? Votre √©panouissement professionnel !\nNous Avons √† C≈ìur De\nVous accompagner au mieux au travers d'un suivi personnalis√©\nVous faire monter en comp√©tences en vous proposant des formations tout au long de votre carri√®re\nComprendre vos besoins et respecter nos engagements\nVous proposer des missions de qualit√© avec des technologies innovantes\nCultiver votre potentiel gr√¢ce √† notre programme de d√©veloppement personnel Addvise\nVotre bien-√™tre passe aussi par des activit√©s extraprofessionnelles, c'est pourquoi nous vous proposons des s√©ances sportives anim√©es par nos coachs, soir√©es pour se retrouver et animations (√† l'agence ou en visio), Gaming nights...\nDescription Du Poste\nLes missions d'un Amiltonien :\nEn tant que Data Engineer\n(H/F)\n, vous serez en charge des missions suivantes :\nConcevoir et d√©velopper les futures fonctionnalit√©s de la plateforme Big Data sous Google Cloud Platform.\nConcevoir les flux d'alimentation et les tables (structure de donn√©e).\nAutomatiser et industrialiser les flux.\nAssurer le run applicatif, le cas √©ch√©ant.\nLa Stack Technique\nMa√Ætrise des langages suivants : SQL, Talend, BigQuery\nConnaissances de Google (GCP)\nNotion de programmation fonctionnelle\nDescription Du Profil\nLe profil d'un Amiltonien :\nDipl√¥m√© Bac+4/5 (Ecole d'ing√©nieur/Master), vous disposez de 2 ann√©es d'exp√©rience dans le d√©veloppement de data.\nToujours sur le qui-vive des nouveaut√©s technologiques, vous √™tes force de proposition sur des technos, des outils ou des process qui permettent d'am√©liorer la qualit√© du code et la stabilit√© de nos applications.\nOutre vos comp√©tences techniques, nous nous int√©ressons √©galement √† votre potentiel et votre motivation.\nNos postes sont ouverts aux personnes en situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Capgemini",
        "location": "Valbonne, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/senior-data-engineer-valbonne-at-capgemini-3888072146?position=6&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=MuhVomcbD2tDd3DYDYZk7w%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Choisir Capgemini, c'est choisir une entreprise o√π vous serez en mesure de fa√ßonner votre carri√®re selon vos aspirations. Avec le soutien et l'inspiration d'une communaut√© d‚Äôexperts dans le monde entier, vous pourrez r√©√©crire votre futur. Rejoignez nous pour red√©finir les limites de ce qui est possible, contribuer √† lib√©rer la valeur de la technologie pour les plus grandes organisations et participer √† la construction d‚Äôun monde plus durable et inclusif.\nVos missions :\nEn tant que Senior Data Engineer au sein d'une √©quipe multidisciplinaire, vos responsabilit√©s principales seront les suivantes :\nParticiper √† des ateliers clients.\nAcqu√©rir des donn√©es et optimiser le stockage.\nCr√©er de flux de donn√©es optimis√©s et √©laborer des algorithmes de transformation.\nTraiter et analyser pour la visualisation et le machine learning.\nEncadrer des ing√©nieurs juniors et contribuer √† la communaut√© Data.\nVotre profil :\nVous poss√©dez un dipl√¥me d'ing√©nieur informatique et/ou Master avec une sp√©cialit√© data.\nVous parlez couramment fran√ßais et anglais.\nVous poss√©dez au minimum 6 ans d'exp√©rience sur un r√¥le similaire.\nVous ma√Ætrisez les outils Spark, Python, Scala ainsi qu'une bonne compr√©hension des syst√®mes d'extraction, de transformation et de changement (ETL).\nVous avez un certain leadership et un esprit d'√©quipe, id√©alement dans un cadre agile.\n3 raisons de nous rejoindre :\nQualit√© de vie au travail :\naccord de t√©l√©travail en France et √† l'international, accord sur l'√©galit√© professionnelle, la parentalit√©, l'√©quilibre des temps et la mobilit√© durable.\nApprentissage en continu\n: certifications et formations en libre acc√®s, accompagnement sur mesure avec votre carrer manager, parcours d'int√©gration sur 9 mois.\nAvantages groupe & CSE :\nplan actionnariat, activit√©s √† tarifs pr√©f√©rentiels, remboursement partiel vacances, remboursement de votre abonnement sportif ou culturel.\nNos engagements et priorit√©s :\nLe groupe Capgemini encourage une culture inclusive dans un cadre multiculturel et handi-accueillant. En nous rejoignant, vous int√©grez un collectif qui valorise la diversit√©, d√©veloppe le potentiel de ses talents, s'engage dans des initiatives solidaires avec ses partenaires, et se mobilise pour r√©duire son impact environnemental sur tous ses sites et aupr√®s de ses clients.\nA Propos de Capgemini :\nCapgemini est un leader mondial, responsable et multiculturel, regroupant pr√®s de 350 000 personnes dans plus de 50 pays. Fort de 55 ans d‚Äôexp√©rience, nous sommes un partenaire strat√©gique des entreprises pour la transformation de leurs activit√©s en tirant profit de toute la puissance de la technologie et des innovations dans les domaines en perp√©tuelle √©volution tels que le cloud, la data, l‚ÄôIntelligence Artificielle, la connectivit√©, les logiciels, l‚Äôing√©nierie digitale ou les plateformes.\nGet The Future You Want* | www.capgemini.com/fr-fr\n*Capgemini, le futur que vous voulez\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Junior",
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "6 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "BigData": [
                "Spark"
            ],
            "Other": [
                "Machine Learning",
                "Cloud"
            ],
            "EnSoftSkils": [
                "Leadership"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "METEOJOB by CleverConnect",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/alternance-data-engineer-lille-f-h-at-meteojob-by-cleverconnect-3901971331?position=7&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=5Xzo%2B%2FEkXS0PjKMcS6ocHQ%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nL'ISCOD, sp√©cialiste de la formation en Digital Learning, recherche pour son entreprise partenaire, une ESN agile et un groupe international, son Data Engineerq, en contrat d'apprentissage, pour pr√©parer l'une de nos formations dipl√¥mantes reconnues par l'Etat de niveau 5 √† niveau 7 (Bac+2, Bachelor/Bac+3 ou Mast√®re/Bac+5).\nOptez pour l'alternance nouvelle g√©n√©ration avec l'ISCOD !\nDescription Du Poste\nDurant cette alternance, tu auras l'opportunit√© de :\nCollaborer avec les √©quipes pour d√©finir les besoins ;\nOrganiser et traiter le flux de donn√©es quotidien ;\nEffectuer la visualisation des donn√©es ;\nOrganiser, synth√©tiser l'information\nLivrer les r√©sultats (rapports, pr√©sentations, tableaux de bord...) ;\nDescription Du Profil\n√âtudiant en derni√®re ann√©e, tu suis une formation ax√©e sur le Data Engineering et tu recherches une alternance.\nTu as une grande app√©tence pour le domaine de la Data et tu ma√Ætrises un des langages de programmation suivants: SQL, Python, Java ‚Ä¶\nTu connais le fonctionnement des ETL et les outils de visualisation, notamment Power BI et des gestion de donn√©es, notamment de MS Excel.Poste bas√© √† LilleR√©mun√©ration selon niveau d'√©tudes + √¢ge\nFormation prise en charge √† 100% par l'entreprise\nCe poste vous int√©resse ? Envoyez vite votre candidature !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "Bac+5, Bac+3",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataVisualisation": [
                "Power BI"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "METEOJOB by CleverConnect",
        "location": "Pont-√†-Mousson, Grand Est, France",
        "link": "https://fr.linkedin.com/jobs/view/alternant-data-engineer-h-f-at-meteojob-by-cleverconnect-3868016363?position=8&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=hZRnMmL0q03FEFk%2BycezAg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Entreprise\nSaint-Gobain\ncon√ßoit, produit et distribue des mat√©riaux et des solutions pens√©s pour le bien-√™tre de chacun et l'avenir de tous. Rejoignez une communaut√© innovante, passionn√©e et entreprenante pour am√©liorer le monde de demain.\nActeur mondial de r√©f√©rence et leader europ√©en de solutions compl√®tes de canalisation en fonte ductile,\nSaint-Gobain PAM Canalisation\ncon√ßoit, produit et commercialise un √©ventail complet de solutions d√©di√©es au\ntransport de l'eau\n.\nSaint-Gobain PAM Canalisation dispose dans la r√©gion Grand-Est, √† Maidieres, d'un\nCentre de Recherche\nunique dont les comp√©tences et le savoir-faire contribuent √† apporter √† nos clients des\nsolutions innovantes\net √† valeur ajout√©e dans le\ncontr√¥le et la gestion patrimoniale de l'eau.\nDescription Du Poste\nL'entreprise Saint-Gobain PAM Canalisation, sp√©cialis√©e dans le d√©veloppement, la production et la vente d'√©quipements de canalisation (tuyaux, robinetteries et regards de voirie) recrute pour son Si√®ge √† PONT-A-MOUSSON :\nUN ALTERNANT DATA ENGINEER (H/F)\nInt√©gr√©.e Aux √âquipes De La Direction Des Syst√®mes D'Information PAM DIGITAL & IT Et Rattach√©.e √† Son Domaine DIGITAL TECHNOLOGIES, Vous Serez Notamment Amen√©(e) √†\nIdentifier les sources des donn√©es et mettre en place leur int√©gration dans SnowFlake via notre Cloud AZURE,\nConcevoir et mod√©liser des datawarehouses et des data hubs en fonction des Use Cases sur lesquels vous travaillerez,\nRestituer des donn√©es par visualisation (avec PowerBI) ou APIsation (avec Microsoft Data Factory),\nR√©aliser des projets sur des technologies innovantes,\nMonitorer et maintenir des plateformes d'√©changes h√©bergeant ces flux.\nContrat d'alternance\nLe poste est bas√© √† Pont-√†-Mousson ( √† 30km de Nancy et Metz )\nDescription Du Profil\nVous pr√©parez un Master (Bac+ 5),\nVous √™tes passionn√© par le Big data, la Business intelligence, la Valorisation de donn√©es,\nVous avez des bases concernant les langages : Python , SQL et Java (Spring Batch) ainsi que les bases de donn√©es SQL,\nVous connaissez Snowflake,\nVous avez envie de faire de la DataViz avec PowerBI.\nVous √™tes reconnu¬∑e par vos coll√®gues pour :\nVotre aptitude √† aller vers les autres, communiquer et √©couter\nVotre capacit√© √† travailler en √©quipe, dans un environnement international\nVotre rigueur, et votre autonomie.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Other": [
                "Cloud",
                "Big Data"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Vestiaire Collective",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/analytics-engineer-at-vestiaire-collective-3875996826?position=9&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=aKEX6pSYETA3IWaU83cDlg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Vestiaire Collective is the leading global online marketplace for desirable pre-loved fashion. Our mission is to transform the fashion industry for a more sustainable future by empowering our community to promote the circular fashion movement. Vestiaire was founded in 2009 and is headquartered in Paris with offices in London, Berlin, New York, Singapore, Ho Chi Minh and Hong Kong and warehouses in Tourcoing (France), Crawley (UK), Hong Kong and New York.\nWe currently have a diverse global team of 700 employees representing more than 50 nationalities. Our values are Activism, Transparency, Dedication and Greatness and Collective. We are proud to be a BCorp.\nAbout The Role\nThis role is central to our data strategy and requires a balance of technical expertise and business acumen. As a Junior Analytics Engineer, you will be at the heart of our data-driven initiatives, working closely with cross-functional teams to transform raw data into a single source of truth data mart. Your work will directly influence key decisions in finance, payment systems and business performance.\nWhat You'll Do\nDesign, implement, and maintain efficient and reliable data pipelines using a modern data stack: Airflow, Snowflake, DBT\nDevelop advanced data models to support complex analytics, including financial reconciliations, cost effectiveness and profitability models. Collaborate with finance, payments and tech teams to understand their data requirements and translate these into sophisticated technical solutions\nEnsure scalability and performance of our data infrastructure to handle large-scale, multi-faceted data sets from diverse sources\nImplement and maintain data quality checks and monitoring systems for accuracy and consistency\nInnovate and integrate new technologies and methodologies to enhance data capabilities across finance domains\nAssist the finance team in building key dashboards in Tableau to enable data driven decision making\nWho You Are\nRequired Qualifications:\nBachelor‚Äôs/Master‚Äôs in Computer Science, Engineering, Statistics, or related field\nAt least one previous experience in analytics engineering, with strong skills in ETL and data modeling, an awareness of data warehousing and dataOps practices\nProficient in SQL and programming languages like Python or R\nExperience with cloud data technologies and big data tools\nDesirable Skills:\nApache Airflow: an understanding of workflow management\nGit: Solid knowledge in version control and CI/CD integration\nCloud Service: AWS, Snowflake or similar cloud experience\nData Visualization Tools: Proficiency in tools like Tableau, Looker, Snowsight\nPrevious experience in DBT for data modeling\nWhat we offer\nüéÅ\nA meaningful job with an impact on the way people consume fashion and promote sustainability\nFlexible work possibilities\nThe opportunity to do career-defining work in a fast-growing French-born scale up\nThe possibility to work as part of a globally diverse team with more than 50 nationalities\nTwo days to help Project - reinforcing your activist journey and volunteer for an association\nSignificant investment in your learning and growth\nCompetitive Compensation And Benefits Package\nAs full member of our entrepreneurial project, you will be eligible to free shares\nVestiaire Collective is an equal opportunities employer\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Full",
                "Junior"
            ],
            "TypeContract": "",
            "Salary": "Package",
            "Level": "",
            "Experience": null
        },
        "title": "Other",
        "skills": {
            "ProgLanguage": [
                "R",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "R"
            ],
            "BigData": [
                "Apache Airflow"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "Snowflake"
            ],
            "Automation": [
                "Airflow"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "CI/CD"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Harnham",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-harnham-3860004924?position=10&pageNum=12&refId=apDik61Q8UOc4buF%2Fpy6Ew%3D%3D&trackingId=8Jke7%2FnB8H6WLoCi6KALqw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Data engineer\nMinimum 2 ans\nParis\n2j tt\nCDI\nUP TO 55k‚Ç¨\nRejoignez une √©quipe dynamique au c≈ìur de l'innovation dans le domaine du retail !\nStack technique : Python, Pandas, SQL, Tableau\nVous utiliserez votre connaissance du d√©veloppement et de la testabilit√© pour am√©liorer la conception, promouvoir des d√©cisions d'ing√©nierie pr√©cises et mettre en ≈ìuvre des strat√©gies de pr√©vention des bogues √† la fois √©volutives et maintenables dans ce ensemble de responsabilit√©s cl√©s :\nCoop√©rer avec divers d√©partements tels que la Recherche et le D√©veloppement, l'Infrastructure, l'Ing√©nierie et le Backend pour garantir la qualit√© et la ponctualit√© des livraisons de produits.\n√âlaborer et mettre en ≈ìuvre des sc√©narios de test, √† la fois manuels et automatis√©s, pour les applications logicielles afin d'assurer la fiabilit√© des pipelines de donn√©es, des processus ETL et des transformations de donn√©es.\nConcevoir et automatiser des tableaux de bord de qualit√© pour surveiller en continu la qualit√© des donn√©es.\nEffectuer des tests fonctionnels, d'int√©gration, de r√©gression et de performance des syst√®mes de bases de donn√©es en utilisant des technologies standard de l'industrie telles que SQL, Python, etc.\nCr√©er et maintenir la documentation d√©taill√©e des plans de test, des cas de test et des r√©sultats des tests.\nContribuer activement au succ√®s du d√©ploiement europ√©en en assurant la fiabilit√© et la qualit√© des produits livr√©s.\nVotre profil :\nSolide exp√©rience dans le d√©veloppement et les tests de logiciels, avec une compr√©hension approfondie des pipelines de donn√©es, des processus ETL et des transformations de donn√©es.\nMa√Ætrise des technologies standard de l'industrie telles que SQL, Python, etc., avec une capacit√© av√©r√©e √† √©laborer et ex√©cuter des cas de test manuels et automatis√©s.\nComp√©tences avanc√©es en mati√®re de surveillance et de garantie de la qualit√© des donn√©es, y compris la cr√©ation et l'automatisation de tableaux de bord de qualit√©.\nCapacit√© √† travailler efficacement en collaboration avec diverses √©quipes, y compris la Recherche et le D√©veloppement, l'Infrastructure et le Backend, pour garantir la qualit√© et la ponctualit√© des livraisons de produits.\nExcellentes comp√©tences en communication et en documentation, avec une capacit√© √† maintenir des rapports d√©taill√©s des plans de test, des cas de test et des r√©sultats des tests.\nInt√©ress√©(e) ? Postulez !\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DataAnalytics": [
                "Pandas"
            ],
            "DataVisualisation": [
                "Tableau"
            ],
            "EnSoftSkils": [
                "Communication",
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Talent-R",
        "location": "Paris",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-ml-engineer-up-to-75k-boulogne-at-talent-r-3904965048?position=1&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=goHKTsw2R1FcHVw1qNEXpg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üìç\nLocalisation\n: R√©gion Parisienne & Remote Flexible (60%) - CDI\nüîç\nSeniorit√©\n: Mid/Senior (>4/5 years of Data)\nüí∞\nSalaire\n: Up to 75K‚Ç¨ fixe + Prime de participation, prime vacances et bonus...\nL'entreprise\nüíº\nLe groupe entre dans une nouvelle √®re gr√¢ce √† la strat√©gie qui place\nL‚ÄôIA\nau c≈ìur du business. Acteur incontournable de ce nouveau cycle ils participent activement √† relever les challenges des\nnouvelles mobilit√©s et de l‚Äôindustrie 4.0.\nP√¥le Architecture et Data :\nL'objectif est de mettre en place les bases de\nla plateforme IA\nafin de r√©pondre aux nouveaux besoins m√©tiers.\nLes missions\n‚öôÔ∏è\nDans ce r√¥le, vous travaillerez en √©troite collaboration avec les √©quipes m√©tiers et les autres membres du P√¥le Architecture & Data (Data Analysts, Scientists, architectes, etc.), en exploitant des quantit√©s massives de donn√©es (flux d'√©v√©nements en continu, traitements par lots et en temps r√©el, ainsi que les appels aux APIs).\nL'objectif est notamment d'alimenter des mod√®les d'apprentissage automatique pour des t√¢ches telles que la segmentation des clients et la d√©tection automatique des pannes des v√©hicules.\nLes avantages\nüòç\nVariable de 6% (Objectifs individuels / Performance du groupe)\nPrime int√©ressement (√† peu pr√®s un mois de salaire)\nPrime vacances (1% de salaire annuel)\nTarif pr√©f√©rentiels achats de v√©hicules\nAvantage CE (200 - 400‚Ç¨ de ch√®ques cadeaux)\nT√©l√© travail : 3 jours / semaine\nMat√©riel IT + participation frais d‚Äôinternet\n10 jours de RTT\nLe stack technique\nüëâüèª\nGoogle Cloud Platform (BigQuery, PubSub, Dataflow, Vertex AI)\nAirflow\nTerraform\nPython\nLooker\nDataiku\nKubernetes, SQL, Git\nPostulez si et seulement si\nVous disposez d'au moins\n5\nans\nd‚Äôexp√©rience en data\nVous disposez d‚Äôune solide exp√©rience en d√©veloppement\nPython\net\nframework ML\n(Vertex, Tensorflow, Scikit, PyTorch‚Ä¶)\nVous poss√©dez une exp√©rience de d√©veloppement et orchestration de chaines ETL complexes via\nAirflow\nou √©quivalent\nVous savez utiliser des services cloud (pr√©f√©rablement\nGCP\n)\nVous √™tes capable d‚Äô√©changer en\nanglais\ntechnique √©crit et oral\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote",
                "Senior"
            ],
            "TypeContract": "CDI",
            "Salary": "75K",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "MachineLearning": [
                "PyTorch",
                "TensorFlow"
            ],
            "CloudComputing": [
                "GCP",
                "Google Cloud Platform"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Automation": [
                "Airflow",
                "Kubernetes"
            ],
            "InfrastructureAsCode": [
                "Terraform"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "StackEase",
        "location": "Marseille, Provence-Alpes-C√¥te d'Azur, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-stackease-3906618983?position=2&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=CTOOVDi7B%2FDx%2BYw6tjYbRA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Context :\nIt is challenging in many ways to develop a net zero electricity grid. Adding renewable energies means more need for storage and for grid balancing. Grid-scale batteries handle these issues but their operation is not quite simple.\nA battery has access to many different revenue streams, each being specific. No revenue alone is enough to make it economically viable. All revenues stacked and optimized together ensure profitability in the long run and a fast deployment.\nStackEase‚Äôs ambition is to develop optimization and trading algorithms that merge the battery revenue streams and ensure their sustainable development. The final tool will be an autopilot that makes the best decisions in real time, controls the battery accordingly and sends the corresponding orders to¬† the market.\nAbout StackEase:\nStackEase is a deeptech spinoff from the INRIA (French Institute for Research in Computer Science). It was created in August 2023 and secured its first fundings. Members are located in Marseille and Paris.\nOur values are innovation, customer satisfaction, merit and sustainability. The company's purpose is to leverage Machine Learning and Mathematical Optimization to accelerate the energy transition.\nMissions :\nDefine and develop the backend architecture of StackEase\nSet up databases and data pipelines collecting battery and market data\nDeploy and maintain optimisation algorithms and forecasts\nDevelop a robust and scalable SaaS platform for 24/7 battery management with high cyber standards\nParticipate in the UI/UX product definition\nSkills Wishlist :\nScientific BS/MS/PhD with 2+ years of experience in software engineering\nExperience with the common backend tools: Python, Git, Kubernetes, SQL/NoSQL ‚Ä¶ Knowledge of the AWS environment is a plus\nEnthusiastic, rigorous, autonomous and willing to be involved in major technical decisions\nKnowledge/Interest in the energy sector and ancillary services\nCompensation :\n45k‚Ç¨ - 60k‚Ç¨ salary range (incl. healthcare, unemployment rate, vacations, ‚Ä¶)\nFlexible remote work policies\nYou do not need to meet 100% of the requirements to apply, we will study all applications: please send your resume to jobs@stackease.fr. References and a cover letter are also welcome but not mandatory.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "100",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL",
                "NoSQL"
            ],
            "CloudComputing": [
                "AWS"
            ],
            "DevTools": [
                "Git"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Other": [
                "Machine Learning"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Kyriba",
        "location": "St.-Cloud, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-intern-at-kyriba-3886667187?position=3&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=hDnXaxqe0%2FKcc1Uyp5ctjw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "It's fun to work in a company where people truly BELIEVE in what they're doing!\nWe're committed to bringing passion and customer focus to the business.\nKyriba is the global leader in cloud-based Enterprise Liquidity Management management solutions, delivering Software-as-a-Service (SaaS) financial technology to corporate CFOs and Treasurers. More than 2,600 global organizations (including Spotify, Ripple, Adecco, Auchan, Adobe, EuropCar, Eurostar International, Expedia, Electronic Arts and Takeda) use Kyriba to enhance their global cash visibility, improve financial controls, and increase productivity across their cash and liquidity, payments, supply chain finance and risk management operations\n.\nBeing an intern at Kyriba means more than simply getting involved in the day-to-day operations of the company. It is an opportunity to introduce you to your potential future work environment so that you can better decide if it meets your career goals\nWe are looking for an enthusiastic\nData Engineer Intern\nto join our Machine Learning Platform team, which is part of our Engineering Department. The main goal of this internship is to participate in building and delivery of Kyriba ML Platform. For this internship, you are going to work with the engineering manager, senior developers of your team, and also QA team.\nKeywords: Data, Python, Java, SQL, Git, Machine Learning (ML), Containers, Orchestration, Kubernetes.\nRequirements\nSolid understanding of basics in Python and Java programming languages\nKnowledge of SQL\nIntermediate (at least) level of English\nAbility to work in team\nMotivation to learn new technologies and tools\nKnowing ML fundamentals will be a plus but is not strictly required\nUnderstanding and basic knowledge of Cloud and Cloud Platforms is also a plus\nResponsibilities\nWork as a part of our development team to build and deliver the Kyriba ML Platform\nFocus on different aspects of the software development and operation lifecycle (designing, coding, test coverage, etc)\nParticipate in all the development-related ceremonies (Pull Requests review, SCRUM activities, grooming sessions, etc)\nLearning Opportunities\nHands-on experience in designing and building cloud-native solutions using microservices-based architecture\nDiscover the world of Machine Learning from the developer‚Äôs perspective and exposure to industry-leading tools and technologies in this domain\nPossibility to deep dive in containers‚Äô orchestration\nInsight into the intricacies of the Treasury Management, specifically in a Working Capital business domain\nBenefits\nMentorship from experienced professionals in the area\nPractical experience within an international SaaS provider leveraging public cloud solutions\nGain practical experience in utilizing Jira for project management\nFlexibility in work hours to accommodate academic commitments\nNetworking opportunities within the organization\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Senior"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DevTools": [
                "Git"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Kubernetes"
            ],
            "Collaboration": [
                "JIRA"
            ],
            "Other": [
                "Machine Learning",
                "Cloud",
                "ML"
            ],
            "EnSoftSkils": [
                "Flexibility",
                "Organization"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Listen too",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-bi-at-listen-too-3903073048?position=4&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=Rx7XexD69MeYfSqLD3%2ByeA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "üì¢ Imaginez un lieu o√π votre voix compte autant que vos comp√©tences‚Ä¶\nChez Listen too, nous sommes convaincus que l‚Äô√©coute est indissociable du progr√®s. Cultiv√©e au quotidien, elle devient un catalyseur de croissance et de r√©ussite, tant pour l‚Äôentreprise que pour ses collaborateur¬∑rice¬∑s.\nListen too est une agence conseil sp√©cialis√©e depuis 2013 dans la digitalisation de la Relation Client & Collaborateur. Gr√¢ce √† la justesse de l‚Äôaccompagnement offert √† nos clients comme √† nos consultant¬∑e¬∑s, nous nous sommes construits au fil des ann√©es sur notre march√© une r√©putation de v√©ritables ¬´ horlogers du conseil ¬ª.\nNos domaines d‚Äôexpertise ?\nüéØ Le Product Management : pour porter la vision Produit et maximiser la cr√©ation de valeur.\nüé® Le Product Design : pour une conception Produit centr√©e utilisateur.\nüîß Le Product TechOps : pour d√©ployer les meilleures solutions digitales.\nüì¢ Le Product Marketing : pour maximiser le succ√®s d‚Äôun produit/service sur son march√©.\nüìä Le Product Data : pour mettre la donn√©e au c≈ìur de la strat√©gie de nos clients.\nü§ù La Gouvernance projet : pour optimiser le mode de management et le cadre organisationnel de nos clients.\n√ätre listenien¬∑ne, c‚Äôest √™tre au c≈ìur d‚Äôune entreprise qui valorise votre voix, votre parcours, et votre √©volution. Ici, nous appliquons les principes du Design Thinking √† votre carri√®re, co-construisant avec vous une trajectoire o√π vos expertises, vos aspirations, et vos forces sont au premier plan.\nüí™ Vos futures missions\nNous recherchons un Data Engineer qui aura la capacit√© de g√©rer √† la fois les aspects MCO/TMA mais √©galement la partie projets pour les diff√©rentes directions du groupe en collaboration avec le reste de l‚Äô√©quipe.\n√Ä la suite d‚Äôune passation avec notre centre de service, vous devrez √©galement √™tre en mesure d‚Äôinnover et d‚Äôam√©liorer les processus existants pour perfectionner l‚Äôefficience de l‚Äô√©quipe au quotidien dans ses diff√©rentes t√¢ches.\nIl s'agit d'un poste pour de l'internalisation.\nConcr√®tement, vous aurez l‚Äôopportunit√© de :\n‚Ä¢ Participer √† l‚Äôanalyse du besoin avec la supervision du PPO ou Manager et √† la conception du mod√®le de donn√©es permettant de r√©pondre aux enjeux.\n‚Ä¢ Utiliser l‚Äôanalyse des donn√©es pour fournir des √©l√©ments significatifs et √©clairer la conception/mise en ≈ìuvre des projets\n‚Ä¢ Concevoir, d√©velopper et maintenir les pipelines de donn√©es pour l‚Äôacquisition, la transformation, le stockage et la mise √† disposition des donn√©es.\n‚Ä¢ Optimiser les performances des d√©veloppements pour assurer une disponibilit√© et une fiabilit√© maximale\n‚Ä¢ Identifier et r√©soudre les goulots d‚Äô√©tranglement et les probl√®mes de performances dans les flux de donn√©es\n‚Ä¢ Mettre en place des processus et des contr√¥les pour garantir la qualit√© des donn√©es\n‚Ä¢ Concevoir et mettre en ≈ìuvre des syst√®mes d‚Äôalerte, de monitoring et de reprise automatique efficaces.\n‚Ä¢ R√©aliser la recette technique et animer la recette fonctionnelle avec les utilisateurs.\n‚Ä¢ R√©diger des documents associ√©s au projet (Sp√©cification Fonctionnelles, Sp√©cifications Techniques, Cahier de recette Technique, Document d‚Äôexploitation).\n‚Ä¢ D‚Äô√™tre force de proposition sur l‚Äôam√©lioration de notre stack Data.\n‚Ä¢ Faire le reporting d‚Äôavancement des travaux.\n‚Ä¢ Support au d√©ploiement.\n‚Ä¢ Assurer la maintenance corrective ou √©volutive.\n‚Ä¢ S‚Äôassurer de la coh√©rence des livrables avec les r√®gles et bonnes pratiques d√©finies au sein de l‚Äô√©quipe.\n‚Ä¢ S‚Äôassurer L‚Äôenvironnement Data dans le cadre de cette mission est actuellement constitu√© principalement d‚Äôune base de donn√©es Teradata (Cloud) ainsi qu‚Äôun environnement BigQuery.\n‚Ä¢ Langages/Framework : SQL, BigQuery, Python, Java, Shell\n‚Ä¢ Outils : OpenText, Talend\n‚Ä¢ Base de donn√©es : Teradata, BigQuery, SQL Server, IBM DB2\nüòé Votre vie de listenien¬∑ne\nCe qu‚Äôon vous propose ?\nü§ú √ätre membre d‚Äôune communaut√© : partager et enrichir vos comp√©tences au sein d‚Äôune √©quipe soud√©e.\nü§ù √ätre accompagn√©¬∑e : dans votre progression professionnelle et votre quotidien par votre Consultant Manager, votre Business Manager et notre Responsable Exp√©rience collaborateur¬∑rice.\nüéì √ätre form√©¬∑e en continu : gr√¢ce √† un plan de d√©veloppement des comp√©tences co-construit et nourri par nos solutions d‚Äôe-learning, les formations de la Listen too Academy et nos formations externes.\nüöÄ √ätre intrapreneur¬∑e : encourag√© √† innover, √©changer, entreprendre et ainsi contribuer √† l‚Äô√©panouissement du cabinet.\n‚òòÔ∏è √ätre engag√©¬∑e : √† nos c√¥t√©s dans une d√©marche soci√©tale et environnementale responsable et durable.\nü§ó √ätre ¬´ bien ¬ª ! : parce que le bien-√™tre physique et mental de nos collaborateur¬∑rice¬∑s est au c≈ìur de notre r√©ussite.\nConcr√®tement ?\nüéâ De multiples occasions de passer de bons moments avec notamment notre fameux s√©mineige, des afterworks et des teambuildings.\nüèãÔ∏è‚Äç‚ôÇÔ∏è Du sport et de la sant√© avec des √©v√®nements sportifs, nos partenaires\nGymlib\n,\nZenride\n,\nMoka.care\net le programme\nVitality\n.\nüòé Du confort avec du t√©l√©travail indemnis√©.\nü§ó Toujours plus liens avec un onboarding aux petits oignons, notre programme de parrainage, notre appli interne Mylistentoo, nos newsletters, nos podcasts et nos webinaires.\nüëç De nombreux avantages avec notre CSE, des tickets restaurant, une prime vacances, des primes de cooptation, de participation, de d√©veloppement‚Ä¶\nüöÄ Profil recherch√©\nVous avez au minimum 5 ans d‚Äôexp√©rience.\nVous disposez d‚Äôune exp√©rience confirm√©e chez un Grand Compte.\nVous maitrisez l‚Äôanglais dans un contexte professionnel tant √† l‚Äô√©crit qu‚Äô√† l‚Äôoral.\nVous avez envie de progresser et d‚Äô√©voluer au sein d‚Äôune soci√©t√© o√π votre voix compte.\nVous r√™vez de vous investir au sein d‚Äôune communaut√© soud√©e et passionn√©e !\nD√©roulement des entretiens ?\nüôãüèº‚Äç‚ôÄÔ∏è Un premier entretien t√©l√©phonique ou en visio avec l'une de nos charg√©es de recrutement (Ines, Aur√©lie ou Yasmine) pour voir si √ßa colle entre nous !\nüíÅüèª‚Äç‚ôÄÔ∏è Un deuxi√®me en visio ou en pr√©sentiel avec Aur√©lien, notre Directeur de R√©gion ou un de nos Business Manager (Lauranne, Hugo), pour valider notre premi√®re impression.\nüôãüèª‚Äç‚ôÇÔ∏è Un dernier √©change sur mesure avec Florent, Co-fondateur de l'agence, pour confirmer ce que nous savions d√©j√† ! üòâ\nVous l‚Äôaurez compris, cette phase de recrutement est avant tout l‚Äôoccasion de s‚Äô√©couter et d‚Äô√©changer. Chez Listen too, nous c√©l√©brons la diversit√© et l‚Äôinclusion, convaincus que chaque talent, quelle que soit son origine ou son parcours, est une richesse pour notre √©quipe et contribue √† notre r√©ussite commune.\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Confirm√©"
            ],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "DBMS": [
                "SQL Server",
                "BigQuery"
            ],
            "Other": [
                "Cloud"
            ],
            "EnSoftSkils": [
                "Collaboration"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "FINAXYS",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-python-at-finaxys-3887107285?position=5&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=f3RCu7zpUVWESqBI4Y3PBg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "LE CONTEXTE\nLeader en\nIT\n, dans les domaines Banque\n,\nFinance\net\nAssurance\n,\nFinaxys\nest un cabinet de\nconseil\ncr√©√© en 2008. Nous accompagnons au quotidien les plus grandes banques du pays sur leur\ntransformation digitale\n(BNP Paribas, Soci√©t√© G√©n√©rale, Cr√©dit Agricole, Natixis, etc.)\nNos clients bancaires travaillent √©galement dans des contextes Big Data sur des applications centrales rattach√©es aux Datalakes.\nLES MISSIONS\nD√©veloppement et traitements sur des applications Big Data (Python)\n√ätre force de proposition sur les choix techniques les plus pertinents\nMaintenir la qualit√© des solutions, mesure de cette qualit√©, alerte sur les non-conformit√©s et validation des solutions d√©finitives.\nAnalyser des risques li√©s aux solutions envisag√©es et proposition des actions de rem√©diation.\nApporter des solutions IT r√©pondant au mieux aux besoins du business port√© par la/le Product Owner (M√©tiers/Fonctions) en cherchant toujours la maximisation de la valeur g√©n√©r√©e\nAccompagner les √©quipes dans les migrations Cloud\nENVIRONNEMENT TECHNIQUE\nPython\nPandas\nScirpting Big Data\nCulture DevOps (Jenkins, Maven, Ansible)\nPROFIL\nComp√©tences Techniques et Fonctionnelles requises\nMaitrise obligatoire de l‚Äôanglais\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataAnalytics": [
                "Pandas"
            ],
            "DevTools": [
                "Jenkins"
            ],
            "Automation": [
                "Ansible"
            ],
            "Other": [
                "Cloud",
                "Big Data",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "ALLIANCE EMPLOI",
        "location": "La Couture, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/alternant-data-engineer-h-f-at-alliance-emploi-3913991795?position=6&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=IUN5rmntBuBv0NURN0B9ZA%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "La campagne \" alternance2024 \" est lanc√©e ! √ätes-vous pr√™t(e) √† monter en comp√©tences et acqu√©rir de l'exp√©rience ? Avec 25 ans d'expertise, 2000 salari√©s et notre r√©seau de 400 entreprises issues des secteurs industriel, agroalimentaire, automobile, m√©tallurgie, pharmaceutique, sid√©rurgie ou encore logistique, nous sommes la destination id√©ale pour celles et ceux qui cherchent une alternance.\nLaur√©at 2021 des P√©pites de l'alternance, notre mission est simple : apporter la bonne comp√©tence au bon moment. Et c'est l√† que vous entrez en jeu !\nNous sommes √† la recherche d'un Alternant - Data Engineer (H/F) en alternance pour une entreprise partenaire sp√©cialis√©e dans la chimie pour une dur√©e de 12 √† 24 mois au sein du Utilit√©s qui a en charge la fourniture d'utilit√©s pour l'ensemble du site.\nLe poste et les missions ?\nInformatique\nModernisation des pratiques de collecte de donn√©es industrielles,\nCr√©ation d'interface saisie d'encours de production\nRefonte des rapports d'exploitation\nAlgorithme de num√©risation des rapports PDF\nAutomatisation des archivages\nGestion des datas consolid√©es\nStatisitques\nInt√©gration des statistiques au coeur des syst√®mes de production\nCartes de contr√¥les\nPr√©visions statistiques\nEtude d'implantation de machine learning\nAlgorithme de traitement et nettoyage des donn√©es (analyse de la d√©rive)\nOrganisationnel\nD√©ploiement des solutions de power BI au sein du service\nOutils d'affiche, de partage et d'analyse de donn√©es\nAnalyse fonctionnelle des flux de donn√©es\nR√©daction des logigrammes de gestion de donn√©es\nR√©daction des bonnes pratiques d'archivage et de traitement de don√©nes\nFormation des utilisateurs et propri√©taires\nCe que nous allons aimer chez vous ?.\nVous avez le sens de l'organisation et du service, une capacit√© √† s'int√©grer dans un milieu technique de terrain, vous avez le sens de l'organisation et des priorit√©s, vous √™tes rigoureux\nMais aussi :\nVous pr√©parez un dipl√¥me d'ing√©nieur Bac +4 / 5 en informatique et analyse de donn√©es\nVous ma√Ætrisez le d√©veloppement informatique (base de donn√©es, Python, Java)\nLes avantages de rejoindre Alliance Emploi\nContrat : ALTERNANCE de 12 √† 24 mois\nD√©but : Septembre 2024\nLieu : LESTREM [ site non accessible en transport en commun]\nR√©mun√©ration : Selon le bar√®me de l'alternance\nEt aussi : int√©ressement, mutuelle, pr√©voyance, CSE, formations qualifiantes\nEt ce n'est pas tout ! En choisissant Alliance Emploi, vous vivrez une exp√©rience bas√©e sur la confiance, la solidarit√© et l'engagement. Vous d√©velopperez vos comp√©tences √† travers une diversit√© de missions et notre r√©seau d'entreprises, et nous nous engageons √† vous proposer un accompagnement personnalis√© pour booster votre carri√®re.\nAlors convaincu(e) ?\nN'attendez plus pour postuler et venez d√©couvrir la diff√©rence Alliance Emploi !\nLa diversit√© est une force. Nous sommes engag√©s pour l'inclusion en offrant des opportunit√©s de carri√®re √† toutes les personnes, ind√©pendamment de leur genre ou de leur situation de handicap.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "",
            "Level": "",
            "Experience": "25 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Java",
                "Python"
            ],
            "DataVisualisation": [
                "Power BI"
            ],
            "Other": [
                "Statistiques",
                "Machine Learning"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "WHIZE",
        "location": "Neuilly-sur-Seine, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/consultant-data-engineer-at-whize-3916769237?position=7&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=tBWGhRh06hBOkPIFde2ODg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Offre d‚Äôemploi pour un CDI : Consultant Data Engineer\nWHIZE est sp√©cialis√©e dans le d√©veloppement de solutions sur mesure en architecture Serverless (Azure, Amazon WS, Google CP) et de solutions bas√©es sur l'√©cosyst√®me Microsoft 365 (SharePoint, Teams, Power Platform).\nVos missions :\nConcevoir des solutions de traitement de volume tr√®s important de donn√©es.\nD√©veloppement de flux de donn√©es et pr√©paration de leur analyse.\nPr√©paration des donn√©es pour l'analyse des donn√©es collect√©es.\nProfil recherch√© :\n2 ans minimum d‚Äôexp√©rience.\nMa√Ætrise du langage Python et Scala\nConnaissance d'un ou plusieurs ETL du march√© (Talent , SSIS, Azure Data Factory, ...)\nForte expertise en SQL\n√ätre √† l‚Äôaise avec un ou plusieurs outils Devops (Jenkins, git, GitHub, gitlab, docker, kubernetes, etc‚Ä¶)\nConnaissances appr√©ci√©es :\nHadoop, Spark, Kafka\nConnaissance des syst√®mes NoSQL : Elasticsearch, HBase, Cassandra, Redshift\nConnaissance de l'offre data d'un des providers Cloud (GCP, Azure, AWS)\nQu‚Äôattendez vous pour nous rejoindre ?\nVous ferez partie d‚Äôune soci√©t√© √† taille humaine et qui b√©n√©ficie des avantages d'un grand groupe. Nous adressons une centaine de clients en direct dont la moiti√© sont des grands comptes.\nVous serez accompagn√©(e) et manag√©(e) par le CEO de WHIZE (THE WHIZE MAN).\nVous allez compl√©ter notre √©quipe dynamique et travailler avec nous dans une ambiance Start-up et conviviale.\nVous occuperez des postes int√©ressants et √©volutifs.\nVous b√©n√©ficierez des √©v√®nements internes organis√©s pour parler tech, business et projets.\nVous r√©aliserez des projets √† forte valeur ajout√©e.\nüìç : Neuilly-Sur-Seine+ T√©l√©travail\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "CDI",
            "Salary": "",
            "Level": "",
            "Experience": "2 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "HBase",
                "Elasticsearch",
                "Cassandra",
                "SQL",
                "NoSQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "CloudComputing": [
                "GCP",
                "AWS",
                "Azure"
            ],
            "DevTools": [
                "Git",
                "Docker",
                "Jenkins"
            ],
            "SoftBigDataProcessing": [
                "Cassandra",
                "HBase"
            ],
            "Automation": [
                "Kubernetes"
            ],
            "Containers": [
                "Docker",
                "Kubernetes"
            ],
            "Collaboration": [
                "Teams"
            ],
            "Other": [
                "Cloud",
                "DevOps"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "AXA en France",
        "location": "Lille, Hauts-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/tech-lead-data-engineer-at-axa-en-france-3837261944?position=8&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=ZUYIbU0zZpg2YW%2FH1I6apw%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Environnement\nEn tant que\nTech Lead Data Engineer F/H\n, vous allez contribuer directement aux projets des directions m√©tier (ex : fraude sant√©, multi√©quipements, pricing IARD, optimisation du lead management, fragilit√© auto, ‚Ä¶) d‚ÄôAXA France et √† la construction du socle technique Big Data.\nVous allez int√©grer une √©quipe d'une dizaine de personne compos√©e de Data Engineer et des Tech Lead travaillant en mode Feature Team au sein des tribus m√©tier de la Direction Transformation Digital Tech et DATA (DT2).\nLa Direction Transformation Digital Tech et DATA d'AXA France en quelques mots :\n- Une organisation agile en feature teams : tribus, guildes, squads\n- Des projets sur des applications innovantes √† fort trafic (web, mobile‚Ä¶)\n- Des m√©thodologies craft (TDD, BDD, clean code, code review‚Ä¶) et DevOps\n- Une communaut√© de partage de bonnes pratiques (BBL, dojo, meetup, conf‚Ä¶)\nVotre r√¥le et vos missions\nVous aurez pour missions principales de d√©velopper les projets Big Data demand√©s par le m√©tier, et notamment :\nD‚Äôaccompagner techniquement les Data Engineer de l‚Äô√©quipe (coaching, code review, pair programming‚Ä¶)\nPasser de la donn√©e brute √† de la donn√©e exploitable, expos√©e sous forme de tables requ√™tables dans le datalake\nConsolider ces donn√©es au fur et √† mesure de leur alimentation r√©currente dans le data lake\nLes exploiter pour atteindre la finalit√© business (exposition de business view, r√©int√©gration des r√©sultats dans le SI, service de scoring, ‚Ä¶)\nDe travailler √† la cr√©ation du socle technique Big Data et industrialiser le cycle de d√©veloppement de l'√©quipe\nDe mettre en place et de garantir le respect dans la dur√©e d'un processus qualit√© sur l'ensemble du cycle de DEV (documents, tests unitaires / int√©gration / fonctionnels, commentaires, versionning, etc.)\nVotre profil\nD'une formation sup√©rieure en informatique ou scientifique (Master ou Dipl√¥me d'ing√©nieur), vous justifiez de plusieurs exp√©riences significatives (+ de 7 ans)\nsur du d√©veloppement big data, en particulier sur du PySpark.\nComp√©tences techniques :\nConnaissances avanc√©es en d√©veloppement en\nPySpark (Spark avec le langage Python)\nMaitrise de l'environnement\nMicrosoft Azure\nConnaissances avanc√©es d'outils de BI comme\nPowerBI\nComp√©tences transverses :\nCapacit√© √† interagir avec des parties prenantes diverses : Business analyst, Architectes, M√©tier\nExp√©rience en mode de delivery Agile (Scrum, Kanban, etc...)\nDriver et accompagner des Data Engineer sur le plan op√©rationnel\nEt Id√©alement :\nAvoir une exp√©rience en tant que lead\nDes Connaissances sur Azure DevOps, Azure Pipeline, GIT, JIRA\nMaitrise des Traitements Big Data en mode Streaming\nMaitrise des Bases de donn√©es relationnelles et NoSQL\nUne exp√©rience professionnelle avec des outils comme Azure Databricks, Azure Data Lake Storage ou encore Azure Data Factory\nMais pourquoi AXA France ?\nNous sommes persuad√©s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs ! Les avantages que nous proposons √† nos salari√©s sont nombreux.\nNous choisir, c‚Äôest b√©n√©ficier par exemple :\nD‚Äôun package de r√©mun√©ration complet comprenant un salaire fixe, un compl√©ment de r√©mun√©ration variable, des primes, de la participation et de l‚Äôint√©ressement, la possibilit√© d‚Äôacqu√©rir des actions AXA, ou encore des solutions d‚Äô√©pargne avantageuses ;\nEquilibre vie Pro / Perso. : D‚Äôun cadre de travail flexible jusqu‚Äô√† 3 jours de t√©l√©travail possible par semaine, des tickets restaurant pour les jours t√©l√©travaill√©s ou encore une participation √† l‚Äôachat d‚Äôun √©cran ou fauteuil ergonomique ;\nD‚Äôune politique visant √† concilier vie personnelle et vie professionnelle avec 28 jours de cong√©s pay√©s, entre 14 et 16 RTT selon les ann√©es, des formules de travail √† temps partiel ou encore des jours d‚Äôabsence r√©mun√©r√©es pour la rentr√©e scolaire ou un d√©m√©nagement par exemple ;\nDe la possibilit√© de s‚Äôengager pour une cause qui vous tient √† c≈ìur gr√¢ce √† nos associations telles que AXA Atout C≈ìur, AXA Comp√©tences Solidaires ou encore AXA Pr√©vention ;\nEt bien plus encore ! Perspectives de d√©veloppement des comp√©tences et de carri√®res immenses, CE, conciergerie, offres privil√®ges, soutien en cas d‚Äô√©preuve personnelle‚Ä¶On s‚Äôarr√™te l√†, la liste est longue\nQui sommes nous ?\nAXA est un des leaders de l‚Äôassurance et de la gestion d‚Äôactifs dans le monde.\nNous aidons nos 108 millions de clients √† traverser les petites et grandes difficult√©s de la vie.\nChaque jour, nous agissons ensemble pour inventer la meilleure mani√®re de les prot√©ger et voulons donner √† chacun les moyens de vivre une vie meilleure.\nUn challenge qui donne le sourire et envie de se lever le matin !\nChez AXA, nous sommes persuad√©s que pour bien prendre soin de nos clients, nous devons commencer par bien prendre soin de nos collaborateurs. C‚Äôest pour cette raison que nous menons une politique RH engag√©e qui favorise la diversit√©, qui pr√©serve l‚Äô√©quilibre vie priv√©e-vie professionnelle et acc√©l√®re le d√©veloppement des comp√©tences et des carri√®res.\nAinsi, en rejoignant AXA France vous travaillerez dans une entreprise responsable, offrant une v√©ritable culture d‚Äôexpertise, acc√©l√©rant le d√©veloppement des comp√©tences de chacun et proposant une r√©mun√©ration attractive.\nPourquoi nous rejoindre ?\nVous √™tes porteur d‚Äôid√©es et d‚Äôinitiatives innovantes ? Vous proposez des solutions et √™tes au service du client ? Faites partie de notre grande famille en rejoignant\nUn leader mondial offrant des opportunit√©s de carri√®res int√©ressantes\nUne entreprise qui donne une place de choix √† l‚Äôinnovation, √† l‚Äôinitiative et aux actions solidaires (notamment via l‚Äôassociation AXA Atout C≈ìur)\nUn environnement inclusif √† tous les niveaux (mixit√©, handicap, initiatives pour favoriser l‚Äôinsertion des jeunes, orientation sexuelle, etc.)\nUn acc√®s √† de multiples avantages (cong√©s, temps partiel, t√©l√©travail, etc.)\nUn cadre stimulant, qui permet de rencontrer des collaborateurs performants et d‚Äôenrichir ses comp√©tences\nVictime ou t√©moin, en cas de discrimination, vous pouvez adresser vos signalements et/ou alertes discrimination √† alerte.discrimination.harcelement@axa.fr\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "3, 3",
            "Level": "",
            "Experience": "7 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "NoSQL"
            ],
            "BigData": [
                "Databricks",
                "Spark"
            ],
            "CloudComputing": [
                "Azure"
            ],
            "DevTools": [
                "Git"
            ],
            "Collaboration": [
                "Teams",
                "JIRA"
            ],
            "Other": [
                "Big Data",
                "DevOps"
            ],
            "EnSoftSkils": [
                "Initiative"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "Pictarine",
        "location": "Toulouse, Occitanie, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-at-pictarine-3911913926?position=9&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=n3HEyd%2F98Jz4oy37fKFd1g%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Mission and challenges üéØ\nSi tu es enthousiaste √† embarquer dans la nouvelle √©quipe data de Pictarine pour la faire rayonner avec tout ton savoir-faire, alors c‚Äôest l‚Äôaventure qu‚Äôil te faut! üèîÔ∏è\nAvec plus de 1K tables, 2M de clients et 4M de commandes en 2022, les √©quipes de Pictarine ne sont jamais √† court d‚Äôid√©es pour explorer de nouveaux horizons. üöÄ\nEn tant que Data Engineer chez Pictarine tu vas pouvoir utiliser toutes tes comp√©tences SQL pour garantir la qualit√© de la data sur GCP, accompagner et challenger les besoins data.\nTu √©volueras au sein de l‚Äô√©quipe Engineering, compos√©e des p√¥les dev & data.\nTon r√¥le comprendra les aspects suivants üëáüèª\nTu es garant de la qualit√© de la data !\nEn simplifiant la structure de la data et r√©duisant le nombre de tables\nEn transformant les donn√©es pour les rendre facilement utilisables\nEn orchestrant le flux des donn√©es de mani√®re continue et automatique\nTu accompagnes et challenges les √©quipes de Pictarine !\nEn co-construisant des solutions data appropri√©es\nEn √©levant le niveau de jeu des m√©thodes data existantes\nEn faisant rayonner la data autour de bonnes pratiques et d‚Äôoutillages ad√©quates\nProfil Recherch√©\nAbout you üíé\nTu as au moins 5 ans d‚Äôexp√©rience sur un poste similaire\nTu ma√Ætrises le data warehouse BigQuery et son langage SQL\nTu es √† l'aise avec les services GCP\nTu as de bonnes connaissances dans la conception de mod√®les de donn√©es et les strat√©gies d'optimisation des requ√™tes SQL\nTu as des comp√©tences en DevOps pour le d√©ploiement et la gestion efficace des pipelines de donn√©es\nTu as une bonne ma√Ætrise de Python & Github\nTu es organis√©, rigoureux et portes une grande attention aux d√©tails\nTu es dot√© d‚Äôexcellentes qualit√©s relationnelles, de communication et de vulgarisation\nTu as une passion pour r√©soudre des probl√®mes business avec la programmation\nTu es curieux de tester des nouvelles technologies\nTu es un team player et toujours √† l'aff√ªt de nouvelles id√©es\nWork @ Pictarine‚ú®\nUn environnement de travail agile, collaboratif, international et multiculturel\nDes perspectives d‚Äô√©volution rapides\nDes locaux tout beaux √† Lab√®ge avec du mat√©riel dernier cri (mais aussi des snacks √† profusion et un frigo √† boissons toujours bien rempli)\nUn apprentissage permanent : conf√©rence, meet-up, Pictarine Academy, cours d‚Äôanglais.\nDes events tous les mois : massage, pilates, TGIF, team building .\nUn environnement de travail flexible : horaires, politique de remote hybride.\nUn package de r√©mun√©ration attractif : salaire comp√©titif, RTT, mutuelle & pr√©voyance 100% prise en charge, int√©ressement.\nDes petits + : D√©veloppement de photos gratuit, subvention sport, 3 jours ‚Äúentraide familiale‚Äù, jours de cong√©s en plus avec l'anciennet√©... ü§´ on ne te d√©voile pas tout !\nRecruitment process ‚öôÔ∏è\nTu souhaites nous rejoindre ? Viens rencontrer les gens avec qui tu vas bosser :\n1er √©change pour apprendre √† se conna√Ætre avec Marie - Engineering Manager Data (15‚Äô)\nEntretien Manager avec Marie (60-90‚Äô)\nTest pratique afin de nous montrer tes talents üôÇ (3 heures)\nEntretien final avec 2 membres du Codir (90‚Äô)\nWelcome aboard !\nShow more\nShow less",
        "details": {
            "JobDetail": [
                "Hybride",
                "Remote"
            ],
            "TypeContract": "",
            "Salary": "100, 100",
            "Level": "",
            "Experience": "5 an(s)"
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "CloudComputing": [
                "GCP"
            ],
            "DBMS": [
                "BigQuery"
            ],
            "Other": [
                "DevOps"
            ],
            "EnSoftSkils": [
                "Communication"
            ]
        }
    },
    {
        "source": "LinkedIn",
        "company": "CITECH",
        "location": "Paris, √éle-de-France, France",
        "link": "https://fr.linkedin.com/jobs/view/data-engineer-h-f-at-citech-3908612761?position=10&pageNum=15&refId=j70Dez0slhRvaNSu6nPrqg%3D%3D&trackingId=QDl21Yh58PhaCfqsKPTodg%3D%3D&trk=public_jobs_jserp-result_search-card",
        "description": "Description de l'entreprise\nCITECH recrute !\nSi vous souhaitez apporter vos comp√©tences dans la r√©alisation d‚Äôun projet important, nous avons LA mission pour vous ! Nous recherchons en effet un(e)\nData Engineer (H/F)\nVotre mission est pour un client reconnu dans le secteur bancaire, implant√© dans de nombreuses villes en France, il a pour objectif d'acc√©l√©rer sa transformation digitale afin d'offrir toujours plus de solutions et de services innovants.\nDescription du poste\nVous aurez donc les missions principales suivantes :\nSupport de l'application (notamment lors des cl√¥tures mensuelles).\nParticiper √† la maintenance √©volutive.\nParticiper √† la conception et l'impl√©mentation de nouvelles fonctionnalit√©s.\nParticiper √† la refonte technique.\nParticiper √† la migration de Talend vers Spark/scala.\nQualifications\nDe formation sup√©rieure en informatique, vous justifiez de 5 ann√©es d‚Äôexp√©rience minimum sur un poste similaire.\n‚öôÔ∏è Les comp√©tences attendues sont les suivantes :\n‚úîÔ∏è Vous ma√Ætrisez Spark, Talend (Data Int√©gration, Big Data) et Scala.\n‚úîÔ∏èVous avez des comp√©tences en d√©veloppement (Shell unix, Perl, PHP, Python, git, github).\n‚úîÔ∏èVous avez des comp√©tences sur l‚Äôenvironnement technique suivant :\nHadoop (Big Data), Hive, Microsoft PowerBI, Microsoft SQLServer Analysis services (Olap), Integration services, Reporting services, Scripting (GuitHub, Ansible, AWX, shell, vba) et SQL Server Database.\nInformations suppl√©mentaires\nPoste situ√© √†\nParis\n‚òÄÔ∏è\nSalaire :\n45-65 K‚Ç¨ brut/an\nFreelance :\n300-450 ‚Ç¨ brut/jour\nR√©f√©rence :\n240424_OUTIL DE PILOTAGE FINANCIER - TALEND / SPARK / SCALA\nPourquoi rejoindre Citech ?\nUne ambiance de travail conviviale avec des afterworks organis√©s r√©guli√®rement !\nDes missions de longues dur√©es\nDes formations adapt√©es √† vos envies et vos aspirations\nUne mobilit√© que si vous le souhaitez\nUn accompagnement personnalis√© avec un suivi r√©gulier (autour d‚Äôun caf√© ou un th√©, c‚Äôest vous qui choisissez )\nUne mutuelle avantageuse pour vous mais aussi pour les membres de votre famille\nUne flexibilit√© sur la gestion de vos repas\nUn statut Cadre et une convention collective SYNTEC\nAlors qu‚Äôattendez-vous pour nous rejoindre ?\nCompany Description\nCITECH ce n‚Äôest pas une Entreprise de Services du Num√©rique comme les autres : c‚Äôest avant tout une aventure humaine. Nous cherchons des personnalit√©s passionn√©es qui nous ressemblent ! Dans un environnement convivial et chaleureux, venez r√©v√©ler vos talents !\nNos fid√®les clients, reconnus √† l‚Äô√©chelle internationale, offrent √† nos collaborateurs de multiples possibilit√©s de carri√®re. Nos domaines d‚Äôinterventions sont vari√©s : banque, assurance, automobile, sant√©, transport ou encore la robotique, vous trouverez forc√©ment un secteur √©panouissant, √† votre image !\nVous avez un projet professionnel ? Nous sommes l√† pour vous aider √† le d√©velopper. Pour nous, l‚Äôessentiel c‚Äôest vous. C‚Äôest pourquoi nous assurons un suivi r√©gulier et portons une attention toute particuli√®re √† votre plan de carri√®re.\nShow more\nShow less",
        "details": {
            "JobDetail": [],
            "TypeContract": "",
            "Salary": "45",
            "Level": "",
            "Experience": null
        },
        "title": "data engineer",
        "skills": {
            "ProgLanguage": [
                "Scala",
                "Python"
            ],
            "DataBase": [
                "SQL"
            ],
            "BigData": [
                "Hadoop",
                "Spark"
            ],
            "DevTools": [
                "Git"
            ],
            "DBMS": [
                "SQL Server"
            ],
            "Automation": [
                "Ansible"
            ],
            "Other": [
                "Big Data"
            ]
        }
    }
]